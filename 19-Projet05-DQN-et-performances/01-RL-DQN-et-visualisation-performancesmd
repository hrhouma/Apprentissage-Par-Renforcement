# **Apprentissage par Renforcement pour l'Environnement CartPole : EntraÃ®nement DQN en Temps RÃ©el avec Visualisations de Performances**

---

## **Objectif du Projet**  
ğŸ“ *Former un agent DQN pour Ã©quilibrer une tige sur un chariot dans l'environnement CartPole-v1, en utilisant l'apprentissage par renforcement.*

### **But VisÃ© :**
ğŸ› ï¸ *DÃ©velopper un agent capable de maximiser le temps d'Ã©quilibre en prenant des dÃ©cisions intelligentes basÃ©es sur des observations.*

---

## **Approche UtilisÃ©e**  
ğŸ§  **Algorithme :** DQN avec *replay d'expÃ©rience* et *politique epsilon-greedy*, implÃ©mentÃ© avec TensorFlow dans l'environnement CartPole d'OpenAI Gym.  
ğŸ’¡ **Techniques ClÃ©s :** DQN, Replay d'expÃ©rience, Politique epsilon-greedy.

### **Ã‰tapes Principales de l'Apprentissage par Renforcement**  
1. ğŸ“ **Formulation du ProblÃ¨me**  
   ğŸ”½

2. ğŸŒ **CrÃ©ation de l'Environnement**  
   ğŸ”½

3. ğŸ **DÃ©finition des RÃ©compenses**  
   ğŸ”½

4. ğŸ¤– **CrÃ©ation de l'Agent**  
   ğŸ”½

5. ğŸ‹ï¸ **EntraÃ®nement de l'Agent**  
   ğŸ”½

6. ğŸ§ª **Validation de l'Agent**  
   ğŸ”½

7. ğŸš€ **DÃ©ploiement de la Politique**  

---

## **Configuration des HyperparamÃ¨tres**  
- **Gamma (Î³)** : 0.95
- **Epsilon** : 1.0 (dÃ©croÃ®t avec le temps)
- **DÃ©croissance d'Epsilon** : 0.995
- **Minimum dâ€™Epsilon** : 0.01
- **Taux d'apprentissage (Î±)** : 0.001

---

## **Fonctionnement de l'Environnement CartPole**  
ğŸ›’ **Environnement :** OpenAI Gym  
ğŸ¯ **Objectif de l'agent :** Garder la tige en Ã©quilibre sur le chariot.  
ğŸ† **RÃ©compense :** Points pour chaque pas oÃ¹ la tige reste en Ã©quilibre; pÃ©nalitÃ© pour Ã©chec.  

---

## **Processus Ã‰tape par Ã‰tape**  

1. **Formuler le ProblÃ¨me**  
   - Objectif : Maximiser le temps d'Ã©quilibre de la tige.
   
2. **CrÃ©er l'Environnement**  
   - Utiliser OpenAI Gym pour simuler l'environnement de CartPole.
   
3. **DÃ©finir les RÃ©compenses**  
   - RÃ©compense positive pour chaque moment oÃ¹ la tige est en Ã©quilibre.  

4. **CrÃ©er l'Agent**  
   - Construction d'un rÃ©seau neuronal pour prÃ©dire les actions.

5. **EntraÃ®ner l'Agent**  
   - EntraÃ®nement Ã  partir des interactions et des expÃ©riences re-jouÃ©es.

6. **Valider l'Agent**  
   - Tester les performances de l'agent dans l'environnement.

7. **DÃ©ployer la Politique**  
   - Finaliser et appliquer la politique formÃ©e.

---

## **Comparaison : Apprentissage par Renforcement vs. IA Symbolique**  
| **Aspect** | **IA Symbolique** | **Apprentissage par Renforcement** |
|------------|--------------------|-------------------------------------|
| **Approche de rÃ©solution** | Raisonnement dÃ©ductif, rÃ¨gles | Essai-erreur, basÃ© sur les rÃ©compenses |
| **Quand les utiliser** | Lorsque les rÃ¨gles sont claires | Lorsque l'exploration est nÃ©cessaire |
| **Exemple** | Diagnostic mÃ©dical (Mycin) | Tesla Autopilot |

---

## **Exemples d'Environnements et d'Objectifs d'Apprentissage**  

- **CartPole-v1** : Ã‰quilibrer une tige sur un chariot en mouvement.  
- **MountainCar-v0** : Conduire une voiture en utilisant la force pour grimper une colline.  
- **LunarLander-v2** : Atterrir une navette lunaire en utilisant le carburant de maniÃ¨re optimale.  
- **Breakout-v0** : DÃ©truire des briques avec une balle, en optimisant la prÃ©cision et les rÃ©flexes.

---

## **Structure de RÃ©compenses par Environnement**  

| **Environnement**   | **RÃ©compense Positive**                     | **Description**                               |
|---------------------|---------------------------------------------|-----------------------------------------------|
| CartPole-v0         | Pour chaque instant oÃ¹ la tige est stable   | Encourage une durÃ©e d'Ã©quilibre prolongÃ©e     |
| MountainCar-v0      | Pour atteindre le drapeau, nÃ©gative par pas | Motive la montÃ©e vers l'objectif              |
| LunarLander-v2      | Pour atterrissage rÃ©ussi, nÃ©gative sinon    | Ã‰quilibre entre sÃ©curitÃ© d'atterrissage et fuel |

---

## **Estimation des CoÃ»ts sur AWS SageMaker**  
ğŸ’° **Instance recommandÃ©e :** p3.2xlarge  
ğŸ’² **CoÃ»t horaire :** 3,825 $/h  
ğŸ’¸ **CoÃ»t mensuel (estimÃ© en USD)** : 2206,40 $

---

## **ModÃ¨les UtilisÃ©s et MÃ©thodes d'Ã‰valuation**  
- **ModÃ¨les d'Apprentissage par Renforcement** :
  - DDPG (Deep Deterministic Policy Gradient)
  - PPO (Proximal Policy Optimization)
  - DQN (Deep Q-Network)
  
- **MÃ©thodes d'Ã‰valuation** :
  - **RÃ©compense par Ã‰pisode** : Total de points accumulÃ©s.
  - **Taux de SuccÃ¨s** : Pourcentage d'objectifs atteints.
  - **Temps de Survie** : Temps avant l'Ã©chec de l'agent.
  

