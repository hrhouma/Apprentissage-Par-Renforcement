{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Quiz à choix multiples : Apprentissage par Différence Temporelle (TD), SARSA et Q-Learning**\n",
        "\n",
        "1. **Qu'est-ce que l'apprentissage par différence temporelle (TD) ?**\n",
        "   - A. Une méthode basée sur des épisodes complets\n",
        "   - B. Une approche combinant Monte Carlo et programmation dynamique\n",
        "   - C. Une méthode nécessitant un modèle de l'environnement\n",
        "   - D. Une méthode de supervision directe\n",
        "\n",
        "2. **Lequel des éléments suivants est un avantage des méthodes TD ?**\n",
        "   - A. Elles nécessitent un modèle de l'environnement\n",
        "   - B. Elles attendent la fin de l'épisode pour mettre à jour les valeurs\n",
        "   - C. Elles peuvent fonctionner avec des épisodes incomplets\n",
        "   - D. Elles ont besoin de calculs complexes\n",
        "\n",
        "3. **Quelle est la règle de mise à jour TD(0) pour estimer la valeur d’un état ?**\n",
        "   - A. $$ V(s) \\leftarrow R_{t+1} $$\n",
        "   - B. $$ V(s) \\leftarrow \\alpha [ R_{t+1} - V(s)] $$\n",
        "   - C. $$ V(s) \\leftarrow V(s) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(s)] $$\n",
        "   - D. $$ V(s) \\leftarrow \\gamma V(s) $$\n",
        "\n",
        "4. **Dans les méthodes TD, que représente ? $$\\gamma$$ **\n",
        "   - A. Le taux d'apprentissage\n",
        "   - B. La probabilité d'exploration\n",
        "   - C. Le facteur d'actualisation\n",
        "   - D. Le nombre d'actions possibles\n",
        "\n",
        "5. **Pourquoi SARSA est-il considéré comme une méthode sur politique ?**\n",
        "   - A. Il utilise une politique différente pour le comportement et la mise à jour\n",
        "   - B. Il suit la même politique pour la prise d'action et la mise à jour\n",
        "   - C. Il nécessite un modèle de l'environnement\n",
        "   - D. Il ne dépend pas d'une politique d'apprentissage\n",
        "\n",
        "6. **Quel est le rôle du paramètre $$\\alpha$$ dans les méthodes TD ?**\n",
        "   - A. Fixer la probabilité d'explorer\n",
        "   - B. Ajuster la vitesse de mise à jour des estimations\n",
        "   - C. Sélectionner la prochaine action\n",
        "   - D. Déterminer la politique de récompense\n",
        "\n",
        "7. **Dans une politique epsilon-greedy, que se passe-t-il si epsilon est proche de 1 ?**\n",
        "   - A. L'agent choisit toujours l'action optimale\n",
        "   - B. L'agent explore souvent de nouvelles actions\n",
        "   - C. L'agent devient complètement aléatoire\n",
        "   - D. L'agent ne met jamais à jour ses estimations\n",
        "\n",
        "8. **Quelle est la règle de mise à jour pour SARSA ?**\n",
        "   - A. $$ Q(s, a) \\leftarrow \\alpha [R_{t+1} + \\gamma \\max Q(S_{t+1}, a') - Q(s, a)] $$\n",
        "   - B. $$ Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(s, a)] $$\n",
        "   - C. $$ V(s) \\leftarrow V(s) + \\alpha R_{t+1} $$\n",
        "   - D. $$ Q(s, a) \\leftarrow \\max Q(s, a) $$\n",
        "\n",
        "9. **Pourquoi Q-Learning est-il considéré comme une méthode hors politique ?**\n",
        "   - A. Il suit une politique unique pour la prise de décision\n",
        "   - B. Il maximise les récompenses immédiates seulement\n",
        "   - C. Il utilise une politique différente pour la prise d’action et la mise à jour\n",
        "   - D. Il fonctionne sans connaître les états de l'environnement\n",
        "\n",
        "10. **Dans Q-Learning, la mise à jour des valeurs est basée sur :**\n",
        "    - A. L'action réellement entreprise\n",
        "    - B. La moyenne des actions de l'état suivant\n",
        "    - C. La valeur maximale de l'état suivant\n",
        "    - D. La somme des valeurs de tous les états\n",
        "\n",
        "11. **Que signifie \"TD\" dans l'apprentissage TD(0) ?**\n",
        "    - A. Temps différentiel\n",
        "    - B. Taux de décroissance\n",
        "    - C. Différence temporelle\n",
        "    - D. Décision temporelle\n",
        "\n",
        "12. **Dans une politique epsilon-greedy, que signifie un epsilon proche de 0 ?**\n",
        "    - A. L'agent explore fréquemment de nouvelles actions\n",
        "    - B. L'agent exploite souvent les meilleures actions\n",
        "    - C. L'agent ne prend que des actions aléatoires\n",
        "    - D. L'agent ignore les valeurs de récompense\n",
        "\n",
        "13. **Quelle affirmation est vraie concernant les méthodes TD ?**\n",
        "    - A. Elles nécessitent des épisodes complets\n",
        "    - B. Elles mettent à jour les estimations après chaque action\n",
        "    - C. Elles nécessitent un modèle de transition\n",
        "    - D. Elles ignorent la fonction de récompense\n",
        "\n",
        "14. **Laquelle de ces méthodes utilise la valeur Q maximale pour la mise à jour ?**\n",
        "    - A. SARSA\n",
        "    - B. Q-Learning\n",
        "    - C. TD(0)\n",
        "    - D. Monte Carlo\n",
        "\n",
        "\n",
        "15. Quelle est la fonction de la variable epsilon dans une politique epsilon-greedy ?\n",
        "$$\\epsilon$$\n",
        "\n",
        "    - A. Réduire le facteur d'actualisation\n",
        "    - B. Déterminer le taux d'apprentissage\n",
        "    - C. Contrôler la probabilité d'exploration\n",
        "    - D. Fixer la récompense à chaque étape\n",
        "\n",
        "16. **Dans SARSA, que représente le terme ?** $$ Q(S_{t+1}, A_{t+1}) $$  \n",
        "    - A. La valeur Q de l'action optimale à l'état suivant\n",
        "    - B. La récompense cumulée à la fin de l'épisode\n",
        "    - C. La valeur Q de l'action choisie par la politique à l'état suivant\n",
        "    - D. La somme de toutes les valeurs Q précédentes\n",
        "\n",
        "17. **Dans Q-Learning, ce terme signifie :** $$ \\max_{a'} Q(S_{t+1}, a') $$  \n",
        "\n",
        "    - A. La moyenne des valeurs Q dans l'état suivant\n",
        "    - B. La récompense de la prochaine action\n",
        "    - C. La valeur maximale des actions possibles dans l'état suivant\n",
        "    - D. La valeur Q minimale de l'état suivant\n",
        "\n",
        "18. **En apprentissage hors politique, l'agent :**\n",
        "    - A. Suit la même politique pour le comportement et la mise à jour\n",
        "    - B. Utilise une politique de comportement différente de la politique de mise à jour\n",
        "    - C. Ne met jamais à jour ses valeurs Q\n",
        "    - D. Se base uniquement sur des épisodes complets\n",
        "\n",
        "19. **Quel est l'effet de choisir un taux d'apprentissage très élevé ? $$\\alpha$$**\n",
        "\n",
        "    - A. Les valeurs Q convergent rapidement et restent stables\n",
        "    - B. Les valeurs Q peuvent devenir instables et ne pas converger\n",
        "    - C. Les valeurs Q sont ignorées\n",
        "    - D. L'agent devient plus explorateur\n",
        "\n",
        "20. **Dans SARSA, l'agent met à jour ses valeurs Q en fonction :**\n",
        "    - A. De l'action optimale dans l'état suivant\n",
        "    - B. De l'action réellement prise par la politique dans l'état suivant\n",
        "    - C. De la moyenne des actions de l'état suivant\n",
        "    - D. De l'intervalle de temps entre les actions\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tIbqQi9sm20n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8-O2i99mvdG"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sjoEwn9MoT4l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}