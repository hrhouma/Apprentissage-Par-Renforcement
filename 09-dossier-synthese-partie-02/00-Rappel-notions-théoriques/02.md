# Apprentissage par Différence Temporelle (TD) en Apprentissage par Renforcement**



## 1. Introduction à l'Apprentissage par Différence Temporelle (TD)

### 1.1 Qu'est-ce que l'Apprentissage par Différence Temporelle ?
L'apprentissage par différence temporelle (TD) est une technique qui combine les idées des méthodes de Monte Carlo et de la programmation dynamique. Il permet d'estimer la valeur d'un état en utilisant non seulement les récompenses observées mais aussi les valeurs d'états futurs estimées, sans attendre la fin des épisodes. 

### 1.2 Avantages des Méthodes TD
- **Sans modèle** : Les méthodes TD n'ont pas besoin d'un modèle de transition de l'environnement.
- **Mise à jour en temps réel** : Contrairement aux méthodes de Monte Carlo, elles peuvent mettre à jour les estimations sans attendre la fin d'un épisode.
- **Estimations dynamiques** : Les valeurs d'états sont basées sur des estimations successives, ce qui permet à l'agent d'apprendre plus vite.

---

## 2. Prédiction TD(0)

### 2.1 Objectif de TD(0)
TD(0) est une méthode de prédiction qui estime la valeur d'un état pour une politique donnée. La valeur d'un état est mise à jour en fonction de la récompense immédiate reçue et de la valeur de l'état suivant.

### 2.2 Règle de Mise à Jour TD(0)
La mise à jour de TD(0) pour un état \( s \) est donnée par :

$$ V(s) \leftarrow V(s) + \alpha [R_{t+1} + \gamma V(S_{t+1}) - V(s)] $$

où :
- \( V(s) \) : valeur actuelle de l’état \( s \),
- \( \alpha \) : taux d’apprentissage (paramètre d’ajustement),
- \( R_{t+1} \) : récompense reçue après transition de \( s \) à \( S_{t+1} \),
- \( \gamma \) : facteur d’actualisation (pondère les récompenses futures).

### 2.3 Exemple d'Implémentation de TD(0)
Voici un code Python simple pour estimer \( V(s) \) pour une politique donnée :

```python
import numpy as np

# Définir les états et la politique
states = [0, 1, 2, 3, 4]
policy = {s: np.random.choice(['a', 'b']) for s in states}

# Règle de mise à jour TD(0)
def td_prediction(policy, episodes=100, alpha=0.1, gamma=0.9):
    V = np.zeros(len(states))
    for _ in range(episodes):
        state = np.random.choice(states)
        while state != 4:  # État terminal
            next_state = np.random.choice(states)
            reward = np.random.randn()  # Récompense aléatoire
            V[state] += alpha * (reward + gamma * V[next_state] - V[state])
            state = next_state
    return V

# Exécution de TD(0)
value_function = td_prediction(policy)
print("Fonction de Valeur Estimée :", value_function)
```

---

## 3. Contrôle TD : SARSA et Q-Learning

### 3.1 SARSA : Sur-Politique
SARSA est une méthode de contrôle sur-politique qui met à jour la fonction de valeur d'action en suivant l'action réellement entreprise par la politique actuelle.

**Règle de Mise à Jour SARSA :**
$$ Q(s, a) \leftarrow Q(s, a) + \alpha [R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(s, a)] $$

où :
- \( Q(s, a) \) : valeur de l'action \( a \) dans l'état \( s \),
- \( A_{t+1} \) : prochaine action choisie selon la politique actuelle.

### 3.2 Q-Learning : Hors-Politique
Q-Learning est une méthode de contrôle hors-politique qui met à jour la fonction de valeur d'action en utilisant la valeur maximale de l'état suivant, indépendamment de l'action entreprise.

**Règle de Mise à Jour Q-Learning :**
$$ Q(s, a) \leftarrow Q(s, a) + \alpha [R_{t+1} + \gamma \max_{a'} Q(S_{t+1}, a') - Q(s, a)] $$

---

## 4. Politique Epsilon-Greedy et Dilemme Exploration/Exploitation

### 4.1 Politique Epsilon-Greedy
La politique epsilon-greedy est utilisée pour équilibrer exploration et exploitation :
- Avec une probabilité \( \epsilon \), l'agent choisit une action aléatoire.
- Avec une probabilité \( 1 - \epsilon \), il choisit l'action qui maximise la valeur estimée.

### 4.2 Importance de \( \epsilon \)
- Si \( \epsilon \) est trop faible : l'agent pourrait ne jamais explorer et rester bloqué dans des actions sous-optimales.
- Si \( \epsilon \) est trop élevé : l'agent explorera constamment, sans jamais exploiter les actions optimales.

---

## 5. Implémentation en Python : SARSA et Q-Learning

### 5.1 SARSA (Sur-Politique)

```python
def sarsa(episodes, alpha=0.1, gamma=0.9, epsilon=0.1):
    Q = defaultdict(lambda: np.zeros(len(actions)))
    for _ in range(episodes):
        state = np.random.choice(states)
        action = epsilon_greedy_policy(Q, state, epsilon)
        while state != 4:  # État terminal
            next_state = np.random.choice(states)
            reward = np.random.randn()  # Récompense aléatoire
            next_action = epsilon_greedy_policy(Q, next_state, epsilon)
            Q[state][actions.index(action)] += alpha * (reward + gamma * Q[next_state][actions.index(next_action)] - Q[state][actions.index(action)])
            state, action = next_state, next_action
    return Q

# Exécution de SARSA
Q_sarsa = sarsa(episodes=1000)
print("Valeurs Q Estimées (SARSA) :", Q_sarsa)
```

### 5.2 Q-Learning (Hors-Politique)

```python
def q_learning(episodes, alpha=0.1, gamma=0.9, epsilon=0.1):
    Q = defaultdict(lambda: np.zeros(len(actions)))
    for _ in range(episodes):
        state = np.random.choice(states)
        while state != 4:  # État terminal
            action = epsilon_greedy_policy(Q, state, epsilon)
            next_state = np.random.choice(states)
            reward = np.random.randn()  # Récompense aléatoire
            best_next_action = np.argmax(Q[next_state])
            Q[state][actions.index(action)] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][actions.index(action)])
            state = next_state
    return Q

# Exécution de Q-Learning
Q_q_learning = q_learning(episodes=1000)
print("Valeurs Q Estimées (Q-Learning) :", Q_q_learning)
```

---

## Conclusion

Les méthodes d'apprentissage par différence temporelle permettent d'estimer les valeurs d'état ou d'état-action sans modèle de l'environnement. SARSA et Q-Learning sont deux algorithmes populaires pour le contrôle TD, chacun avec des avantages uniques :
- **SARSA** : suit la politique actuelle et est sensible aux choix d'actions, garantissant que les actions choisies sont compatibles avec la politique.
- **Q-Learning** : utilise l'action optimale, ce qui le rend plus agressif et indépendant de la politique de comportement.

Ces algorithmes, en utilisant une politique epsilon-greedy, permettent à l'agent d'apprendre tout en explorant l'environnement, résolvant ainsi le dilemme entre exploration et exploitation.
