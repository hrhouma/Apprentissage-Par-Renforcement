# Partie 01

| Méthode         | Caractéristiques                                        | Cas d'utilisation                                         | Exemple pratique                                                |
|-----------------|---------------------------------------------------------|-----------------------------------------------------------|------------------------------------------------------------------|
| **TD(0)**       | - Utilise une seule étape pour la mise à jour.<br>- Mise à jour rapide sans attendre la fin de l'épisode.<br>- Précis pour des environnements stables avec peu de dépendances à long terme. | - Situations nécessitant des prédictions immédiates basées sur l'état actuel.<br>- Adapté aux environnements où les états changent rapidement mais restent liés à l'état précédent. | **Recommandation de produits** : Proposer un produit similaire après une première action (ex. clic sur un produit) sans prendre en compte d'autres actions passées. |
| **TD(1)**       | - Utilise deux étapes pour la mise à jour.<br>- Prend en compte des dépendances immédiates supplémentaires.<br>- Approprié pour des environnements avec des dépendances sur une courte période. | - Prévisions à court terme où deux états consécutifs influencent le résultat.<br>- Idéal lorsque l'état actuel dépend directement de la transition précédente. | **Analyse de santé** : Prédire l'effet d'un traitement après un jour en considérant à la fois l'état initial et l'état du jour précédent. |
| **TD(2)**       | - Utilise trois étapes pour la mise à jour.<br>- Plus précis que TD(0) et TD(1), mais nécessite plus de calculs.<br>- Convient aux environnements où l'état actuel dépend de plusieurs transitions passées. | - Utilisation dans des environnements modérément dynamiques nécessitant des prévisions basées sur plusieurs étapes successives. | **Logistique** : Estimer l'impact d'une action (ex. livraison) en tenant compte des conditions des deux jours précédents (ex. embouteillages, météo récente). |
| **TD(n)**       | - Généralise le nombre de transitions prises en compte (paramètre \( n \)).<br>- Converge vers Monte Carlo lorsque \( n \to \infty \).<br>- Plus \( n \) est grand, plus la méthode est précise mais complexe. | - Environnements dynamiques nécessitant des prévisions à long terme.<br>- Utile pour des situations avec des dépendances complexes entre les états. | **Gestion de l'énergie** : Prédire la consommation d'énergie hebdomadaire en tenant compte des habitudes de consommation sur plusieurs jours consécutifs. |
| **SARSA**       | - Méthode sur-politique.<br>- Met à jour la valeur d'une action en suivant la politique actuelle.<br>- Sensible aux changements de la politique d'exploration. | - Environnements où l'exploration est modérée et où la politique est conservatrice.<br>- Idéal pour des situations où l'agent suit une politique stable et préfère éviter les risques. | **Navigation GPS** : Adapter les recommandations de trajet en fonction des préférences conservatrices (ex. éviter les autoroutes) pour un trajet sûr sans changements agressifs. |
| **Q-Learning**  | - Méthode hors-politique.<br>- Met à jour la valeur d'une action en utilisant l'action optimale dans l'état suivant.<br>- Approprié pour des environnements nécessitant de l'exploration pour optimiser les résultats. | - Environnements où l'agent peut explorer et optimiser les actions à long terme.<br>- Convient pour des actions risquées avec des bénéfices à long terme. | **Publicité en ligne** : Tester plusieurs annonces pour trouver celle qui maximise l'engagement utilisateur, en explorant différentes stratégies. |

### Explications des cas d'utilisation et des exemples pratiques :

- **TD(0)** : Idéal pour des situations immédiates où seule la prochaine transition influence directement la décision. Exemple : recommander un produit juste après qu'un utilisateur ait cliqué sur un autre produit similaire.

- **TD(1)** et **TD(2)** : Ces méthodes prennent en compte une ou deux étapes supplémentaires, améliorant la précision pour des environnements légèrement dynamiques où les états successifs ont une influence. Par exemple, en analyse de santé, la réponse d’un patient à un traitement peut être influencée par son état initial et son état du jour précédent.

- **TD(n)** : En utilisant plusieurs étapes, TD(n) convient à des prévisions plus complexes, comme la gestion de l’énergie où l'on tient compte des habitudes de consommation sur plusieurs jours pour mieux prévoir la consommation hebdomadaire.

- **SARSA** : Méthode plus conservatrice, adaptée aux environnements où l’on souhaite éviter des choix risqués. Par exemple, dans la navigation GPS, si l’utilisateur préfère éviter les autoroutes, SARSA peut s’assurer que les trajets recommandés restent dans ces préférences sans explorer des options plus risquées.

- **Q-Learning** : Méthode plus exploratoire et adaptée pour les situations où l’on veut trouver une stratégie optimale. En publicité en ligne, Q-Learning peut tester plusieurs annonces, explorant différentes options pour maximiser l'engagement, en se basant sur les résultats les plus performants.

