{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8605f6",
   "metadata": {},
   "source": [
    "# Apprentissage par différence temporelle (TD) dans l'apprentissage par renforcement\n",
    "\n",
    "L'apprentissage par différence temporelle (TD) est une approche sans modèle dans l'apprentissage par renforcement qui combine des idées issues des méthodes de Monte Carlo et de la programmation dynamique. Elle permet aux agents d'apprendre directement à partir de l'expérience brute sans avoir besoin d'un modèle de la dynamique de l'environnement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941161a5",
   "metadata": {},
   "source": [
    "## Prédiction TD\n",
    "\n",
    "Les méthodes de prédiction TD sont utilisées pour estimer la fonction de valeur $V(s)$ pour une politique donnée $\\pi$. La principale caractéristique des méthodes TD est qu'elles mettent à jour la valeur d'un état en fonction de la valeur estimée de l'état suivant, plutôt que d'attendre le résultat final.\n",
    "\n",
    "### Règle de mise à jour TD(0) :\n",
    "---\n",
    "\n",
    ">$$V(s) \\leftarrow V(s) + \\alpha [R_{t+1} + \\gamma V(S_{t+1}) - V(s)]$$\n",
    "\n",
    "Où :\n",
    "- $V(s)$ est la valeur actuelle de l'état $s$\n",
    "- $\\alpha$ est le taux d'apprentissage\n",
    "- $R_{t+1}$ est la récompense reçue après la transition de l'état $s$ à l'état $S_{t+1}$\n",
    "- $\\gamma$ est le facteur d'actualisation\n",
    "\n",
    "---\n",
    "\n",
    "### Avantages de TD :\n",
    "- Les méthodes TD sont sans modèle, ce qui signifie qu'elles ne nécessitent pas de connaissance des fonctions de transition et de récompense du MDP.\n",
    "- Les méthodes TD peuvent apprendre à partir d'épisodes incomplets, contrairement aux méthodes de Monte Carlo, qui nécessitent des épisodes complets.\n",
    "- Les méthodes TD développent une estimation basée sur une estimation, ce qui leur permet d'apprendre avant de connaître le résultat final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d107068",
   "metadata": {},
   "source": [
    "## Contrôle TD\n",
    "\n",
    "Les méthodes de contrôle TD étendent la prédiction TD pour apprendre des politiques optimales. Elles utilisent des fonctions de valeur pour améliorer la politique de manière itérative.\n",
    "\n",
    "### Politique de mise à jour et politique de comportement :\n",
    "- **Politique de mise à jour** : la politique utilisée pour mettre à jour la fonction de valeur.\n",
    "- **Politique de comportement** : la politique utilisée pour générer le comportement (actions) de l'agent.\n",
    "\n",
    "### Sur politique vs. Hors politique :\n",
    "- **Sur politique** : l'agent suit une politique unique pour mettre à jour la fonction de valeur et générer le comportement.\n",
    "- **Hors politique** : l'agent utilise des politiques différentes pour mettre à jour la fonction de valeur et générer le comportement.\n",
    "\n",
    "### Exemple sur politique : SARSA\n",
    "SARSA (État-Action-Récompense-État-Action) est un algorithme de contrôle TD sur politique. Il met à jour la valeur Q en fonction de l'action réellement effectuée par la politique actuelle.\n",
    "\n",
    "### Exemple hors politique : Q-Learning\n",
    "Q-Learning est un algorithme de contrôle TD hors politique. Il met à jour la valeur Q en fonction de la valeur Q maximale possible de l'état suivant, indépendamment de l'action entreprise par la politique actuelle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043149e",
   "metadata": {},
   "source": [
    "## SARSA\n",
    "\n",
    "SARSA est un algorithme de contrôle TD sur politique qui met à jour la fonction de valeur d'action en fonction de la politique actuelle.\n",
    "\n",
    "### Règle de mise à jour SARSA :\n",
    "---\n",
    "\n",
    ">$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(s, a)]$$\n",
    "\n",
    "Où :\n",
    "- $Q(s, a)$ est la valeur actuelle de l'action $a$ dans l'état $s$\n",
    "- $\\alpha$ est le taux d'apprentissage\n",
    "- $R_{t+1}$ est la récompense reçue après avoir effectué l'action $a$ dans l'état $s$\n",
    "- $\\gamma$ est le facteur d'actualisation\n",
    "- $S_{t+1}$ est l'état suivant\n",
    "- $A_{t+1}$ est la prochaine action effectuée par la politique actuelle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf9f12f",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Q-Learning est un algorithme de contrôle TD hors politique qui met à jour la fonction de valeur d'action en fonction de la valeur Q maximale possible de l'état suivant.\n",
    "\n",
    "### Règle de mise à jour de l'apprentissage Q :\n",
    "---\n",
    "\n",
    ">$$Q(s, a) \\leftarrow Q(s, a) + \\alpha [R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a') - Q(s, a)]$$\n",
    "\n",
    "Où :\n",
    "- $Q(s, a)$ est la valeur actuelle de l'action $a$ dans l'état $s$\n",
    "- $\\alpha$ est le taux d'apprentissage\n",
    "- $R_{t+1}$ est la récompense reçue après avoir effectué l'action $a$ dans l'état $s$\n",
    "- $\\gamma$ est le facteur d'actualisation\n",
    "- $S_{t+1}$ est l'état suivant\n",
    "- $\\max_{a'} Q(S_{t+1}, a')$ est la valeur Q maximale de l'état suivant sur toutes les actions possibles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24fcda2b",
   "metadata": {},
   "source": [
    "## Exemple d'implémentation de SARSA et Q-Learning en Python\n",
    "\n",
    "Vous trouverez ci-dessous un exemple d'implémentation des algorithmes SARSA et Q-Learning en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca5608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Q-Values (SARSA):\n",
      "Q(4, a) = 0.00\n",
      "Q(4, b) = 0.00\n",
      "Q(3, a) = -0.05\n",
      "Q(3, b) = -0.35\n",
      "Q(0, a) = -0.26\n",
      "Q(0, b) = 0.17\n",
      "Q(1, a) = -0.08\n",
      "Q(1, b) = -0.04\n",
      "Q(2, a) = -0.15\n",
      "Q(2, b) = -0.27\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "# Définir l'environnement et la politique\n",
    "states = [0, 1, 2, 3, 4]\n",
    "actions = ['a', 'b']\n",
    "\n",
    "# Simuler un environnement\n",
    "def generate_episode(policy):\n",
    "    episode = []\n",
    "    state = np.random.choice(states)\n",
    "    while state != 4:  # Terminal state\n",
    "        action = policy[state]\n",
    "        next_state = np.random.choice(states)\n",
    "        reward = np.random.randn()  # Random reward\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# La politique cupide d'Epsilon\n",
    "def epsilon_greedy_policy(Q, state, epsilon=0.1):\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.choice(actions)\n",
    "    else:\n",
    "        return actions[np.argmax(Q[state])]\n",
    "\n",
    "# Algorithme SARSA\n",
    "def sarsa(episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(len(actions)))\n",
    "    for _ in range(episodes):\n",
    "        state = np.random.choice(states)\n",
    "        action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "        while state != 4:  # Terminal state\n",
    "            next_state = np.random.choice(states)\n",
    "            reward = np.random.randn()  # Random reward\n",
    "            next_action = epsilon_greedy_policy(Q, next_state, epsilon)\n",
    "            Q[state][actions.index(action)] += alpha * (reward + gamma * Q[next_state][actions.index(next_action)] - Q[state][actions.index(action)])\n",
    "            state, action = next_state, next_action\n",
    "    return Q\n",
    "\n",
    "# Exécutez SARSA\n",
    "Q_sarsa = sarsa(episodes=1000)\n",
    "\n",
    "print(\"Estimated Q-Values (SARSA):\")\n",
    "for state, values in Q_sarsa.items():\n",
    "    for action, value in zip(actions, values):\n",
    "        print(f\"Q({state}, {action}) = {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f1cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated Q-Values (Q-Learning):\n",
      "Q(0, a) = 0.05\n",
      "Q(0, b) = 0.19\n",
      "Q(4, a) = 0.00\n",
      "Q(4, b) = 0.00\n",
      "Q(2, a) = -0.12\n",
      "Q(2, b) = 0.12\n",
      "Q(3, a) = -0.03\n",
      "Q(3, b) = -0.14\n",
      "Q(1, a) = -0.19\n",
      "Q(1, b) = 0.23\n"
     ]
    }
   ],
   "source": [
    "# Algorithme Q-Learning\n",
    "def q_learning(episodes, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(len(actions)))\n",
    "    for _ in range(episodes):\n",
    "        state = np.random.choice(states)\n",
    "        while state != 4:  # Terminal state\n",
    "            action = epsilon_greedy_policy(Q, state, epsilon)\n",
    "            next_state = np.random.choice(states)\n",
    "            reward = np.random.randn()  # Random reward\n",
    "            best_next_action = np.argmax(Q[next_state])\n",
    "            Q[state][actions.index(action)] += alpha * (reward + gamma * Q[next_state][best_next_action] - Q[state][actions.index(action)])\n",
    "            state = next_state\n",
    "    return Q\n",
    "\n",
    "# Exécutez Q-Learning\n",
    "Q_q_learning = q_learning(episodes=1000)\n",
    "\n",
    "print(\"Estimated Q-Values (Q-Learning):\")\n",
    "for state, values in Q_q_learning.items():\n",
    "    for action, value in zip(actions, values):\n",
    "        print(f\"Q({state}, {action}) = {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e48105",
   "metadata": {},
   "source": [
    "## Explication de l'implémentation de l'apprentissage par différence temporelle\n",
    "\n",
    "### SARSA\n",
    "- **Initialisation** : nous initialisons les valeurs Q pour toutes les paires état-action.\n",
    "- **Générer des épisodes** : l'agent suit une politique epsilon-greedy pour générer des épisodes.\n",
    "- **Mettre à jour les valeurs Q** : les valeurs Q sont mises à jour en fonction de l'action réelle entreprise par la politique.\n",
    "\n",
    "### Apprentissage Q\n",
    "- **Initialisation** : nous initialisons les valeurs Q pour toutes les paires état-action.\n",
    "- **Générer des épisodes** : l'agent suit une politique epsilon-greedy pour générer des épisodes.\n",
    "- **Mettre à jour les valeurs Q** : les valeurs Q sont mises à jour en fonction de la valeur Q maximale de l'état suivant, indépendamment de l'action entreprise par la politique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8640e0a4",
   "metadata": {},
   "source": [
    "Les valeurs Q estimées fournissent le rendement attendu pour chaque paire état-action dans le cadre de la politique donnée. L'algorithme SARSA met à jour les valeurs Q en fonction de l'action réellement entreprise par la politique, tandis que l'algorithme Q-Learning met à jour les valeurs Q en fonction de la valeur Q maximale possible de l'état suivant."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
