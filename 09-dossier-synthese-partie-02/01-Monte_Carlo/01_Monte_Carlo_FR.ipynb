{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79c3db50",
   "metadata": {},
   "source": [
    "# Méthodes de Monte Carlo dans l'apprentissage par renforcement\n",
    "\n",
    "Les méthodes de Monte Carlo sont une classe d'algorithmes qui s'appuient sur un échantillonnage aléatoire répété pour obtenir des résultats numériques. Dans le contexte de l'apprentissage par renforcement, les méthodes de Monte Carlo sont utilisées pour estimer la valeur des états ou des paires état-action en fonction des épisodes observés."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f345213",
   "metadata": {},
   "source": [
    "## Prédiction de Monte Carlo\n",
    "\n",
    "Les méthodes de prédiction de Monte Carlo sont utilisées pour estimer la fonction de valeur \\( V(s) \\) pour une politique donnée $\\pi$ en fonction du rendement moyen de plusieurs épisodes. La valeur d'un état est mise à jour en faisant la moyenne des rendements observés après avoir visité cet état.\n",
    "\n",
    "### Processus :\n",
    "1. **Générer des épisodes** : exécuter la politique $\\pi$ pour générer plusieurs épisodes.\n",
    "2. **Calculer les rendements** : pour chaque état de chaque épisode, calculer le rendement (récompense cumulée) de cet état jusqu'à la fin de l'épisode.\n",
    "3. **Mettre à jour la fonction de valeur** : faire la moyenne des rendements de chaque état sur plusieurs épisodes pour estimer la fonction de valeur.\n",
    "\n",
    "### Méthodes :\n",
    "- **Every-Visit** : met à jour la fonction de valeur en utilisant la moyenne de tous les rendements observés à chaque fois qu'un état est visité.\n",
    "- **First-Visit** : met à jour la fonction de valeur en utilisant la moyenne des rendements observés uniquement la première fois qu'un état est visité dans chaque épisode.\n",
    "\n",
    "### Équation :\n",
    "---\n",
    "\n",
    "- ​​**Chaque visite** :\n",
    ">$$V(s) \\approx \\frac{1}{N(s)} \\sum_{i=1}^{N(s)} G_i(s)$$\n",
    "\n",
    "- **Première visite** :\n",
    ">$$V(s) \\approx \\frac{1}{N_{\\text{first}}(s)} \\sum_{i=1}^{N_{\\text{first}}(s)} G_i(s)$$\n",
    "\n",
    "Où :\n",
    "- $N(s)$ est le nombre de fois que l'état $s$ a été visité\n",
    "- $N_{\\text{first}}(s)$ est le nombre de fois que l'état $s$ a été visité pour la première fois dans chaque épisode, et $G_i(s)$ est le retour observé de l'état $s$ dans le $i$-ième épisode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60695251",
   "metadata": {},
   "source": [
    "## Contrôle Monte Carlo\n",
    "\n",
    "Les méthodes de contrôle Monte Carlo sont utilisées pour trouver la politique optimale $pi^*$ en apprenant à partir des épisodes générés par la politique actuelle. Il existe deux approches principales : l'apprentissage sur politique et l'apprentissage hors politique.\n",
    "\n",
    "### Contrôle Monte Carlo sur politique\n",
    "\n",
    "Les méthodes sur politique mettent à jour la politique en fonction des actions entreprises par la politique actuelle. La politique est améliorée de manière itérative à l'aide de la fonction de valeur d'action estimée $Q(s, a)$.\n",
    "\n",
    "### Contrôle Monte Carlo hors politique\n",
    "\n",
    "Les méthodes hors politique apprennent la valeur de la politique optimale $\\pi^*$ tout en suivant une politique de comportement différente $\\mu$. Cette approche utilise l'échantillonnage d'importance pour corriger la différence entre la politique de comportement et la politique cible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b700ba",
   "metadata": {},
   "source": [
    "## Apprentissage hors politique et sur politique\n",
    "\n",
    "### Apprentissage sur politique\n",
    "Dans l'apprentissage sur politique, la politique utilisée pour générer les épisodes est la même que la politique en cours d'amélioration. La fonction de valeur d'action $Q(s, a)$ est mise à jour à l'aide des retours observés à partir des épisodes générés par la politique actuelle.\n",
    "\n",
    "### Apprentissage hors politique\n",
    "Dans l'apprentissage hors politique, la politique de comportement $\\mu$ est différente de la politique cible $\\pi$. La fonction de valeur d'action $Q(s, a)$ est mise à jour à l'aide de l'échantillonnage d'importance pour corriger la différence entre la politique de comportement et la politique cible.\n",
    "\n",
    "### Échantillonnage d'importance\n",
    "L'échantillonnage d'importance est utilisé pour peser les retours observés à partir de la politique de comportement \\( \\mu \\) afin d'estimer les retours de la politique cible \\( \\pi \\).\n",
    "\n",
    "### Équation :\n",
    "---\n",
    "\n",
    ">$$Q(s, a) \\leftarrow Q(s, a) + \\alpha \\cdot \\frac{\\pi(a|s)}{\\mu(a|s)} \\left( G - Q(s, a) \\right)$$\n",
    "\n",
    "Où :\n",
    "- $G$ est le retour observé à partir de l'épisode \n",
    "- $\\alpha$ est le taux d'apprentissage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc18c6f",
   "metadata": {},
   "source": [
    "## Politique Epsilon-Greedy\n",
    "\n",
    "La politique epsilon-greedy est une méthode permettant d'équilibrer l'exploration et l'exploitation. Elle garantit que l'agent explore l'environnement en choisissant des actions aléatoires avec une probabilité $\\epsilon$ et exploite les actions les plus connues avec une probabilité $1 - \\epsilon$.\n",
    "\n",
    "### Algorithme de la politique Epsilon-Greedy\n",
    "\n",
    "1. **Initialisation** : définissez $\\epsilon$ (taux d'exploration).\n",
    "2. **Sélection d'action** :\n",
    "- Avec une probabilité $\\epsilon$, choisissez une action aléatoire.\n",
    "- Avec une probabilité $1 - \\epsilon$, choisissez l'action qui maximise la valeur estimée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005c5f7e",
   "metadata": {},
   "source": [
    "## Algorithme de Monte Carlo\n",
    "\n",
    "L'algorithme de Monte Carlo consiste à générer des épisodes, à calculer les rendements et à mettre à jour les estimations de valeur en fonction des moyennes empiriques.\n",
    "\n",
    "### Pseudo-code :\n",
    "\n",
    "1. Initialiser la fonction de valeur $V(s)$ de manière arbitraire\n",
    "2. Pour chaque épisode :\n",
    "- Générer l'épisode en utilisant la politique $\\pi$\n",
    "- Calculer le rendement $G$ à partir de l'état $s$\n",
    "- Mettre à jour $V(s)$ en utilisant la moyenne empirique des rendements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7439c4a2",
   "metadata": {},
   "source": [
    "### Algorithme de contrôle de Monte Carlo (sur politique) :\n",
    "\n",
    "1. Initialiser $Q(s, a)$ arbitrairement et $\\pi$ pour qu'il soit epsilon-greedy\n",
    "2. Pour chaque épisode :\n",
    "- Générer l'épisode en utilisant la politique $\\pi$\n",
    "- Pour chaque paire état-action $(s, a)$ dans l'épisode :\n",
    "- Calculer le rendement $G$ à partir de $(s, a)$\n",
    "- Mettre à jour $Q(s, a)$ en utilisant la moyenne empirique des rendements\n",
    "- Mettre à jour la politique $\\pi$ pour qu'elle soit gourmande par rapport à $Q$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c62fa7",
   "metadata": {},
   "source": [
    "## Exemple d'implémentation de méthodes de Monte Carlo en Python\n",
    "\n",
    "Vous trouverez ci-dessous un exemple d'implémentation de méthodes de prédiction et de contrôle de Monte Carlo en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0cd95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated State-Value Function:\n",
      "V(3) = -0.03\n",
      "V(1) = 0.11\n",
      "V(2) = 0.11\n",
      "V(0) = 0.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "# Définir l'environnement et la politique\n",
    "states = [0, 1, 2, 3, 4]\n",
    "actions = ['a', 'b']\n",
    "policy = {s: np.random.choice(actions) for s in states}\n",
    "\n",
    "# Simuler un environnement\n",
    "def generate_episode(policy):\n",
    "    episode = []\n",
    "    state = np.random.choice(states)\n",
    "    while state != 4:  # Terminal state\n",
    "        action = policy[state]\n",
    "        next_state = np.random.choice(states)\n",
    "        reward = np.random.randn()  # Random reward\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "    return episode\n",
    "\n",
    "# Prédiction de Monte Carlo (première visite)\n",
    "def monte_carlo_prediction_first_visit(policy, episodes, gamma=0.9):\n",
    "    V = defaultdict(float)\n",
    "    returns = defaultdict(list)\n",
    "    for _ in range(episodes):\n",
    "        episode = generate_episode(policy)\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, _, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if state not in visited:\n",
    "                visited.add(state)\n",
    "                returns[state].append(G)\n",
    "                V[state] = np.mean(returns[state])\n",
    "    return V\n",
    "\n",
    "# Prédiction de la course de Monte Carlo (première visite)\n",
    "value_function = monte_carlo_prediction_first_visit(policy, episodes=1000)\n",
    "\n",
    "print(\"Estimated State-Value Function:\")\n",
    "for state, value in value_function.items():\n",
    "    print(f\"V({state}) = {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106a23cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy:\n",
      "State 0: a\n",
      "State 1: b\n",
      "State 2: b\n",
      "State 3: a\n",
      "State 4: b\n",
      "\n",
      "Estimated Action-Value Function:\n",
      "Q(2, a) = -0.50\n",
      "Q(2, b) = -0.06\n",
      "Q(0, a) = 0.12\n",
      "Q(0, b) = -1.13\n",
      "Q(1, a) = -2.84\n",
      "Q(1, b) = 0.33\n",
      "Q(3, a) = -0.33\n",
      "Q(3, b) = -0.80\n"
     ]
    }
   ],
   "source": [
    "# Contrôle de Monte-Carlo (sur politique, à chaque visite)\n",
    "def monte_carlo_control_on_policy(episodes, gamma=0.9, epsilon=0.1):\n",
    "    Q = defaultdict(lambda: np.zeros(len(actions)))\n",
    "    policy = {s: np.random.choice(actions) for s in states}\n",
    "\n",
    "    def epsilon_greedy_policy(state):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(actions)\n",
    "        else:\n",
    "            return actions[np.argmax(Q[state])]\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        episode = []\n",
    "        state = np.random.choice(states)\n",
    "        while state != 4:  # Terminal state\n",
    "            action = epsilon_greedy_policy(state)\n",
    "            next_state = np.random.choice(states)\n",
    "            reward = np.random.randn()  # Random reward\n",
    "            episode.append((state, action, reward))\n",
    "            state = next_state\n",
    "\n",
    "        G = 0\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, reward = episode[t]\n",
    "            G = gamma * G + reward\n",
    "            if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
    "                Q[state][actions.index(action)] += (G - Q[state][actions.index(action)]) / len(episode)\n",
    "                policy[state] = actions[np.argmax(Q[state])]\n",
    "                \n",
    "    return policy, Q\n",
    "\n",
    "# Exécuter le contrôle Monte Carlo (selon la politique, à chaque visite)\n",
    "optimal_policy, action_value_function = monte_carlo_control_on_policy(episodes=1000)\n",
    "\n",
    "print(\"\\nOptimal Policy:\")\n",
    "for state, action in optimal_policy.items():\n",
    "    print(f\"State {state}: {action}\")\n",
    "\n",
    "print(\"\\nEstimated Action-Value Function:\")\n",
    "for state, values in action_value_function.items():\n",
    "    for action, value in zip(actions, values):\n",
    "        print(f\"Q({state}, {action}) = {value:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b96752",
   "metadata": {},
   "source": [
    "La fonction valeur-état estimée fournit le rendement attendu pour chaque état dans le cadre de la politique donnée. La politique optimale montre la meilleure action à entreprendre dans chaque état en fonction de la fonction valeur-action apprise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
