# Tutoriel Interactif : TD-Learning avec Python

### Contexte
**Objectif :** Apprendre √† impl√©menter l'algorithme de TD-Learning en Python pour r√©soudre un probl√®me d'apprentissage par renforcement. Dans ce tutoriel, nous allons travailler √©tape par √©tape pour explorer les bases du TD-Learning, v√©rifier notre compr√©hension √† chaque √©tape, et relever des d√©fis pratiques.

**√âtapes √† suivre :**
1. **Introduction au TD-Learning**
2. **Exploration du concept de valeur d'√©tat**
3. **Mise en place de l'environnement et de la fonction de r√©compense**
4. **Impl√©mentation de la mise √† jour TD**
5. **Visualisation de l'√©volution des valeurs**
6. **√âvaluation et ajustement des param√®tres**
7. **Conclusion et nettoyage**

Chaque √©tape est con√ßue pour √™tre interactive et engageante, avec des questions et des explications d√©taill√©es.

---

### √âtape 1 : Introduction au TD-Learning

Le **TD-Learning** est une m√©thode d'apprentissage par renforcement qui met √† jour les valeurs d'√©tat en se basant sur la diff√©rence temporelle entre l'√©tat actuel et l'√©tat suivant.

**Pourquoi TD-Learning ?** Il combine les concepts de Monte Carlo (√©valuation par exp√©riences compl√®tes) et d'it√©ration de valeur (calcul des valeurs √† partir d'√©tats voisins) pour obtenir un apprentissage plus rapide et adaptatif.

#### üìù Question : Qu‚Äôest-ce qu'une "diff√©rence temporelle" ?
- **R√©ponse attendue :** Une diff√©rence temporelle repr√©sente l'√©cart entre la valeur actuelle d'un √©tat et la valeur estim√©e apr√®s avoir observ√© la r√©compense obtenue en suivant une action.

---

### √âtape 2 : Exploration du Concept de Valeur d'√âtat

Pour chaque √©tat, TD-Learning cherche √† estimer sa "valeur" en fonction des r√©compenses futures possibles.

#### Code √† ex√©cuter

```python
# Initialisation des valeurs des √©tats
import numpy as np

n_states = 5  # Nombre d'√©tats dans notre environnement
values = np.zeros(n_states)  # Initialisation des valeurs des √©tats √† 0
gamma = 0.9  # Facteur d'escompte

print("Valeurs initiales des √©tats :", values)
```

#### Explication d√©taill√©e
- **`n_states` :** D√©termine le nombre d'√©tats dans notre environnement.
- **`gamma` :** Le facteur d'escompte qui r√©duit la valeur des r√©compenses futures.

#### üöÄ D√©fi : Pourquoi fixer `gamma` √† 0.9 ?
- **Indice :** Imaginez que gamma repr√©sente l'importance accord√©e aux r√©compenses futures.
- **R√©ponse :** Un gamma de 0.9 donne de l'importance aux r√©compenses futures sans les rendre dominantes. Si gamma = 1, toutes les r√©compenses futures seraient aussi importantes que la r√©compense imm√©diate.

---

### √âtape 3 : Mise en Place de l'Environnement et de la Fonction de R√©compense

Nous allons cr√©er un environnement simple et d√©finir une fonction de r√©compense.

#### Code √† ex√©cuter

```python
# Fonction de r√©compense
rewards = np.array([-1, 0, -1, 0, 1])  # Exemples de r√©compenses pour chaque √©tat

# Probabilit√©s de transition (simples, chaque action am√®ne √† un nouvel √©tat)
transitions = {
    0: 1,
    1: 2,
    2: 3,
    3: 4,
    4: 4  # √âtat terminal
}

print("R√©compenses par √©tat :", rewards)
```

#### Explication d√©taill√©e
- **R√©compenses** : d√©finissent la valeur imm√©diate qu'un agent re√ßoit lorsqu'il atteint chaque √©tat.
- **Transitions** : indiquent le passage probable d‚Äôun √©tat √† un autre.

**Questions pour les √©tudiants** :
1. **Pourquoi l‚Äô√©tat terminal n‚Äôa pas de transition vers un autre √©tat ?**
   - **R√©ponse :** Un √©tat terminal signifie que le processus s'arr√™te, et l'agent n'a plus d'actions possibles apr√®s l'atteindre.

---

### √âtape 4 : Impl√©mentation de la Mise √† Jour TD

C'est ici que la magie du TD-Learning op√®re ! Nous allons mettre √† jour les valeurs d'√©tat en utilisant la formule de diff√©rence temporelle.

#### Code √† ex√©cuter

```python
alpha = 0.1  # Taux d'apprentissage

# Mise √† jour des valeurs avec la r√®gle de TD
def td_update(state, next_state):
    td_target = rewards[state] + gamma * values[next_state]
    td_error = td_target - values[state]
    values[state] += alpha * td_error
    return td_error

# Ex√©cution de quelques √©tapes d'apprentissage
n_episodes = 10
for episode in range(n_episodes):
    for state in range(n_states - 1):  # Sauf l'√©tat terminal
        next_state = transitions[state]
        td_error = td_update(state, next_state)
        print(f"√âpisode {episode + 1}, √âtat {state}, TD Error : {td_error}")
print("Valeurs mises √† jour des √©tats :", values)
```

#### Explication d√©taill√©e
- **`alpha` :** Contr√¥le la vitesse √† laquelle les valeurs d'√©tat sont mises √† jour.
- **`td_target` et `td_error` :** Repr√©sentent respectivement la cible d'apprentissage et l'erreur entre la pr√©diction actuelle et la cible.

#### üöÄ D√©fi : Pourquoi calculons-nous une "TD Error" ?
- **Indice :** Pensez √† ce que cette erreur nous indique sur la pr√©cision de nos valeurs d'√©tat.
- **R√©ponse :** La TD Error nous indique √† quel point notre estimation actuelle de la valeur d'√©tat est incorrecte, nous permettant d'ajuster cette valeur progressivement.

---

### √âtape 5 : Visualisation de l'√âvolution des Valeurs

Nous allons maintenant visualiser comment les valeurs des √©tats changent au fil des √©pisodes.

#### Code √† ex√©cuter

```python
import matplotlib.pyplot as plt

# Affichage de l'√©volution des valeurs d'√©tat
plt.plot(values, marker='o')
plt.xlabel("√âtats")
plt.ylabel("Valeur estim√©e")
plt.title("Valeur des √©tats apr√®s apprentissage TD")
plt.show()
```

#### Explication d√©taill√©e
Cette visualisation montre les valeurs apprises pour chaque √©tat. Des valeurs plus √©lev√©es indiquent une meilleure "r√©compense attendue".

**Question pour les √©tudiants :**
- **Pourquoi certaines valeurs sont-elles plus √©lev√©es que d‚Äôautres ?**
   - **R√©ponse :** Les √©tats avec des r√©compenses imm√©diates ou proches d‚Äô√©tats positifs ont des valeurs plus √©lev√©es car ils promettent une r√©compense plus importante.

---

### √âtape 6 : √âvaluation et Ajustement des Param√®tres

Il est important de tester diff√©rents param√®tres (comme `alpha` et `gamma`) pour voir comment ils influencent l‚Äôapprentissage. Essayez de modifier `alpha` et `gamma`, puis relancez l‚Äôapprentissage pour voir l‚Äôimpact.

#### Code √† ex√©cuter (modification de `alpha` et `gamma`)

```python
alpha = 0.5
gamma = 0.8

# R√©initialisation des valeurs des √©tats
values = np.zeros(n_states)

# Ex√©cution de l‚Äôapprentissage avec les nouveaux param√®tres
for episode in range(n_episodes):
    for state in range(n_states - 1):
        next_state = transitions[state]
        td_error = td_update(state, next_state)
print("Valeurs mises √† jour avec nouveaux param√®tres :", values)
```

#### üöÄ D√©fi : Quel est l‚Äôeffet d‚Äôun `alpha` plus √©lev√© ?
- **Indice :** Un alpha √©lev√© augmente la vitesse de mise √† jour des valeurs.
- **R√©ponse :** Un `alpha` √©lev√© signifie que le mod√®le ajuste les valeurs d‚Äô√©tat plus rapidement, mais cela peut aussi entra√Æner de l‚Äôinstabilit√©.

---

### √âtape 7 : Conclusion et Nettoyage

Nous avons impl√©ment√© le TD-Learning et explor√© son comportement pour diff√©rents param√®tres. Si vous utilisez un environnement cloud, assurez-vous de fermer toutes les sessions pour √©viter des co√ªts.

**Questions pour les √©tudiants :**
1. **Quelles sont les prochaines √©tapes pour am√©liorer ce mod√®le TD-Learning ?**
   - **R√©ponse :** Explorer des environnements plus complexes, tester des valeurs de `alpha` et `gamma`, ou introduire des politiques d‚Äôexploration comme Œµ-greedy pour optimiser le comportement de l‚Äôagent.

2. **Quelle diff√©rence principale y a-t-il entre le TD-Learning et Monte Carlo ?**
   - **R√©ponse :** TD-Learning met √† jour les valeurs apr√®s chaque transition (sans attendre la fin d'un √©pisode), tandis que Monte Carlo met √† jour les valeurs seulement √† la fin de chaque √©pisode.
