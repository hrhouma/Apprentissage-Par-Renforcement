# Tutoriel Interactif : Algorithme SARSA pour l'Apprentissage par Renforcement

### Contexte
**Objectif :** Apprendre et impl√©menter l‚Äôalgorithme SARSA, un algorithme de contr√¥le pour l'apprentissage par renforcement. SARSA met √† jour les valeurs d‚Äôaction en utilisant une approche on-policy, o√π l‚Äôaction suivante est s√©lectionn√©e en suivant la m√™me politique d‚Äôapprentissage.

**√âtapes √† suivre :**
1. **Introduction √† SARSA**
2. **Cr√©ation de l‚Äôenvironnement et de la fonction de r√©compense**
3. **D√©finition de la politique Œµ-greedy**
4. **Impl√©mentation de l‚Äôapprentissage SARSA**
5. **Visualisation de la convergence des valeurs d'action**
6. **Analyse des r√©sultats et conclusion**

Chaque √©tape inclut une explication, du code, des questions pour tester votre compr√©hension, et des d√©fis pour renforcer l'apprentissage.

---

### √âtape 1 : Introduction √† SARSA

SARSA est un algorithme de contr√¥le en apprentissage par renforcement qui met √† jour la **valeur des actions**. Contrairement √† Q-learning, SARSA est un algorithme on-policy, ce qui signifie qu‚Äôil apprend en suivant la politique actuelle.

**Pourquoi SARSA ?** Cet algorithme prend en compte les actions r√©ellement ex√©cut√©es par l'agent, ce qui est utile pour des environnements o√π l‚Äôagent suit une politique non-greedy (exploratrice) pendant l'apprentissage.

#### üìù Question : Que signifie "on-policy" dans le contexte de SARSA ?
- **R√©ponse attendue :** On-policy signifie que l‚Äôalgorithme utilise et met √† jour les valeurs en suivant la m√™me politique avec laquelle il explore l'environnement.

---

### √âtape 2 : Cr√©ation de l'Environnement et de la Fonction de R√©compense

Commen√ßons par d√©finir un environnement simple avec des √©tats et des r√©compenses.

#### Code √† ex√©cuter

```python
import numpy as np

n_states = 5
n_actions = 2  # Deux actions possibles : avancer ou rester
rewards = np.array([-1, 0, -1, 0, 1])  # R√©compenses pour chaque √©tat

# Initialisation de la table de Q-valeurs (√©tat, action)
Q = np.zeros((n_states, n_actions))

print("Table de Q-valeurs initiale :\n", Q)
```

#### Explication d√©taill√©e
- **`n_actions` :** Nous avons deux actions pour simplifier : `0` (avancer) et `1` (rester).
- **Table de Q-valeurs :** La table `Q` contient les valeurs des paires √©tat-action.

**Questions pour les √©tudiants** :
1. **Pourquoi y a-t-il deux actions m√™me si l‚Äôenvironnement semble lin√©aire ?**
   - **R√©ponse :** La pr√©sence de plusieurs actions permet √† l'agent de prendre diff√©rentes d√©cisions dans chaque √©tat, introduisant une variabilit√© dans le chemin.

---

### √âtape 3 : D√©finition de la Politique Œµ-greedy

Pour que SARSA soit un algorithme on-policy, nous d√©finissons une politique Œµ-greedy, o√π l‚Äôagent explore avec probabilit√© Œµ.

#### Code √† ex√©cuter

```python
epsilon = 0.1  # Taux d'exploration

def epsilon_greedy_policy(state, Q, epsilon=0.1):
    if np.random.rand() < epsilon:
        return np.random.choice(n_actions)  # Action al√©atoire (exploration)
    else:
        return np.argmax(Q[state])  # Meilleure action selon Q (exploitation)

# Test de la politique pour un √©tat donn√©
state = 2
action = epsilon_greedy_policy(state, Q, epsilon)
print(f"Action choisie pour l'√©tat {state} : {action}")
```

#### Explication d√©taill√©e
- **`epsilon_greedy_policy` :** Cette fonction retourne une action exploratoire ou la meilleure action (exploitante) selon `Q`.
- **Exploration vs Exploitation** : La politique Œµ-greedy √©quilibre exploration et exploitation.

**Questions pour les √©tudiants** :
1. **Pourquoi l‚Äôagent explore-t-il avec probabilit√© Œµ ?**
   - **R√©ponse :** L‚Äôexploration permet √† l‚Äôagent de d√©couvrir de nouvelles actions qui pourraient √™tre meilleures, tandis que l‚Äôexploitation maximise les r√©compenses imm√©diates.

---

### √âtape 4 : Impl√©mentation de l'Apprentissage SARSA

Nous allons maintenant impl√©menter l‚Äôapprentissage SARSA. L'agent met √† jour la Q-valeur de la paire √©tat-action courante en fonction de la prochaine action choisie par la politique.

#### Code √† ex√©cuter

```python
alpha = 0.1  # Taux d'apprentissage
gamma = 0.9  # Facteur d'escompte
n_episodes = 100

def sarsa_update(state, action, reward, next_state, next_action, Q, alpha, gamma):
    td_target = reward + gamma * Q[next_state, next_action]
    td_error = td_target - Q[state, action]
    Q[state, action] += alpha * td_error
    return Q

# Apprentissage SARSA sur plusieurs √©pisodes
for episode in range(n_episodes):
    state = 0  # L'√©pisode commence toujours √† l'√©tat initial
    action = epsilon_greedy_policy(state, Q, epsilon)
    
    while state < n_states - 1:  # Jusqu'√† l'√©tat terminal
        next_state = state + 1 if action == 0 else state  # Transition simple
        reward = rewards[state]
        next_action = epsilon_greedy_policy(next_state, Q, epsilon)
        
        # Mise √† jour de la Q-valeur
        Q = sarsa_update(state, action, reward, next_state, next_action, Q, alpha, gamma)
        
        # Passage √† l'√©tat et action suivants
        state, action = next_state, next_action

print("Table de Q-valeurs apr√®s apprentissage :\n", Q)
```

#### Explication d√©taill√©e
- **`sarsa_update` :** Fonction de mise √† jour SARSA utilisant la prochaine action choisie.
- **Mise √† jour Q** : Contrairement √† Q-learning, SARSA utilise l‚Äôaction s√©lectionn√©e par la politique pour calculer le target (td_target).

#### üöÄ D√©fi : Pourquoi SARSA utilise-t-il l‚Äôaction s√©lectionn√©e et non la meilleure action suivante ?
- **Indice :** SARSA est un algorithme on-policy.
- **R√©ponse :** SARSA suit la m√™me politique exploratrice pour s√©lectionner et √©valuer l'action suivante, capturant l'impact de la politique actuelle.

---

### √âtape 5 : Visualisation de la Convergence des Valeurs d'Action

Nous allons maintenant visualiser les Q-valeurs pour voir si elles convergent au fil des √©pisodes.

#### Code √† ex√©cuter

```python
import matplotlib.pyplot as plt

all_q_values = []

# R√©initialisation de Q et ex√©cution de l'apprentissage pour collecte des donn√©es
Q = np.zeros((n_states, n_actions))

for episode in range(n_episodes):
    state = 0
    action = epsilon_greedy_policy(state, Q, epsilon)
    episode_q_values = []
    
    while state < n_states - 1:
        next_state = state + 1 if action == 0 else state
        reward = rewards[state]
        next_action = epsilon_greedy_policy(next_state, Q, epsilon)
        
        Q = sarsa_update(state, action, reward, next_state, next_action, Q, alpha, gamma)
        episode_q_values.append(Q.copy())
        
        state, action = next_state, next_action
    
    all_q_values.append(Q.copy())

# Affichage des valeurs pour chaque √©tat-action au fil des √©pisodes
for action in range(n_actions):
    for state in range(n_states):
        plt.plot([q[state, action] for q in all_q_values], label=f"√âtat {state}, Action {action}")
plt.xlabel("√âpisodes")
plt.ylabel("Q-valeur")
plt.title("Convergence des Q-valeurs avec SARSA")
plt.legend()
plt.show()
```

#### Explication d√©taill√©e
Ce graphique montre comment les valeurs d'action √©voluent et convergent pour chaque √©tat au fil des √©pisodes.

**Questions pour les √©tudiants :**
- **Pourquoi certaines Q-valeurs sont-elles plus √©lev√©es que d'autres ?**
   - **R√©ponse :** Les actions menant vers des √©tats avec des r√©compenses √©lev√©es accumulent des valeurs plus √©lev√©es en raison des r√©compenses futures attendues.

---

### √âtape 6 : Analyse des R√©sultats et Conclusion

F√©licitations, vous avez impl√©ment√© l‚Äôalgorithme SARSA ! Cet exercice montre comment l'agent apprend des Q-valeurs pour chaque paire √©tat-action en suivant une politique exploratrice.

**Questions pour les √©tudiants :**
1. **Comment SARSA diff√®re-t-il de Q-learning ?**
   - **R√©ponse :** Q-learning est off-policy et utilise la meilleure action suivante pour la mise √† jour, tandis que SARSA est on-policy et utilise l‚Äôaction r√©ellement suivie pour la mise √† jour.

2. **Quels sont les avantages d'utiliser SARSA ?**
   - **R√©ponse :** SARSA prend en compte l'impact de la politique actuelle de l'agent, ce qui peut √™tre avantageux dans des environnements o√π la s√©curit√© ou des comportements exploratoires constants sont n√©cessaires.
