# Tutoriel Interactif : Deep Q-Learning (DQN) pour l'Apprentissage par Renforcement

### Contexte
**Objectif :** Apprendre √† impl√©menter un r√©seau de neurones pour estimer les Q-valeurs dans le cadre du Deep Q-Learning. Nous utiliserons une approche simple pour entra√Æner un mod√®le DQN sur un environnement de type "cartpole" o√π l'agent apprend √† √©quilibrer un b√¢ton sur un chariot.

**√âtapes √† suivre :**
1. **Introduction √† DQN**
2. **Cr√©ation de l‚Äôenvironnement et du mod√®le**
3. **Impl√©mentation de la fonction de politique Œµ-greedy**
4. **Mise en place de l‚Äôalgorithme DQN avec m√©moire de replay**
5. **Entra√Ænement et visualisation des performances**
6. **Conclusion et analyse des r√©sultats**

Chaque √©tape inclut une explication th√©orique, du code, des questions pour renforcer l‚Äôapprentissage, et des d√©fis pratiques.

---

### √âtape 1 : Introduction √† DQN

Deep Q-Learning (DQN) utilise un r√©seau de neurones pour approximer les Q-valeurs de chaque paire √©tat-action. Plut√¥t que de stocker les Q-valeurs dans une table, comme dans SARSA ou Q-Learning, le DQN utilise un mod√®le pour pr√©dire les valeurs. 

**Pourquoi DQN ?** Cet algorithme permet de r√©soudre des probl√®mes o√π l‚Äôespace d‚Äô√©tats et d‚Äôactions est trop vaste pour une table de Q-valeurs.

#### üìù Question : Pourquoi utilise-t-on un r√©seau de neurones pour approximer les Q-valeurs dans DQN ?
- **R√©ponse attendue :** Dans les environnements avec un grand nombre d‚Äô√©tats, il est inefficace de stocker des Q-valeurs pour chaque paire √©tat-action ; un r√©seau de neurones permet d‚Äôapprocher ces valeurs efficacement.

---

### √âtape 2 : Cr√©ation de l'Environnement et du Mod√®le

Nous utiliserons l‚Äôenvironnement **CartPole** de Gym, o√π l‚Äôobjectif est de maintenir le b√¢ton en √©quilibre. Ensuite, nous d√©finirons un mod√®le de r√©seau de neurones pour estimer les Q-valeurs.

#### Code √† ex√©cuter

```python
import gym
import torch
import torch.nn as nn
import torch.optim as optim
import random
import numpy as np

# Cr√©ation de l'environnement CartPole
env = gym.make("CartPole-v1")

# D√©finition du mod√®le DQN
class DQN(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(DQN, self).__init__()
        self.fc1 = nn.Linear(input_dim, 128)
        self.fc2 = nn.Linear(128, 128)
        self.fc3 = nn.Linear(128, output_dim)

    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = torch.relu(self.fc2(x))
        return self.fc3(x)

# Cr√©ation d'une instance du mod√®le
input_dim = env.observation_space.shape[0]
output_dim = env.action_space.n
model = DQN(input_dim, output_dim)

print("Mod√®le DQN cr√©√© avec succ√®s")
```

#### Explication d√©taill√©e
- **`DQN`** : Ce r√©seau de neurones est compos√© de trois couches enti√®rement connect√©es pour approximer les Q-valeurs.
- **Dimensions** : `input_dim` est le nombre de caract√©ristiques d'entr√©e, et `output_dim` est le nombre d'actions possibles.

**Questions pour les √©tudiants** :
1. **Pourquoi utilisons-nous une couche de sortie avec `output_dim` neurones ?**
   - **R√©ponse :** Chaque neurone de sortie correspond √† une action, et sa valeur repr√©sente la Q-valeur estim√©e pour cette action.

---

### √âtape 3 : Impl√©mentation de la Politique Œµ-greedy

La politique Œµ-greedy √©quilibre exploration et exploitation en choisissant une action al√©atoire avec probabilit√© Œµ et la meilleure action avec probabilit√© $$1 - \varepsilon$$.

#### Code √† ex√©cuter

```python
epsilon = 1.0  # Taux d'exploration initial
epsilon_min = 0.01  # Taux d'exploration minimal
epsilon_decay = 0.995  # D√©croissance d'epsilon

def epsilon_greedy_policy(state, epsilon):
    if random.random() < epsilon:
        return env.action_space.sample()  # Action al√©atoire (exploration)
    else:
        state = torch.FloatTensor(state).unsqueeze(0)
        with torch.no_grad():
            q_values = model(state)
        return q_values.argmax().item()  # Action avec la Q-valeur maximale (exploitation)

# Test de la politique
state = env.reset()
action = epsilon_greedy_policy(state, epsilon)
print(f"Action choisie : {action}")
```

#### Explication d√©taill√©e
- **Œµ-greedy** : Cette fonction choisit une action al√©atoire avec probabilit√© Œµ, sinon elle choisit l‚Äôaction avec la Q-valeur la plus √©lev√©e.
- **D√©croissance de Œµ** : Au fur et √† mesure que l‚Äôagent apprend, Œµ diminue pour favoriser l‚Äôexploitation des connaissances accumul√©es.

**Questions pour les √©tudiants** :
1. **Pourquoi r√©duisons-nous progressivement Œµ ?**
   - **R√©ponse :** La diminution de Œµ permet de privil√©gier l'exploitation des connaissances acquises une fois que l'agent a explor√© suffisamment l‚Äôenvironnement.

---

### √âtape 4 : Mise en Place de l‚ÄôAlgorithme DQN avec M√©moire de Replay

La m√©moire de replay stocke les exp√©riences pass√©es (√©tat, action, r√©compense, prochain √©tat) pour briser la corr√©lation entre les transitions lors de l‚Äôentra√Ænement du mod√®le.

#### Code √† ex√©cuter

```python
# Param√®tres de la m√©moire de replay
memory = []
memory_size = 10000
batch_size = 32

# Fonction pour stocker une exp√©rience dans la m√©moire
def store_experience(memory, experience):
    if len(memory) >= memory_size:
        memory.pop(0)
    memory.append(experience)

# Fonction de replay pour entra√Æner le mod√®le
def replay(batch_size):
    if len(memory) < batch_size:
        return
    batch = random.sample(memory, batch_size)
    states, actions, rewards, next_states, dones = zip(*batch)
    
    states = torch.FloatTensor(states)
    actions = torch.LongTensor(actions)
    rewards = torch.FloatTensor(rewards)
    next_states = torch.FloatTensor(next_states)
    dones = torch.FloatTensor(dones)

    # Q-valeurs actuelles pour les actions choisies
    q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)
    
    # Cibles de Q-valeur en utilisant Q-learning
    next_q_values = model(next_states).max(1)[0]
    targets = rewards + (1 - dones) * gamma * next_q_values
    
    # Perte et optimisation
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    loss = criterion(q_values, targets)
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

# Param√®tres d'apprentissage
gamma = 0.99  # Facteur d'escompte
```

#### Explication d√©taill√©e
- **M√©moire de replay** : Permet de stocker des exp√©riences pour entra√Æner le mod√®le par mini-lots et briser la corr√©lation temporelle entre les transitions.
- **Fonction de replay** : Entra√Æne le mod√®le sur un √©chantillon de transitions tir√© de la m√©moire.

**Questions pour les √©tudiants** :
1. **Pourquoi utiliser une m√©moire de replay ?**
   - **R√©ponse :** La m√©moire de replay permet de d√©corr√©ler les exp√©riences et am√©liore la stabilit√© de l‚Äôapprentissage.

---

### √âtape 5 : Entra√Ænement et Visualisation des Performances

Nous allons maintenant ex√©cuter plusieurs √©pisodes pour entra√Æner notre agent et observer l'√©volution de sa performance.

#### Code √† ex√©cuter

```python
n_episodes = 500
scores = []

for episode in range(n_episodes):
    state = env.reset()
    done = False
    score = 0
    
    while not done:
        action = epsilon_greedy_policy(state, epsilon)
        next_state, reward, done, _ = env.step(action)
        
        # Stockage de l'exp√©rience dans la m√©moire
        store_experience(memory, (state, action, reward, next_state, done))
        
        # Entra√Ænement du mod√®le
        replay(batch_size)
        
        state = next_state
        score += reward
    
    scores.append(score)
    epsilon = max(epsilon_min, epsilon * epsilon_decay)  # D√©croissance de epsilon
    
    if episode % 10 == 0:
        print(f"√âpisode {episode}, Score moyen des 10 derniers √©pisodes : {np.mean(scores[-10:])}")

# Affichage de la performance
import matplotlib.pyplot as plt

plt.plot(scores)
plt.xlabel("√âpisodes")
plt.ylabel("Score")
plt.title("Performance de l'agent DQN sur CartPole")
plt.show()
```

#### Explication d√©taill√©e
- **Entra√Ænement** : L‚Äôagent joue dans l‚Äôenvironnement, stocke ses exp√©riences,

 et s‚Äôentra√Æne en utilisant des mini-lots tir√©s de la m√©moire.
- **Visualisation** : Le score moyen par √©pisode permet de suivre l‚Äôapprentissage de l‚Äôagent.

**Questions pour les √©tudiants** :
1. **Pourquoi utilisons-nous une moyenne des scores sur les 10 derniers √©pisodes ?**
   - **R√©ponse :** Cela permet de lisser les performances pour observer plus facilement les progr√®s de l'agent.

---

### √âtape 6 : Conclusion et Analyse des R√©sultats

F√©licitations ! Vous avez impl√©ment√© un mod√®le DQN pour r√©soudre le probl√®me CartPole. Gr√¢ce √† cette m√©thode, l‚Äôagent utilise des r√©seaux de neurones pour approximer les valeurs d‚Äôactions et apprendre √† maximiser sa r√©compense cumulative.

**Questions pour les √©tudiants :**
1. **Quelle est la diff√©rence entre DQN et Q-learning ?**
   - **R√©ponse :** DQN utilise un r√©seau de neurones pour approximer les Q-valeurs, tandis que Q-learning utilise une table de Q-valeurs.

2. **Quels sont les avantages d‚Äôutiliser DQN ?**
   - **R√©ponse :** DQN est efficace dans des environnements avec de grands espaces d‚Äô√©tats o√π une table de Q-valeurs serait impraticable.

