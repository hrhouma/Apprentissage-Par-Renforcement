# **Projet 02 : Optimisation Dynamique des Ateliers avec Apprentissage par Renforcement Profond (DQN)**



#### **Contexte et Problématique :**

Dans une usine, une chaîne de production ou un atelier, plusieurs **travaux** doivent être exécutés sur des **machines** en suivant des règles précises. Par exemple, un travail peut être une commande client qui doit passer par plusieurs étapes (découpage, assemblage, peinture) avant d’être terminée. Chaque étape, appelée **opération**, doit être réalisée dans un ordre spécifique, et différentes machines peuvent effectuer une même opération mais avec des durées de traitement différentes.

Les défis rencontrés dans ce contexte incluent :  
1. **L’optimisation des ressources :** Comment affecter les travaux aux machines pour minimiser les retards ?  
2. **La gestion des imprévus :** L’arrivée aléatoire de nouveaux travaux ou des pannes de machines rendent la planification complexe.  
3. **La satisfaction des délais :** Respecter les dates limites de livraison, malgré les contraintes.  

La planification manuelle ou basée sur des heuristiques classiques (comme "d'abord le plus rapide") devient vite inefficace pour des systèmes complexes ou dynamiques.

**Problème :**  
**Comment construire un système intelligent capable de planifier dynamiquement les opérations des travaux dans un atelier, tout en minimisant les retards globaux, même en présence d’imprévus ?**

---

#### **Objectifs du Projet :**

Ce projet a pour but de concevoir et d’entraîner un agent d’apprentissage par renforcement, basé sur un réseau neuronal profond (**DQN – Deep Q-Network**), pour :  
1. **Planifier les travaux dans un atelier dynamique.**  
2. **Minimiser le retard moyen** des travaux, calculé comme le temps au-delà des dates limites.  
3. **Réagir efficacement aux perturbations**, comme l’arrivée de nouveaux travaux ou l’indisponibilité temporaire des machines.  

---

#### **Détails Techniques :**

##### 1. **Modélisation du problème :**

- **Travaux :**  
  Chaque travail est une séquence d'opérations à exécuter dans un ordre précis. Par exemple :  
  Travail 1 : découpage → assemblage → peinture.

- **Opérations :**  
  Une opération peut être traitée par une ou plusieurs machines compatibles, chacune ayant une durée différente.  

- **Machines :**  
  Chaque machine peut exécuter une seule opération à la fois. Une machine devient libre dès qu'elle termine une opération.

- **Dates limites :**  
  Chaque travail a une date limite fixée en fonction de son temps de traitement global et d’un paramètre de **flexibilité temporelle** appelé DDT (**Due Date Tightness**).

---

##### 2. **Environnement simulé :**

Vous allez simuler un atelier où :  
- Des **travaux arrivent dynamiquement** selon une distribution aléatoire (modélisée par un processus de Poisson).  
- Les machines effectuent les opérations assignées en suivant les décisions prises par l’agent.  
- Le système est perturbé par l’arrivée inattendue de nouveaux travaux, ce qui oblige l’agent à réévaluer constamment les priorités.  

---

##### 3. **Agent d’apprentissage par renforcement :**

L'agent prendra des décisions pour :  
1. **Affecter une opération à une machine.**  
2. Choisir la meilleure opération parmi les options disponibles en utilisant des **heuristiques** comme :  
   - **SPT (Shortest Processing Time)** : Opération avec le temps de traitement le plus court.  
   - **EDD (Earliest Due Date)** : Opération avec la date limite la plus proche.  
   - **SLK (Slack Time)** : Opération avec la marge la plus faible avant dépassement de délai.  

---

##### 4. **Espace des États :**

L’agent recevra des informations détaillées sur l’état actuel de l’atelier, comme :  
- Le taux de retard actuel des opérations et des travaux.  
- Le taux de retard attendu, basé sur les estimations des dates limites.  
- Le taux d’avancement des travaux (nombre d’opérations terminées).  
- Les durées restantes pour chaque opération et travail.  
- La charge actuelle des machines (temps restant avant qu'elles ne deviennent libres).

---

##### 5. **Actions possibles :**

À chaque décision, l’agent devra :  
1. Choisir une machine prête à exécuter une opération.  
2. Choisir l'opération à affecter à cette machine parmi plusieurs options disponibles.

---

##### 6. **Fonction de récompense :**

La récompense doit guider l'agent vers l'objectif principal : **minimiser les retards moyens.**  
- Une pénalité sera attribuée pour chaque retard, proportionnelle au temps dépassant la date limite.  
- Une récompense sera donnée pour chaque opération terminée avant la date limite.

---

##### 7. **Apprentissage par renforcement avec DQN :**

L'algorithme DQN repose sur :  
1. **Un réseau neuronal profond :**  
   - En entrée : Les informations sur l’état actuel de l’atelier.  
   - En sortie : Les valeurs Q estimées pour chaque action possible.  

2. **Une stratégie d’exploration (epsilon-greedy) :**  
   - Au début, l'agent choisit des actions aléatoires pour explorer l’environnement.  
   - Progressivement, il exploite les décisions optimales apprises.

3. **Mémoire de relecture (Replay Buffer) :**  
   - L'agent stocke les expériences passées (état, action, récompense, nouvel état).  
   - Ces expériences sont réutilisées pour l’entraînement, ce qui améliore la stabilité de l’apprentissage.

---

#### **Étapes du Projet :**

1. **Modélisation de l’environnement :**
   - Créez un simulateur d’atelier avec des travaux, machines et perturbations dynamiques.  
   - Définissez les règles de fonctionnement et les métriques (retard, taux d’avancement).

2. **Développement de l’agent DQN :**
   - Implémentez un réseau neuronal pour modéliser les décisions de l’agent.  
   - Utilisez PyTorch ou TensorFlow pour entraîner le modèle.  

3. **Entraînement de l’agent :**
   - Laissez l’agent jouer plusieurs épisodes dans l’atelier simulé.  
   - Utilisez des métriques pour suivre son amélioration au fil du temps.

4. **Analyse des résultats :**
   - Comparez les performances de l’agent avec des heuristiques classiques (SPT, EDD).  
   - Visualisez les résultats avec des diagrammes (ex. : diagrammes de Gantt).  

---

#### **Livrables attendus :**

1. **Code fonctionnel :**
   - Un simulateur complet de l’atelier.  
   - Un agent DQN entraîné et capable de minimiser les retards.  

2. **Rapport technique :**
   - Description du problème et des approches testées.  
   - Analyse des performances de l’agent comparé aux heuristiques classiques.  

3. **Visualisations :**
   - Diagrammes de Gantt pour visualiser les décisions prises par l’agent.  
   - Courbes illustrant la réduction progressive des retards au fil des épisodes.  

---

#### **Pourquoi ce projet est-il important ?**

Ce projet combine plusieurs compétences et concepts clés :  
1. **Intelligence artificielle et apprentissage par renforcement :** Développer des solutions adaptatives pour des environnements dynamiques.  
2. **Optimisation industrielle :** Résoudre des problèmes pratiques liés à la gestion de ressources.  
3. **Analyse et simulation :** Modéliser des environnements complexes et évaluer les performances des solutions proposées.

---

#### **Ressources utiles :**

1. **Documentation technique :**
   - [Introduction à l’apprentissage par renforcement (Sutton & Barto)](http://incompleteideas.net/book/the-book.html).  
   - [Tutoriel PyTorch pour le DQN](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html).  

2. **Exemples pratiques :**
   - Études sur le **Dynamic Job Shop Scheduling**.  
   - Projets similaires sur GitHub (explorer des implémentations open-source).
