# **EXAMEN MI-SESSION : APPRENTISSAGE PAR RENFORCEMENT**

---

### **Bar√®me global :**

L'examen est not√© sur **100 points** r√©partis comme suit :

- **Partie 1 : Questions √† choix multiples** (20 points)
  - 5 questions √† 4 points chacune.
  
- **Partie 2 : Questions √† r√©ponse courte** (30 points)
  - 5 questions √† 6 points chacune.
  
- **Partie 3 : Probl√®me pratique 1** (20 points)
  - Sous-questions avec des points attribu√©s en fonction des calculs et explications.
  
- **Partie 4 : Probl√®me pratique 2** (30 points)
  - Mises √† jour de Q-table et raisonnement d√©taill√©.








------------------
# **Partie 1 : Questions √† choix multiples (20 points)**



### 1. Qu'est-ce qu'un Processus de D√©cision Markovien (MDP) ?
   
   a) Un algorithme d'apprentissage supervis√©
   
   b) Un mod√®le math√©matique pour la prise de d√©cision s√©quentielle
   
   c) Une technique de clustering
   
   d) Un type de r√©seau neuronal

### 2. La propri√©t√© de Markov stipule que :
   
   a) L'√©tat futur d√©pend uniquement de l'√©tat pr√©sent
   
   b) L'√©tat futur d√©pend de tous les √©tats pass√©s
   
   c) L'√©tat futur est compl√®tement al√©atoire
   
   d) L'√©tat futur d√©pend uniquement de l'action pr√©sente
   

### 3. Quelle est la diff√©rence entre l'exploration et l'exploitation en apprentissage par renforcement ?
   
   a) L'exploration cherche de nouvelles actions, l'exploitation utilise les meilleures actions connues
   
   b) L'exploration est utilis√©e en apprentissage supervis√©, l'exploitation en apprentissage non supervis√©
   
   c) L'exploration est d√©terministe, l'exploitation est stochastique
   
   d) Il n'y a pas de diff√©rence, ce sont des synonymes
   

### 4. Qu'est-ce que le "discounting" en apprentissage par renforcement ?
   
   a) Une technique pour r√©duire la taille du mod√®le
   
   b) Un moyen de donner moins d'importance aux r√©compenses futures
   
   c) Une m√©thode pour acc√©l√©rer l'apprentissage
   
   d) Un type de fonction d'activation

### 5. Quelle est la diff√©rence entre la valeur d'√©tat et la valeur Q-state ?
    
   a) La valeur d'√©tat consid√®re toutes les actions possibles, la valeur Q-state une action sp√©cifique
   
   b) La valeur d'√©tat est toujours plus grande que la valeur Q-state
   
   c) La valeur d'√©tat est utilis√©e pour les environnements d√©terministes, la valeur Q-state pour les stochastiques
   
   d) Il n'y a pas de diff√©rence, ce sont des synonymes


------------------
# **Partie 2 : Questions √† r√©ponse courte (30 points)**



#### 6. Expliquez la diff√©rence entre un environnement dynamique et un environnement statique en apprentissage par renforcement.
   

#### 7. D√©crivez l'approche Œµ-greedy en apprentissage par renforcement. Pourquoi est-elle utilis√©e ?
   

#### 8. Qu'est-ce qu'un √©tat absorbant dans un MDP ? Donnez un exemple.
    

#### 9. Expliquez la diff√©rence entre une strat√©gie (policy) et une strat√©gie optimale en apprentissage par renforcement.
    

#### 10. D√©crivez bri√®vement l'algorithme d'it√©ration de valeur (value iteration). Quel est son objectif principal ?
    

------------------
# **Partie 3 : Probl√®me pratique 1 (20 points)**




###  11. Consid√©rez le MDP suivant repr√©sentant un petit labyrinthe :

```
   +---+---+---+
   | S | 0 | 0 |
   +---+---+---+
   |-1 |-1 | 0 |
   +---+---+---+
   |-1 |-1 | G |
   +---+---+---+
```

S est l'√©tat initial, G est l'√©tat but (r√©compense +10), les cases marqu√©es -1 sont des pi√®ges (r√©compense -1), et les 0 sont des cases neutres. L'agent peut se d√©placer dans les 4 directions (haut, bas, gauche, droite) et reste sur place s'il tente de sortir de la grille. Le facteur de discount Œ≥ est de 0.9.

###   a) √âcrivez la fonction de r√©compense r(s,a,s') pour ce MDP. 

###   b) Calculez la valeur optimale V*(s) pour l'√©tat initial S en utilisant l'√©quation de Bellman. Montrez vos calculs. 

###   c) Quelle est la strat√©gie optimale pour ce MDP ? Justifiez votre r√©ponse. 

###   d) Comment ce probl√®me changerait-il si l'environnement √©tait stochastique, avec une probabilit√© de 0.2 que l'agent glisse et se d√©place dans une direction al√©atoire diff√©rente de celle choisie ? 



------------------
# **Partie 4 : Probl√®me pratique 2 (30 points)**



## Contexte
Dans cet exercice, vous allez utiliser les concepts de **Q-learning** pour mettre √† jour une Q-table en fonction des d√©cisions prises par un agent √©voluant dans un environnement de grille (Grid World). L'agent devra apprendre une **politique cible** en explorant cet environnement.

## Objectifs
1. Analyser une **Q-table**.
2. Comprendre l'environnement **Grid World** et les d√©placements possibles.
3. Mettre √† jour une valeur de la Q-table.

---

## Partie 4-1 : Mise √† jour de la Q-table apr√®s un d√©placement initial

### √âtape 1 : Q-table initiale

Voici la **Q-table** initiale, qui contient les valeurs des actions possibles dans diff√©rents √©tats. Chaque ligne repr√©sente un √©tat, et chaque colonne repr√©sente une action (gauche, droite, haut, bas).

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   1    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |  0.75  | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```

### √âtape 2 : Environnement Grid World

L'environnement dans lequel l'agent √©volue est une grille (Grid World). Chaque cellule de la grille contient une r√©compense. Les √©tats terminaux ont des r√©compenses sp√©ciales, comme indiqu√© ci-dessous :

```
+----+----+----+
| -1 | -1 | -1 |
+----+----+----+
| -1 | -1 | -1 |
+----+----+----+
| -1 | -10| +10|
+----+----+----+
```

- **R√©compenses** : Toutes les actions entra√Ænent une r√©compense de **-1**, sauf lorsqu'un √©tat terminal est atteint. Les r√©compenses dans les √©tats terminaux sont **-10** et **+10** respectivement.

### √âtape 3 : D√©placement de l'agent

L'agent, repr√©sent√© par un robot **(ü§ñ)**, commence dans l'√©tat **S1**, en haut √† gauche de la grille :

```
+------+----+----+
| ü§ñ S1 | -1 | -1 |
+------+----+----+
| -1   | -1 | -1 |
+------+----+----+
| -1   | -10| +10|
+------+----+----+
```

### √âtape 4 : Prise de d√©cision

L'agent prend une d√©cision bas√©e sur une **politique de comportement**. Depuis l'√©tat **S1**, l'agent peut se d√©placer **vers la droite** ou **vers le bas** :

```
+---------+----+----+----+
| ü§ñ S1 ‚Üí | -1 | -1 | -1 |
+---‚Üì-----+----+----+----+
|   -1    | -1 | -1 | -1 |
+---------+----+----+----+
|   -1    | -10| +10| -1 |
+---------+----+----+----+
```

### √âtape 5 : Action choisie

L'agent choisit de se d√©placer **vers la droite**, atteignant ainsi l'√©tat **S2** :

```
+------+----+----+
| ü§ñ S1|‚Üí -1 (S2) | -1 | 
+------+----+----+
| -1   | -1 | -1 | 
+------+----+----+
| -1   | -10| +10| 
+------+----+----+
```

### √âtape 6 : Mise √† jour de la Q-table

**Question :** En utilisant vos connaissances du Q-learning (r√©f√©rez-vous √† vos notes de cours), mettez √† jour la valeur de **Q(S1, droite)**, initialement √† **1**, dans la Q-table ci-dessous.

- **Alpha** (taux d'apprentissage) : 0.1
- **R√©compenses** : Toutes les r√©compenses sont de **-1**, sauf pour les √©tats terminaux o√π les r√©compenses sont **-10** et **+10**.
- **Bruit** : Aucun bruit.

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   ?    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |  0.75  | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```

---

### Instructions :
- Calculez la nouvelle valeur de **Q(S1, droite)** en appliquant la mise √† jour de la Q-table.
- Indiquez les √©tapes de votre raisonnement, et montrez comment vous avez utilis√© les r√©compenses et le taux d'apprentissage pour ajuster cette valeur.

---

## Partie 4-2 : D√©placement suppl√©mentaire et mise √† jour de la Q-table

### √âtape 1 : D√©placement suppl√©mentaire de l'agent

Apr√®s avoir mis √† jour la valeur de **Q(S1, droite)**, l'agent continue √† se d√©placer. Cette fois-ci, depuis l'√©tat **S2**, l'agent choisit de se d√©placer **vers la droite**, atteignant ainsi l'√©tat **S3**.

```
+----+----+----+----+
| -1 | ü§ñ S2 ‚Üí | S3 -1 |
+----+----+----+----+
| -1 | -1 | -1 | -1 |
+----+----+----+----+
| -1 | -10 | +10| -1 |
+----+----+----+----+
```

### √âtape 2 : Mise √† jour de la Q-table pour S2

**Question :** Mettez √† jour la valeur de **Q(S2, droite)** en appliquant √† nouveau vos connaissances du Q-learning. R√©f√©rez-vous √† vos notes de cours pour effectuer cette mise √† jour.

- **Alpha** (taux d'apprentissage) : 0.1
- **R√©compenses** : Toutes les r√©compenses sont de **-1**, sauf pour les √©tats terminaux o√π les r√©compenses sont **-10** et **+10**.
- **Bruit** : Aucun bruit.

Voici la Q-table √† compl√©ter :

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   ?    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |   ?    | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```

---

### Instructions :
- Calculez la nouvelle valeur de **Q(S2, droite)** en appliquant la mise √† jour de la Q-table.
- Expliquez les √©tapes de votre raisonnement et montrez comment vous avez utilis√© les r√©compenses et le taux d'apprentissage pour ajuster cette valeur.

---

## Partie 4-3 : R√©√©valuer dans un environnement stochastique

Dans cette partie, vous allez r√©√©valuer les r√©sultats des **Parties 1 et 2**, mais cette fois-ci dans un **environnement stochastique**.

### Question

Comment ce probl√®me changerait-il si l'environnement √©tait **stochastique**, avec une probabilit√© de **0.2** que l'agent glisse et se d√©place dans une direction al√©atoire diff√©rente de celle choisie ?

---

### Instructions :
- Reprenez les **Parties 1 et 2** en tenant compte du fait que l'agent peut glisser dans une direction al√©atoire avec une probabilit√© de 0.2.
- Calculez √† nouveau les mises √† jour des valeurs de **Q(S1, droite)** et **Q(S2, droite)** en prenant en compte cet environnement stochastique.
- Expliquez comment cela affecte votre raisonnement et les valeurs mises √† jour dans la Q-table.





