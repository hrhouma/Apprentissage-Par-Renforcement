# Quiz à Choix Multiple : Les Problèmes de Bandits

#### 1. Introduction aux Problèmes de Bandits

1. Dans le contexte des problèmes de bandits, que représente le terme "bandit" ?
   - A) Un choix aléatoire
   - B) Une action ou une option qui peut rapporter une récompense
   - C) Un agent intelligent
   - D) Un ensemble de règles de décision

2. Que signifie "explorer" dans les problèmes de bandits ?
   - A) Tester une option connue pour maximiser la récompense
   - B) Tester une option au hasard pour découvrir une nouvelle récompense
   - C) Arrêter l’apprentissage pour éviter les erreurs
   - D) Exécuter uniquement l’action avec la meilleure récompense connue

3. Quel est l'objectif principal dans un problème de bandits ?
   - A) Minimiser les erreurs
   - B) Maximiser la récompense totale en équilibrant exploration et exploitation
   - C) Maximiser le nombre de tentatives
   - D) Ignorer les options peu performantes

4. Quel est un exemple d’application concrète des problèmes de bandits ?
   - A) Trier des fichiers sur un ordinateur
   - B) Choisir des annonces publicitaires à afficher
   - C) Calculer la vitesse d'un processeur
   - D) Optimiser la mémoire d’un programme

5. Dans le contexte des machines à sous, que signifie "exploiter" une option ?
   - A) Choisir une action aléatoire
   - B) Choisir l’option qui semble la plus prometteuse
   - C) Ignorer l’option actuelle
   - D) Tester une nouvelle option 

#### 2. Problèmes de Bandits Simples

6. Comment appelle-t-on un problème de bandits avec plusieurs actions possibles, comme les bras de machines à sous ?
   - A) Bandit simple
   - B) Bandit multi-bras
   - C) Bandit aléatoire
   - D) Bandit séquentiel

7. Dans une application de recommandation de produits, pourquoi est-il important d'explorer de nouvelles options ?
   - A) Pour maximiser les résultats immédiats
   - B) Pour découvrir des produits qui pourraient intéresser l’utilisateur
   - C) Pour minimiser les coûts
   - D) Pour éviter de montrer le même produit

8. Quelle est la meilleure stratégie si toutes les récompenses des actions possibles sont déjà connues ?
   - A) Explorer davantage
   - B) Exploiter uniquement l’option avec la récompense la plus élevée
   - C) Ignorer toutes les options
   - D) Alterner entre exploration et exploitation

9. Quelle est la différence principale entre exploration et exploitation ?
   - A) L’exploration est aléatoire, tandis que l’exploitation est systématique
   - B) L’exploration maximise les récompenses, l’exploitation minimise les risques
   - C) L’exploration consiste à tester de nouvelles options, l’exploitation utilise l’option la plus prometteuse
   - D) L’exploration est rapide, l’exploitation est lente

10. Lequel des exemples suivants est un cas d’exploitation ?
   - A) Tester une option au hasard pour voir ce qu’elle rapporte
   - B) Choisir systématiquement la meilleure option connue
   - C) Alterner entre toutes les options possibles
   - D) Ignorer les options non testées

#### 3. Les Algorithmes Principaux des Bandits

11. Comment fonctionne l'algorithme **ε-greedy** dans un problème de bandits ?
   - A) Choisit toujours l’action avec la meilleure récompense moyenne
   - B) Choisit une action aléatoirement avec une probabilité ε et exploite la meilleure action avec une probabilité \(1 - ε\)
   - C) Explore toutes les options sans exploitation
   - D) Ignore les actions non explorées

12. Pourquoi l'algorithme **Upper Confidence Bound (UCB)** est-il utile dans les problèmes de bandits ?
   - A) Il utilise un bonus d'exploration pour tester les actions moins explorées
   - B) Il ignore les actions qui n’ont pas été testées
   - C) Il maximise uniquement les récompenses immédiates
   - D) Il choisit l’action avec la récompense la plus faible

13. Que fait l’algorithme Thompson Sampling dans les problèmes de bandits ?
   - A) Il choisit l’action la moins explorée
   - B) Il exploite uniquement l’action la plus rentable
   - C) Il échantillonne les récompenses et choisit l’action avec la meilleure récompense attendue
   - D) Il ne fait pas de différence entre exploration et exploitation

14. Dans quel cas l'algorithme **ε-greedy** est-il particulièrement utile ?
   - A) Lorsque les récompenses changent rapidement
   - B) Lorsque toutes les actions ont la même récompense
   - C) Lorsque l'agent souhaite éviter toute exploration
   - D) Lorsque le nombre d'options est très limité

15. Que signifie un ε faible dans l'algorithme **ε-greedy** ?
   - A) Plus de temps consacré à l’exploration
   - B) Plus de temps consacré à l’exploitation de la meilleure option connue
   - C) Moins de tests initiaux
   - D) Plus d’alternance entre toutes les options

#### 4. Comparaison des Algorithmes de Bandits

16. Lequel des algorithmes suivants ajoute un bonus d'exploration aux actions peu explorées ?
   - A) Thompson Sampling
   - B) ε-greedy
   - C) UCB
   - D) Q-learning

17. En quoi l’algorithme **UCB** est-il différent de l’algorithme **ε-greedy** ?
   - A) UCB choisit toujours l'action la plus testée
   - B) UCB ajoute un bonus d'exploration pour les actions peu explorées
   - C) ε-greedy explore davantage que UCB
   - D) ε-greedy ne prend jamais de décisions aléatoires

18. Quel est un avantage de l’algorithme **Thompson Sampling** ?
   - A) Il n’explore pas du tout
   - B) Il s’adapte bien aux changements dans les récompenses
   - C) Il choisit systématiquement la même action
   - D) Il a une complexité calculatoire très faible

19. Pourquoi l'algorithme **UCB** est-il parfois considéré comme "intelligent" pour l'exploration ?
   - A) Il favorise l'exploration des options moins explorées tout en maximisant les gains
   - B) Il choisit uniquement les actions les plus explorées
   - C) Il ne choisit jamais la même option deux fois
   - D) Il choisit toujours l’action avec la plus faible récompense moyenne

20. Lequel des algorithmes suivants utilise une approche bayésienne pour déterminer la meilleure action ?
   - A) ε-greedy
   - B) UCB
   - C) Thompson Sampling
   - D) Q-learning
