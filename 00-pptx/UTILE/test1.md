# Examen : Mise √† jour de la Q-table

## Contexte
Dans cet exercice, vous allez utiliser les concepts de **Q-learning** pour mettre √† jour une Q-table en fonction des d√©cisions prises par un agent √©voluant dans un environnement de grille (Grid World). L'agent devra apprendre une **politique cible** en explorant cet environnement.

## Objectifs
1. Analyser une **Q-table**.
2. Comprendre l'environnement **Grid World** et les d√©placements possibles.
3. Mettre √† jour une valeur de la Q-table.

---

# Partie 1 : Mise √† jour de la Q-table apr√®s un d√©placement initial

### √âtape 1 : Q-table initiale

Voici la **Q-table** initiale, qui contient les valeurs des actions possibles dans diff√©rents √©tats. Chaque ligne repr√©sente un √©tat, et chaque colonne repr√©sente une action (gauche, droite, haut, bas).

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   1    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |  0.75  | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```

### √âtape 2 : Environnement Grid World

L'environnement dans lequel l'agent √©volue est une grille (Grid World). Chaque cellule de la grille contient une r√©compense. Les √©tats terminaux ont des r√©compenses sp√©ciales, comme indiqu√© ci-dessous :

```
+----+----+----+
| -1 | -1 | -1 |
+----+----+----+
| -1 | -1 | -1 |
+----+----+----+
| -1 | -10| +10|
+----+----+----+
```

- **R√©compenses** : Toutes les actions entra√Ænent une r√©compense de **-1**, sauf lorsqu'un √©tat terminal est atteint. Les r√©compenses dans les √©tats terminaux sont **-10** et **+10** respectivement.

### √âtape 3 : D√©placement de l'agent

L'agent, repr√©sent√© par un robot **(ü§ñ)**, commence dans l'√©tat **S1**, en haut √† gauche de la grille :

```
+------+----+----+
| ü§ñ S1 | -1 | -1 |
+------+----+----+
| -1   | -1 | -1 |
+------+----+----+
| -1   | -10| +10|
+------+----+----+
```

### √âtape 4 : Prise de d√©cision

L'agent prend une d√©cision bas√©e sur une **politique de comportement**. Depuis l'√©tat **S1**, l'agent peut se d√©placer **vers la droite** ou **vers le bas** :

```
+---------+----+----+----+
| ü§ñ S1 ‚Üí | -1 | -1 | -1 |
+---‚Üì-----+----+----+----+
|   -1    | -1 | -1 | -1 |
+---------+----+----+----+
|   -1    | -10| +10| -1 |
+---------+----+----+----+
```

### √âtape 5 : Action choisie

L'agent choisit de se d√©placer **vers la droite**, atteignant ainsi l'√©tat **S2** :

```
+------+----+----+
| ü§ñ S1|‚Üí -1 (S2) | -1 | 
+------+----+----+
| -1   | -1 | -1 | 
+------+----+----+
| -1   | -10| +10| 
+------+----+----+
```

### √âtape 6 : Mise √† jour de la Q-table

**Question :** En utilisant vos connaissances du Q-learning (r√©f√©rez-vous √† vos notes de cours), mettez √† jour la valeur de **Q(S1, droite)**, initialement √† **1**, dans la Q-table ci-dessous.

- **Alpha** (taux d'apprentissage) : 0.1
- **R√©compenses** : Toutes les r√©compenses sont de **-1**, sauf pour les √©tats terminaux o√π les r√©compenses sont **-10** et **+10**.
- **Bruit** : Aucun bruit.

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   ?    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |  0.75  | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```

---

### Instructions :
- Calculez la nouvelle valeur de **Q(S1, droite)** en appliquant la mise √† jour de la Q-table.
- Indiquez les √©tapes de votre raisonnement, et montrez comment vous avez utilis√© les r√©compenses et le taux d'apprentissage pour ajuster cette valeur.

---

# Partie 2 : D√©placement suppl√©mentaire et mise √† jour de la Q-table

### √âtape 1 : D√©placement suppl√©mentaire de l'agent

Apr√®s avoir mis √† jour la valeur de **Q(S1, droite)**, l'agent continue √† se d√©placer. Cette fois-ci, depuis l'√©tat **S2**, l'agent choisit de se d√©placer **vers la droite**, atteignant ainsi l'√©tat **S3**.

```
+----+----+----+----+
| -1 | ü§ñ S2 ‚Üí | S3 -1 |
+----+----+----+----+
| -1 | -1 | -1 | -1 |
+----+----+----+----+
| -1 | -10 | +10| -1 |
+----+----+----+----+
```

### √âtape 2 : Mise √† jour de la Q-table pour S2

**Question :** Mettez √† jour la valeur de **Q(S2, droite)** en appliquant √† nouveau vos connaissances du Q-learning. R√©f√©rez-vous √† vos notes de cours pour effectuer cette mise √† jour.

- **Alpha** (taux d'apprentissage) : 0.1
- **R√©compenses** : Toutes les r√©compenses sont de **-1**, sauf pour les √©tats terminaux o√π les r√©compenses sont **-10** et **+10**.
- **Bruit** : Aucun bruit.

Voici la Q-table √† compl√©ter :

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   ?    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |   ?    | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```

---

### Instructions :
- Calculez la nouvelle valeur de **Q(S2, droite)** en appliquant la mise √† jour de la Q-table.
- Expliquez les √©tapes de votre raisonnement et montrez comment vous avez utilis√© les r√©compenses et le taux d'apprentissage pour ajuster cette valeur.

---

# Partie 3 : R√©√©valuer dans un environnement stochastique

Dans cette partie, vous allez r√©√©valuer les r√©sultats des **Parties 1 et 2**, mais cette fois-ci dans un **environnement stochastique**.

### Question

Comment ce probl√®me changerait-il si l'environnement √©tait **stochastique**, avec une probabilit√© de **0.2** que l'agent glisse et se d√©place dans une direction al√©atoire diff√©rente de celle choisie ?

---

### Instructions :
- Reprenez les **Parties 1 et 2** en tenant compte du fait que l'agent peut glisser dans une direction al√©atoire avec une probabilit√© de 0.2.
- Calculez √† nouveau les mises √† jour des valeurs de **Q(S1, droite)** et **Q(S2, droite)** en prenant en compte cet environnement stochastique.
- Expliquez comment cela affecte votre raisonnement et les valeurs mises √† jour dans la Q-table.

