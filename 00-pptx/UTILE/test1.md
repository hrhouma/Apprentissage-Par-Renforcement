# Exercice : Mise √† jour de la Q-table

### Contexte
Dans cet exercice, vous allez utiliser les concepts de **Q-learning** pour mettre √† jour une Q-table en fonction des d√©cisions prises par un agent √©voluant dans un environnement de grille (Grid World). L'agent devra apprendre une **politique cible** en explorant cet environnement.

### Objectifs
1. Analyser une **Q-table**.
2. Comprendre l'environnement **Grid World** et les d√©placements possibles.
3. Mettre √† jour une valeur de la Q-table.

---

### #1 Q-table initiale

Voici la **Q-table** initiale, qui contient les valeurs des actions possibles dans diff√©rents √©tats. Chaque ligne repr√©sente un √©tat, et chaque colonne repr√©sente une action (gauche, droite, haut, bas).

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   1    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |  0.75  | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```

---

### #2 Environnement Grid World

L'environnement dans lequel l'agent √©volue est une grille (Grid World). Chaque cellule de la grille contient une r√©compense. Les √©tats terminaux ont des r√©compenses sp√©ciales, comme indiqu√© ci-dessous :

```
+----+----+----+
| -1 | -1 | -1 |
+----+----+----+
| -1 | -1 | -1 |
+----+----+----+
| -1 | -10| +10|
+----+----+----+
```

- **R√©compenses** : Toutes les actions entra√Ænent une r√©compense de **-1**, sauf lorsqu'un √©tat terminal est atteint. Les r√©compenses dans les √©tats terminaux sont **-10** et **+10** respectivement.

---

### #3 D√©placement de l'agent

L'agent, repr√©sent√© par un robot **(ü§ñ)**, commence dans l'√©tat **S1**, en haut √† gauche de la grille :

```
+------+----+----+
| ü§ñ S1 | -1 | -1 |
+------+----+----+
| -1   | -1 | -1 |
+------+----+----+
| -1   | -10| +10|
+------+----+----+
```

---

### #4 Prise de d√©cision

L'agent prend une d√©cision bas√©e sur une **politique de comportement**. Depuis l'√©tat **S1**, l'agent peut se d√©placer **vers la droite** ou **vers le bas** :

```
+---------+----+----+----+
| ü§ñ S1 ‚Üí | -1 | -1 | -1 |
+---‚Üì-----+----+----+----+
|   -1    | -1 | -1 | -1 |
+---------+----+----+----+
|   -1    | -10| +10| -1 |
+---------+----+----+----+
```

---

### #5 Action choisie

L'agent choisit de se d√©placer **vers la droite**, atteignant ainsi l'√©tat **S2** :

```
+------+----+----+
| ü§ñ S1|‚Üí -1 (S2) | -1 | 
+------+----+----+
| -1   | -1 | -1 | 
+------+----+----+
| -1   | -10| +10| 
+------+----+----+
```

---

### #6 Mise √† jour de la Q-table

**Question :** En utilisant vos connaissances du Q-learning (r√©f√©rez-vous √† vos notes de cours), mettez √† jour la valeur de **Q(S1, droite)**, initialement √† **1**, dans la Q-table ci-dessous.

- **Alpha** (taux d'apprentissage) : 0.1
- **R√©compenses** : Toutes les r√©compenses sont de **-1**, sauf pour les √©tats terminaux o√π les r√©compenses sont **-10** et **+10**.
- **Bruit** : Aucun bruit.

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   ?    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |  0.75  | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```

---

**Instructions :**
- Calculez la nouvelle valeur de **Q(S1, droite)** en appliquant la mise √† jour de la Q-table.
- Indiquez les √©tapes de votre raisonnement, et montrez comment vous avez utilis√© les r√©compenses et le taux d'apprentissage pour ajuster cette valeur.

