
# 1 
- On consid√®re la Q-table suivante :

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   1    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |  0.75  | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```

# 2

- On consid√®re l'environnement suivant **Grid World** :
```
+----+----+----+
| -1 | -1 | -1 |
+----+----+----+
| -1 | -1 | -1 |
+----+----+----+
| -1 | -10| +10|
+----+----+----+
```

# 3
- Nous voulons que l'agent apprenne la politique cible.
- Dans chaque image, le robot (ü§ñ) repr√©sente l'agent, les fl√®ches indiquent les mouvements possibles ou choisis, 


```
+------+----+----+----+
| ü§ñ  | -1 | -1 | -1 |
+------+----+----+----+
| -1   | -1 | -1 | -1 |
+------+----+----+----+
| -1   | -10| +10| -1 |
+------+----+----+----+
```

- Le robot est repr√©sent√© par le symbole ü§ñ en haut √† gauche. 
- Chaque cellule de la grille correspond √† une r√©compense ou une p√©nalit√©, avec -10 et +10 comme objectifs importants.




# 4

### Le rebot est initialement dans l'√©tat S1

```
+------+----+----+----+
| ü§ñ S1 | -1 | -1 | -1 |
+------+----+----+----+
| -1   | -1 | -1 | -1 |
+------+----+----+----+
| -1   | -10| +10| -1 |
+------+----+----+----+

L'agent prend une d√©cision bas√©e sur une politique de comportement.
```

# 5
###  L'agent peut se d√©placer vers la droite ou vers le bas

```
+---------+----+----+----+
| ü§ñ S1 ‚Üí | -1 | -1 | -1 |
+---‚Üì-----+----+----+----+
|   -1    | -1 | -1 | -1 |
+---------+----+----+----+
|   -1    | -10| +10| -1 |
+---------+----+----+----+
```


# 6

### L'agent prend la d√©cision de se d√©placer vers la droite.

```
+------+----+----+----+
|ü§ñ S1|‚Üí -1 (S2) | -1 | -1 |
+------+----+----+----+
| -1   | -1 | -1 | -1 |
+------+----+----+----+
| -1   | -10| +10| -1 |
+------+----+----+----+


```


#7 Question

EN  utilisant les √©quations de bellman comme vu en classe , mettre √† jour la Q-table pour le Q(s1, right) qui est initalement √† 1

```
+----+--------+--------+------+-------+
| Q  | gauche | droite |  haut | bas  |
+----+--------+--------+------+-------+
| S1 |  -0.5  |   ?    |  2.1 |  1.3  |
+----+--------+--------+------+-------+
| S2 |   0.5  |  0.75  | -0.5 |  1.5  |
+----+--------+--------+------+-------+
| S6 |  -1.2  |   1.2  |  0.7 |  1.7  |
+----+--------+--------+------+-------+
```







-----------


## Partie 3 : Probl√®me pratique (30 points)

11. Consid√©rez le MDP suivant repr√©sentant un petit labyrinthe :

```
   +---+---+---+
   | S | 0 | 0 |
   +---+---+---+
   |-1 |-1 | 0 |
   +---+---+---+
   |-1 |-1 | G |
   +---+---+---+
```

S est l'√©tat initial, G est l'√©tat but (r√©compense +10), les cases marqu√©es -1 sont des pi√®ges (r√©compense -1), et les 0 sont des cases neutres. L'agent peut se d√©placer dans les 4 directions (haut, bas, gauche, droite) et reste sur place s'il tente de sortir de la grille. Le facteur de discount Œ≥ est de 0.9. Les transitions sont d√©terministes.

a) √âcrivez la fonction de r√©compense r(s,a,s') pour ce MDP. Pr√©cisez clairement les r√©compenses pour chaque type de transition, y compris lorsque l'agent reste dans la grille ou tente d'en sortir. (5 points)

b) Calculez la valeur optimale V*(s) pour l'√©tat initial S en utilisant l'√©quation de Bellman. Montrez vos calculs en d√©tail, en consid√©rant uniquement les d√©placements vers la droite et vers le bas √† partir de S. Utilisez les valeurs des √©tats voisins imm√©diats pour votre calcul. (10 points)

c) Quelle est la strat√©gie optimale pour ce MDP ? Justifiez votre r√©ponse en expliquant votre raisonnement et en calculant V*(s) pour les autres √©tats pertinents du chemin optimal. (10 points)

d) Comment ce probl√®me changerait-il si l'environnement devenait stochastique, avec une probabilit√© de 0.8 que l'agent se d√©place dans la direction choisie, et une probabilit√© de 0.2 qu'il "glisse" et se d√©place dans l'une des trois autres directions al√©atoirement (chacune avec une probabilit√© de 0.2/3) ? Discutez des implications sur la strat√©gie optimale et la difficult√© du probl√®me. (5 points)



