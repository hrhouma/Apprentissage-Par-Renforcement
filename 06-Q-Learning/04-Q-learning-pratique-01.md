Chers √©tudiant.e.s,

Dans cette s√©ance, vous allez travailler sur **quatre algorithmes de Q-Learning** appliqu√©s au probl√®me **MountainCar-v0**. Chaque algorithme est con√ßu pour vous permettre de **tester, comprendre et analyser** l'impact des **diff√©rents taux d'apprentissage (alpha)** dans l'entra√Ænement d'un agent. 

L'objectif de cet exercice est de vous guider √©tape par √©tape √† travers les diff√©rents algorithmes et de vous permettre de **visualiser les r√©sultats** √† travers des simulations et des graphiques.

---

## üìú **Instructions G√©n√©rales :**
1. **Clonez les quatre projets GitHub**.
2. **Ex√©cutez les algorithmes** pour chacun des d√©p√¥ts.
3. **Interpr√©tez les r√©sultats** √† partir des simulations et des graphiques fournis.
4. **Posez-vous les bonnes questions** sur l'impact d'**alpha**, et tentez d'identifier les diff√©rences entre chaque version.

---

### **1 - RLCode2-1** :
- **Ce que fait l'algorithme :** 
  - Ce premier code utilise une **seule valeur de taux d'apprentissage (alpha = 0.5)**. Il entra√Æne un agent Q-Learning sur **2000 √©pisodes** et l'√©value sur **10 essais**. 
  - Vous observerez une simulation visuelle en **temps r√©el** de la voiture dans l'environnement **MountainCar-v0**. √Ä la fin des essais, des **graphes r√©capitulatifs** affichent le succ√®s de l'agent et le nombre de pas n√©cessaires.
- **Rendu visuel :** Oui.
- **Graphiques :** Oui (succ√®s/√©checs et nombre de pas).
  
**Commandes d'ex√©cution :**
```bash
git clone https://github.com/hrhouma/RLCode2-1.git
cd RLCode2-1
pip install venv
# Set-ExecutionPolicy Unrestricted -Scope CurrentUser -Force 
python3.11 -m venv mountain_car_env
#source mountain_car_env/bin/activate
mountain_car_env\Scripts\activate
pip install -r requirements.txt
python main.py
deactivate
```

-------------------
# Troubleshooting
-------------------

#### 1 -  Ex√©cutez le terminal en tant qu'administrateur
#### 2 -  La vesrsion de python utilis√©e dans le cadre de ce projet est la suivante Python 3.11.9

Si vous avez plusieurs versions de Python install√©es sur votre syst√®me et que vous souhaitez sp√©cifiquement utiliser Python 3.11, ex√©cutez la commande suivante pour cr√©er un environnement virtuel avec cette version :

```bash
python3.11 -m venv mountain_car_env
```

### ==> Cela forcera l'utilisation de Python 3.11 pour l'environnement virtuel `mountain_car_env`."

![image](https://github.com/user-attachments/assets/52d8a2b1-fe5c-4bba-a42e-42171fd219fc)



#### 3 -  Vous pouvez avoir une erreur √† cette √©tape (powershell)
  
```bash
mountain_car_env\Scripts\activate
```

### Voici l'erreur 

```bash
mountain_car_env\Scripts\activate
mountain_car_env\Scripts\activate : Impossible de charger le fichier
C:\Users\rehou\Documents\RLCode2-1\mountain_car_env\Scripts\Activate.ps1, car
l‚Äôex√©cution de scripts est d√©sactiv√©e sur ce syst√®me. Pour plus d‚Äôinformations,
consultez about_Execution_Policies √† l‚Äôadresse
https://go.microsoft.com/fwlink/?LinkID=135170.
Au caract√®re Ligne:1 : 1
+ mountain_car_env\Scripts\activate
+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    + CategoryInfo          : Erreur de s√©curit√©¬†: (:) [], PSSecurityException
    + FullyQualifiedErrorId : UnauthorizedAccess
```

### Solution 

```bash
Set-ExecutionPolicy Unrestricted -Scope CurrentUser -Force
```

---

### **2 - RLCode2-2** :
- **Ce que fait l'algorithme :** 
  - Dans cette version, l'agent est **entra√Æn√© et √©valu√©** avec plusieurs valeurs d'alpha : **0.1, 0.5, 0.9**.
  - Le programme affiche une **simulation en temps r√©el** o√π vous verrez plusieurs voitures (agents) bouger simultan√©ment, chacune utilisant une valeur diff√©rente d'**alpha**.
  - Des **graphes r√©capitulatifs** √† la fin montrent les **succ√®s/√©checs** ainsi que le **nombre moyen de pas** pour chaque alpha.
- **Rendu visuel :** Oui (affichage via `pygame`).
- **Graphiques :** Oui (r√©compenses, taux de succ√®s, nombre moyen de pas).

**Commandes d'ex√©cution :**
```bash
git clone https://github.com/hrhouma/RLCode2-2.git
cd RLCode2-2
python3 -m venv mountain_car
source mountain_car/bin/activate
pip install -r requirements.txt
python main1-pygame-alphas.py  # Avec rendu visuel
python main2-matplotlib-alphas.py  # Sans rendu visuel
deactivate
```

---

### **3 - RLCode2-3** :
- **Ce que fait l'algorithme :** 
  - Ce code ex√©cute plusieurs exp√©riences avec des valeurs d'**alpha** diff√©rentes : **0.1, 0.3, 0.5, 0.7, 0.9**.
  - Apr√®s chaque simulation, les r√©sultats sont enregistr√©s dans un fichier **`.pkl`**. Ensuite, un second script g√©n√®re des graphiques qui montrent l'impact des diff√©rentes valeurs d'**alpha** sur la **r√©compense moyenne** de l'agent.
  - C'est une version plus ax√©e sur l'analyse post-entra√Ænement, sans rendu visuel.
- **Rendu visuel :** Non.
- **Graphiques :** Oui (r√©compenses moyennes au fil des √©pisodes).

**Commandes d'ex√©cution :**
```bash
git clone https://github.com/hrhouma/RLCode2-3.git
cd RLCode2-3
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
python experiment.py
python visualize_results.py
deactivate
```

---

### **4 - RLCode2-4** :
- **Ce que fait l'algorithme :** 
  - Cette derni√®re version est similaire √† **RLCode2-2**, mais elle **compare plusieurs valeurs d'alpha** en g√©n√©rant des visualisations √† la fois **avec et sans rendu visuel**.
  - Chaque version teste diff√©rentes strat√©gies pour entra√Æner l'agent dans l'environnement **MountainCar**. 
  - Le programme compare les performances de l'agent avec des graphiques montrant le **nombre de pas n√©cessaires** pour atteindre le sommet ainsi que le **taux de succ√®s**.
- **Rendu visuel :** Oui.
- **Graphiques :** Oui (nombre de pas, taux de succ√®s).

**Commandes d'ex√©cution :**
```bash
git clone https://github.com/hrhouma/RLCode2-4.git
cd RLCode2-4
python3 -m venv mountain_car
source mountain_car/bin/activate
pip install -r requirements.txt
python main.py
deactivate
```

---

## üìä **Tableau comparatif des algorithmes :**

| **Version**       | **Rendu Visuel** | **Graphiques**           | **Alpha(s) Test√©(s)**         | **Objectif Principal**                    |
|-------------------|------------------|--------------------------|-------------------------------|-------------------------------------------|
| **RLCode2-1**      | Oui              | Succ√®s/√âchecs, Pas        | Œ± = 0.5                       | D√©monstration avec un seul alpha.         |
| **RLCode2-2**      | Oui (Pygame)     | Succ√®s/√âchecs, Pas        | Œ± = 0.1, 0.5, 0.9             | Comparaison avec plusieurs alphas.        |
| **RLCode2-3**      | Non              | R√©compense Moyenne        | Œ± = 0.1, 0.3, 0.5, 0.7, 0.9   | Comparaison des alphas via r√©compenses.   |
| **RLCode2-4**      | Oui              | Succ√®s, Pas               | Œ± multiples (√† d√©finir)        | Analyse combin√©e avec/sans visualisation. |

---

### üéØ **Questions √† vous poser :**

1. **Quel est l'impact d'alpha sur la performance de l'agent ?**
2. **Quelle valeur d'alpha donne les meilleurs r√©sultats en termes de stabilit√© et de rapidit√© d'apprentissage ?**
3. **Quels sont les avantages et les inconv√©nients des diff√©rentes approches ?**

### üîç **Conclusion :**

Prenez le temps de bien comprendre chaque version, de tester les diff√©rents scripts, d‚Äôanalyser les graphiques produits et de **comparer les performances**. N'oubliez pas de **documenter vos observations** et de **poser des questions** √† la fin de chaque pratique.

Bonne chance √† tous,  
**Votre professeur**
