# Introduction à l'évaluation des politiques avec la méthode Monte Carlo

Nous avons précédemment utilisé la **programmation dynamique (DP)** pour évaluer une politique, c'est-à-dire la fonction de valeur (ou **valeur d'état**) d'une politique donnée. Bien que la DP fonctionne bien, elle présente certaines **limitations** :
- Elle nécessite une **connaissance complète** de l'environnement, y compris la matrice de transitions et la matrice de récompenses.
  
Dans la réalité, la matrice de transitions n'est pas toujours connue à l'avance. Un algorithme d'apprentissage par renforcement qui nécessite de connaître un MDP (Modèle de Décision de Markov) est appelé un algorithme **basé sur un modèle**. 

D'un autre côté, les méthodes sans besoin de connaître ces informations à l'avance, comme les méthodes basées sur Monte Carlo, sont dites **sans modèle (model-free)**. Ces méthodes n'ont pas besoin de matrices de transition ou de récompenses pour fonctionner.

### Objectif

Dans cette vidéo, nous allons utiliser la méthode Monte Carlo pour **évaluer une politique**. Nous allons prendre l'exemple de l'environnement **FrozenLake** et supposer que nous n'avons pas accès à sa matrice de transition ni à sa matrice de récompenses.

---

### Comment fonctionne l'évaluation Monte Carlo ?

Nous utilisons le **retour empirique** (empirical mean return) au lieu du retour attendu (expected return) pour estimer la fonction de valeur.

#### Le retour d'un processus :

Le **retour** est la somme des récompenses obtenues à partir d'un moment donné dans le temps \(t\), pondéré par un facteur de discount \(\gamma\). Il est calculé par la formule suivante :
$$
G_t = \sum_k \gamma^k R_{t+k+1}
$$

Il existe deux façons d'évaluer une politique avec Monte Carlo :
1. **Prédiction Monte Carlo de première visite** (**First-visit MC**):
   - On fait la moyenne des retours uniquement pour la **première occurrence** d'un état donné dans un épisode.

2. **Prédiction Monte Carlo de toutes les visites** (**Every-visit MC**):
   - On fait la moyenne des retours pour **chaque occurrence** d'un état donné dans un épisode.

La prédiction **first-visit MC** est plus simple en termes de calculs, c'est pourquoi elle est souvent utilisée.

---

### Exemple avec l'environnement FrozenLake

Nous allons maintenant appliquer la méthode **First-visit Monte Carlo** pour évaluer une politique dans l'environnement FrozenLake. Voici les étapes :

1. **Importer les bibliothèques nécessaires** : Nous utilisons **PyTorch** et **Gym** pour créer une instance de l'environnement FrozenLake.
   
2. **Définir la fonction qui simule un épisode** dans FrozenLake, étant donné une politique, et qui retourne les récompenses et états pour chaque étape.
   
3. **Définir la fonction qui évalue la politique donnée** avec la méthode **first-visit MC**.

4. **Simuler 10 000 épisodes** : Nous fixons le taux de discount \(\gamma\) à 1 pour simplifier les calculs.

5. **Utiliser la politique optimale** obtenue dans la section précédente (en utilisant DP) et l'utiliser comme entrée pour notre fonction **First-visit MC**.

---

### Conclusion

L'évaluation des politiques avec Monte Carlo est une méthode **sans modèle**, qui est utile lorsque nous n'avons pas accès aux informations complètes sur l'environnement. Cela la rend flexible pour des situations réelles où les informations sur les transitions et les récompenses ne sont pas disponibles à l'avance.

