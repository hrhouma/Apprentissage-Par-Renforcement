# **Partie 2 : Contrôle Monte Carlo avec échantillonnage par importance pondérée – Implémentation Partie 1**

#### **Explication détaillée :**

Dans cette partie, nous abordons la première étape de l'implémentation d'un algorithme de **contrôle Monte Carlo (MC) avec échantillonnage par importance pondérée**. Cette méthode est utilisée pour apprendre la **Q-fonction** d'une politique cible, même lorsque les données sont générées à partir d'une politique différente, appelée la **politique de comportement**.

L’objectif ici est de collecter des informations sur les actions et récompenses lors de l'exécution d'un épisode en suivant la **politique de comportement**. Nous allons ensuite utiliser ces informations pour mettre à jour la **Q-fonction** de la politique cible en utilisant un échantillonnage pondéré.

#### **Concepts clés :**

1. **Politique de comportement** :
   - Il s'agit de la politique utilisée pour générer des actions lors de l'exploration de l'environnement. Dans notre cas, nous avons utilisé une **politique aléatoire**, où chaque action est sélectionnée avec une probabilité égale.
   - Cette politique n'est pas nécessairement optimisée pour maximiser les récompenses futures, mais elle permet d'explorer différents états et actions.

2. **Politique cible** :
   - Il s'agit de la politique que nous cherchons à optimiser. Nous utilisons les informations collectées à partir de la politique de comportement pour mettre à jour la **Q-fonction** de cette politique cible.
   - La **Q-fonction** (ou fonction de valeur d'action) est une estimation de la qualité d'une action dans un état donné. Elle nous permet de déterminer quelle action est la meilleure dans chaque état afin de maximiser les récompenses futures.

3. **Exécution d’un épisode** :
   - Nous utilisons une fonction pour exécuter un épisode complet sous la politique de comportement. Pendant cet épisode, nous enregistrons les **états**, les **actions**, et les **récompenses**.
   - Cette collecte de données est cruciale pour mettre à jour la Q-fonction via l’échantillonnage par importance pondérée.

---

#### **Étapes importantes de l'algorithme :**

1. **Exécuter un épisode avec la politique de comportement** :
   - L'épisode commence dans un état initial, et la politique de comportement (ici une politique aléatoire) choisit des actions jusqu'à ce que l'épisode se termine.
   - Pour chaque action prise, l'état résultant, la récompense obtenue, et l'action elle-même sont enregistrés.
   
2. **Collecte des données** :
   - À chaque étape de l'épisode, nous collectons :
     - Les **états visités**,
     - Les **actions prises**,
     - Les **récompenses reçues**.
   - Ces données sont ensuite utilisées pour **mettre à jour la Q-fonction de la politique cible** via une pondération par importance.

---

#### **Exemple de code pour exécuter un épisode** :

Voici la fonction qui exécute un épisode sous la **politique de comportement** et enregistre les informations nécessaires.

```python
def run_episode(env, behavior_policy):
    """
    Exécute un épisode avec la politique de comportement
    @param env: Environnement de Gym (Blackjack)
    @param behavior_policy: La politique de comportement utilisée pour générer des actions
    @return: Liste des états, actions et récompenses durant l'épisode
    """
    # Réinitialisation de l'environnement
    state = env.reset()
    
    # Listes pour stocker les états, actions et récompenses
    rewards = []
    actions = []
    states = []
    
    # Indicateur de fin d'épisode
    is_done = False

    # Boucle tant que l'épisode n'est pas terminé
    while not is_done:
        # Sélection d'une action selon la politique de comportement
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()  # Action échantillonnée aléatoirement
        
        # Enregistrer l'état et l'action
        actions.append(action)
        states.append(state)
        
        # Exécuter l'action dans l'environnement et obtenir la récompense
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)  # Stocker la récompense

        # Si l'épisode est terminé, sortir de la boucle
        if is_done:
            break
    
    # Retourner les listes des états, actions et récompenses
    return states, actions, rewards
```

#### **Explication détaillée du code** :

1. **Initialisation de l’épisode** :
   - On commence par réinitialiser l'environnement avec la méthode **`env.reset()`**. Cela nous permet de démarrer l'épisode dans un état de départ propre.
   
2. **Enregistrement des informations** :
   - Trois listes sont créées pour stocker les **états**, **actions**, et **récompenses** rencontrés au cours de l'épisode.
   
3. **Boucle d’exécution** :
   - La boucle **while** continue tant que l'épisode n'est pas terminé (**`is_done`** est **False**).
   - À chaque étape, la politique de comportement **sélectionne une action** en fonction de l’état actuel. Dans cet exemple, la politique de comportement renvoie des probabilités égales pour chaque action.
   - L’action est choisie à l’aide de **`torch.multinomial()`**, qui échantillonne une action selon les probabilités données.
   
4. **Interaction avec l’environnement** :
   - L’action sélectionnée est exécutée dans l’environnement via **`env.step(action)`**. Cette méthode retourne le nouvel état **`state`**, la récompense **`reward`**, et un indicateur de fin d’épisode **`is_done`**.
   
5. **Fin de l’épisode** :
   - Lorsque **`is_done`** est **True**, cela signifie que l’épisode est terminé (par exemple, dans le jeu de Blackjack, cela pourrait être lorsque la partie est gagnée ou perdue).

6. **Retour des données** :
   - Une fois l’épisode terminé, la fonction renvoie trois listes : les **états**, **actions**, et **récompenses**. Ces informations seront utilisées dans la prochaine étape pour ajuster la **Q-fonction** de la politique cible.

---

#### **Conclusion de la Partie 2** :

Dans cette première étape de l'implémentation, nous avons défini la **politique de comportement** et mis en place une fonction pour **exécuter un épisode** dans l'environnement tout en collectant les données nécessaires (états, actions, récompenses). Ces données seront cruciales pour la mise à jour de la **Q-fonction** via l'algorithme d'échantillonnage pondéré, que nous détaillerons dans la partie suivante.

Cette approche permet de **réutiliser les expériences** générées par la politique de comportement pour améliorer la **politique cible**, même si ces politiques sont différentes.






----------------------
# Annexe:
----------------------

### **Code complet pour le contrôle Monte Carlo avec échantillonnage par importance pondérée :**

```python
import torch
import gym

# Création de l'environnement Blackjack avec Gym
env = gym.make('Blackjack-v0')

# Génération d'une politique aléatoire
def gen_random_policy(n_action):
    probs = torch.ones(n_action) / n_action
    def policy_function(state):
        return probs
    return policy_function

random_policy = gen_random_policy(env.action_space.n)

# Exécution d'un épisode sous la politique de comportement
def run_episode(env, behavior_policy):
    """
    Exécution d'un épisode donné une politique de comportement
    @param env: OpenAI Gym environment
    @param behavior_policy: politique de comportement
    @return: états, actions, et récompenses de l'épisode entier
    """
    state = env.reset()  # Réinitialise l'environnement
    rewards = []
    actions = []
    states = []
    is_done = False
    while not is_done:
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
    return states, actions, rewards

# Implémentation du contrôle MC avec échantillonnage par importance pondérée
from collections import defaultdict

def mc_control_off_policy_weighted(env, gamma, n_episode, behavior_policy):
    """
    Obtention de la politique optimale avec l'échantillonnage par importance pondérée
    @param env: OpenAI Gym environment
    @param gamma: facteur de réduction
    @param n_episode: nombre d'épisodes
    @param behavior_policy: politique de comportement
    @return: fonction Q optimale et la politique optimale
    """
    n_action = env.action_space.n
    N = defaultdict(float)  # Nombre d'occurrences de chaque paire état-action
    Q = defaultdict(lambda: torch.empty(n_action))  # Fonction Q initialisée vide
    
    for episode in range(n_episode):
        w = 1.  # Poids initial
        # Exécute un épisode complet
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0
        # Parcours à l'envers des états, actions et récompenses
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t  # Calcul des retours pondérés
            N[(state_t, action_t)] += w  # Mise à jour de la pondération
            Q[state_t][action_t] += (w/N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])  # Mise à jour de la fonction Q
            # Si l'action ne correspond pas à l'action optimale, on arrête
            if action_t != torch.argmax(Q[state_t]).item():
                break
            # Mise à jour du poids
            w *= 1. / behavior_policy(state_t)[action_t]    
    # Création de la politique optimale
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy

gamma = 1  # Facteur de réduction

n_episode = 500000  # Nombre d'épisodes

# Exécution de l'algorithme pour obtenir la politique optimale
optimal_Q, optimal_policy = mc_control_off_policy_weighted(env, gamma, n_episode, random_policy)

# Simulation d'épisodes pour tester la politique optimale obtenue
def simulate_episode(env, policy):
    state = env.reset()
    is_done = False
    while not is_done:
        action = policy[state]  # Choix de l'action selon la politique optimale
        state, reward, is_done, info = env.step(action)
        if is_done:
            return reward  # Retourne la récompense

# Simulation de plusieurs épisodes
n_episode = 100000
n_win_optimal = 0  # Nombre de victoires
n_lose_optimal = 0  # Nombre de défaites

for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_optimal += 1
    elif reward == -1:
        n_lose_optimal += 1

# Affichage des probabilités de victoire et de défaite
print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))
print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))
```

---

### **Explication étape par étape :**

1. **Création de l'environnement :**
   - Utilisation de `gym.make('Blackjack-v0')` pour créer un environnement de Blackjack.
   - Cet environnement permet de simuler des parties de Blackjack où l'algorithme doit apprendre à prendre les meilleures décisions.

2. **Politique aléatoire :**
   - La fonction `gen_random_policy` génère une politique qui sélectionne chaque action avec une probabilité égale.
   - Cela sert de **politique de comportement** pour explorer l'environnement.

3. **Exécution d'un épisode :**
   - La fonction `run_episode` simule un épisode complet sous la politique de comportement, en enregistrant les états, actions, et récompenses.
   - Cette information est utilisée pour ajuster la politique cible via l'échantillonnage pondéré.

4. **Contrôle Monte Carlo Off-Policy avec échantillonnage pondéré :**
   - La fonction `mc_control_off_policy_weighted` utilise les données de l'épisode pour mettre à jour la fonction Q via l'échantillonnage pondéré.
   - Pour chaque paire état-action rencontrée, le retour est mis à jour en fonction des poids calculés.
   - La politique optimale est obtenue en choisissant l'action avec la valeur Q la plus élevée pour chaque état.

5. **Simulation des épisodes sous la politique optimale :**
   - Après avoir obtenu la politique optimale, la fonction `simulate_episode` exécute plusieurs épisodes pour tester la performance de la politique.
   - Les probabilités de victoire et de défaite sont ensuite calculées.

---

### **Résultats :**
En exécutant le code, vous verrez des statistiques comme :
- **Probabilité de gagner** sous la politique optimale.
- **Probabilité de perdre** sous la politique optimale.
