# Partie 3:

Dans cette partie, nous allons revoir la manière d'implémenter une simulation de Blackjack et d'utiliser la méthode Monte Carlo pour prédire la valeur des états (la valeur estimée de chaque situation ou "state").

### 1. Simulation d'un épisode de Blackjack
La première étape consiste à simuler un épisode du jeu de Blackjack. L'idée est de suivre un joueur jouant selon une certaine politique (règle de décision), jusqu'à ce que l'épisode se termine.

Nous avons ici un code pour la simulation d'un épisode :

```python
def run_episode(env, hold_score):
    state = env.reset()
    rewards = []
    states = [state]
    is_done = False
    while not is_done:
        action = 1 if state[0] < hold_score else 0  # 'hit' (1) si le score est inférieur à la limite, sinon 'stick' (0)
        state, reward, is_done, info = env.step(action)
        states.append(state)
        rewards.append(reward)
        if is_done:
            break
    return states, rewards
```

### Explication :
- **state** : représente l'état actuel du jeu (score du joueur, carte visible du croupier, etc.).
- **env.reset()** : initialise un nouvel épisode, renvoyant l'état initial.
- **env.step(action)** : effectue l'action choisie ('hit' ou 'stick'), renvoie le nouvel état, la récompense, et indique si l'épisode est terminé.
- **hold_score** : la politique ici est simple : le joueur tire des cartes tant que son score est inférieur à 18, et s'arrête sinon.

### 2. Implémentation de la prédiction Monte Carlo (première visite)
Une fois l'épisode terminé, il est temps d'utiliser la méthode Monte Carlo pour estimer la valeur des états rencontrés pendant l'épisode.

```python
from collections import defaultdict

def mc_prediction_first_visit(env, hold_score, gamma, n_episode):
    V = defaultdict(float)  # Valeur des états
    N = defaultdict(int)     # Nombre de visites pour chaque état
    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, hold_score)  # Exécute un épisode
        return_t = 0
        G = {}
        for state_t, reward_t in zip(states_t[1:][::-1], rewards_t[::-1]):  # Remonte dans les états
            return_t = gamma * return_t + reward_t
            G[state_t] = return_t
        for state, return_t in G.items():
            if state[0] <= 21:  # Seuls les états avec des scores <= 21 sont considérés valides
                V[state] += return_t
                N[state] += 1
    for state in V:
        V[state] = V[state] / N[state]  # Moyenne des retours
    return V
```

### Explication :
- **gamma** : c'est le facteur d'actualisation qui pondère les récompenses futures.
- **n_episode** : le nombre d'épisodes simulés.
- **V** : le dictionnaire qui stocke les valeurs des états.
- **N** : le dictionnaire qui stocke le nombre de fois qu'un état a été visité.
- **G[state_t]** : retour cumulé (reward) de chaque état à la première visite.

### 3. Exécution et affichage des résultats

Nous avons défini une politique simple où le joueur continue à tirer des cartes tant que son score est inférieur à 18.

```python
hold_score = 18
gamma = 1
n_episode = 500000

value = mc_prediction_first_visit(env, hold_score, gamma, n_episode)

print('The value function calculated by first-visit MC prediction:\n', value)
print('Number of states:', len(value))
```

### 4. Résultat
Le résultat est une estimation de la valeur de chaque état (c'est-à-dire, quelle est la probabilité pour un joueur, dans un état donné, de gagner, perdre, ou faire match nul). 

Ce modèle, avec plus d'épisodes simulés, permet d'améliorer la précision des prédictions.

### Exercice :
Testez avec différentes politiques :
1. Par exemple, en changeant la valeur du **hold_score** (18, 19, 20).
2. Comparer les résultats obtenus selon ces différentes politiques.

Cette méthode permet de comprendre comment les actions prises à chaque état influencent les résultats à long terme.
