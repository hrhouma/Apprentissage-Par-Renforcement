# Introduction à l'algorithme Epsilon-Greedy appliqué au contrôle Monte Carlo en On-Policy

L'algorithme Epsilon-Greedy est une méthode essentielle pour résoudre les problèmes de **trade-off** entre **exploration** et **exploitation**. Ce compromis est crucial dans les méthodes d'apprentissage par renforcement. Dans ce cours, nous allons explorer en détail comment le contrôle Monte Carlo avec la politique Epsilon-Greedy est utilisé pour apprendre une politique optimale pour le jeu de Blackjack, en mettant en œuvre un algorithme basé sur la bibliothèque `gym` d'OpenAI.

### Partie 1 : Compréhension du principe Epsilon-Greedy

L'objectif principal de l'approche **Epsilon-Greedy** est d'éviter de se limiter à l'exploitation des meilleures actions découvertes jusqu'à présent et d'encourager l'exploration de nouvelles actions qui pourraient être plus prometteuses à long terme.

#### 1.1 Exploration vs Exploitation
- **Exploitation** : Il s'agit de choisir l'action avec la plus haute valeur actuelle de Q (l'action que l'on pense être la meilleure pour un état donné).
- **Exploration** : Choisir aléatoirement une action différente permet d'explorer de nouvelles stratégies qui pourraient éventuellement offrir de meilleurs résultats à long terme.

Avec **l'algorithme Epsilon-Greedy**, l'action optimale est choisie la plupart du temps, mais avec une probabilité epsilon (`ε`), une action aléatoire est sélectionnée pour favoriser l'exploration.

### Partie 2 : Étapes détaillées de l'algorithme

Nous allons décrire les différentes étapes nécessaires pour mettre en œuvre l'algorithme avec une approche détaillée.

#### Étape 1 : Initialisation de l'environnement
Nous commençons par initialiser l'environnement de simulation `Blackjack` fourni par OpenAI Gym.

```python
import torch
import gym

env = gym.make('Blackjack-v0')
```

L'environnement représente un jeu de Blackjack simplifié où l'objectif est de maximiser le score sans dépasser 21 points.

#### Étape 2 : Implémentation de la fonction d'exécution d'un épisode avec Epsilon-Greedy

Nous implémentons une fonction `run_episode` qui exécute un épisode en suivant la politique Epsilon-Greedy. Voici les tâches essentielles de cette étape :

- **Initialisation de l'épisode**.
- **Calcul des probabilités de choisir une action** : Pour l'action optimale (celle avec la meilleure valeur Q actuelle), la probabilité est calculée comme suit :

$$\pi(s,a) = 1 - \epsilon + \frac{\epsilon}{|A|}$$

Sinon, la probabilité de choisir une autre action est :

$$\pi(s,a) = \frac{\epsilon}{|A|}$$

- **Enregistrement des états, actions, et récompenses** pour chaque étape de l'épisode, qui seront utilisés lors de la phase d'évaluation.

```python
def run_episode(env, Q, epsilon, n_action):
    state = env.reset()
    rewards = []
    actions = []
    states = []
    is_done = False
    while not is_done:
        probs = torch.ones(n_action) * epsilon / n_action
        best_action = torch.argmax(Q[state]).item()
        probs[best_action] += 1.0 - epsilon
        action = torch.multinomial(probs, 1).item()
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
    return states, actions, rewards
```

#### Étape 3 : Amélioration de la politique avec Epsilon-Greedy

La politique Epsilon-Greedy améliore la méthode de recherche gloutonne en exploitant l'action ayant la plus grande valeur avec une probabilité de $$1 - \epsilon + \frac{\epsilon}{|A|}$$, tout en permettant une exploration avec une probabilité de $$\frac{\epsilon}{|A|}$$.

- Le paramètre **Epsilon** est un hyperparamètre qui fait le lien entre l'exploration et l'exploitation. Si sa valeur est 0, l'algorithme devient totalement glouton, c'est-à-dire qu'il choisit toujours l'action ayant la plus grande valeur. Si sa valeur est 1, chaque action est choisie de manière aléatoire.
  
- Une technique courante consiste à commencer avec une valeur d'Epsilon légèrement plus grande (par exemple, 0,5 ou 0,7) et à la réduire progressivement au fil du temps pour privilégier l'exploitation des bonnes actions après une phase d'exploration suffisante.

```python
def mc_control_epsilon_greedy(env, gamma, n_episode, epsilon):
    n_action = env.action_space.n
    G_sum = defaultdict(float)
    N = defaultdict(int)
    Q = defaultdict(lambda: torch.empty(n_action))
    for episode in range(n_episode):
        states_t, actions_t, rewards_t = run_episode(env, Q, epsilon, n_action)
        return_t = 0
        G = {}
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            G[(state_t, action_t)] = return_t
        for state_action, return_t in G.items():
            state, action = state_action
            if state[0] <= 21:
                G_sum[state_action] += return_t
                N[state_action] += 1
                Q[state][action] = G_sum[state_action] / N[state_action]
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy
```

### Partie 3 : Évaluation des performances

Après avoir effectué la simulation de l'algorithme Epsilon-Greedy sur 500 000 épisodes, nous calculons et comparons les probabilités de victoire et de défaite.

#### Étape 6 : Résultats et interprétations

En moyenne, après avoir exécuté 100 000 épisodes, nous avons obtenu les résultats suivants :

```python
print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))
```

La **probabilité de victoire** sous la politique optimale obtenue avec Epsilon-Greedy est de **42,44%**, ce qui est supérieur à la probabilité de victoire sans Epsilon-Greedy (**41,28%**).

```python
print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))
```

La **probabilité de défaite** est plus faible avec Epsilon-Greedy (48,05%) par rapport à sans Epsilon-Greedy (49,3%).

### Conclusion

- L'algorithme Epsilon-Greedy surpasse la méthode gloutonne classique en équilibrant l'exploration et l'exploitation. 
- Ce processus permet d'obtenir une politique plus performante avec une plus grande probabilité de victoire et une probabilité réduite de défaite dans un environnement incertain comme le Blackjack.
- La gestion du paramètre Epsilon est cruciale, et son ajustement progressif permet d'optimiser les résultats à long terme.
