### Contexte et Objectif

L'objectif est de **calculer la fonction de valeur** pour une politique donnée dans un environnement, en l'occurrence **FrozenLake**, en utilisant la méthode **Monte Carlo**. Nous allons voir comment cela fonctionne en simulant plusieurs épisodes, en collectant les récompenses, et en estimant la valeur des états.

---

## Étape 1 : Comprendre le problème

Nous voulons savoir combien "vaut" un état donné dans l'environnement **FrozenLake** si nous suivons une certaine politique. La fonction de valeur **\(V(s)\)** nous dit la récompense attendue à long terme si nous commençons dans l'état **\(s\)** et suivons la politique donnée.

### Environnement FrozenLake

FrozenLake est un environnement où un agent doit se déplacer sur une surface glacée pour atteindre un but, en évitant les trous. À chaque étape, l'agent reçoit une récompense (0 ou 1), et son but est d'atteindre la case finale qui lui donne une récompense de 1.

### Politique

Une **politique** est simplement une stratégie que l'agent suit. Elle dit à l'agent quelle action prendre dans chaque état. Dans cet exemple, nous allons utiliser une **politique optimale prédéfinie**.

---

## Étape 2 : Créer l'environnement

Nous allons utiliser la bibliothèque **Gym** pour créer l'environnement **FrozenLake**. Nous utiliserons également **PyTorch** pour stocker et manipuler les données.

### Code :

```python
import torch
import gym

# Créer l'environnement FrozenLake
env = gym.make('FrozenLake-v0')
```

### Explication :

- **gym.make('FrozenLake-v0')** : Cette commande crée une instance de l'environnement FrozenLake. C'est ici que notre agent va évoluer.
  
---

## Étape 3 : Simuler un épisode

Un **épisode** est une séquence d'actions prises par l'agent dans l'environnement, jusqu'à ce que l'épisode se termine (soit parce que l'agent atteint le but, soit parce qu'il tombe dans un trou). À chaque étape, l'agent effectue une action, et l'environnement lui donne une récompense.

Nous allons écrire une fonction pour exécuter un épisode, c'est-à-dire simuler une séquence d'actions.

### Code :

```python
def run_episode(env, policy):
    state = env.reset()  # Réinitialiser l'environnement au début de l'épisode
    rewards = []  # Stocker les récompenses
    states = [state]  # Stocker les états visités
    is_done = False  # Variable pour savoir si l'épisode est terminé
    
    while not is_done:  # Continuer tant que l'épisode n'est pas terminé
        action = policy[state].item()  # Sélectionner l'action selon la politique
        state, reward, is_done, info = env.step(action)  # Effectuer l'action et obtenir le nouvel état et la récompense
        states.append(state)  # Ajouter l'état à la liste des états visités
        rewards.append(reward)  # Ajouter la récompense à la liste des récompenses
        if is_done:  # Si l'épisode est terminé
            break
    
    states = torch.tensor(states)  # Convertir les états en tenseur PyTorch
    rewards = torch.tensor(rewards)  # Convertir les récompenses en tenseur PyTorch
    return states, rewards  # Retourner les états et les récompenses
```

### Explication :

1. **env.reset()** : Réinitialise l'environnement à son état initial (commence un nouvel épisode).
2. **policy[state].item()** : L'agent choisit une action en fonction de la politique pour l'état actuel.
3. **env.step(action)** : L'environnement exécute l'action et renvoie le nouvel état, la récompense, et si l'épisode est terminé.
4. **states.append(state)** et **rewards.append(reward)** : Nous stockons les états visités et les récompenses reçues pour pouvoir les utiliser plus tard.

---

## Étape 4 : Prédiction Monte Carlo de Première Visite

La **prédiction Monte Carlo** consiste à simuler plusieurs épisodes, puis à utiliser les récompenses pour estimer la fonction de valeur. Avec la méthode de **première visite**, nous faisons la moyenne des retours uniquement pour la première fois qu'un état est visité dans chaque épisode.

### Code :

```python
def mc_prediction_first_visit(env, policy, gamma, n_episode):
    n_state = policy.shape[0]  # Nombre d'états dans la politique
    V = torch.zeros(n_state)  # Initialisation de la fonction de valeur
    N = torch.zeros(n_state)  # Compteur de visites pour chaque état

    for episode in range(n_episode):  # Simuler plusieurs épisodes
        states_t, rewards_t = run_episode(env, policy)  # Obtenir les états et récompenses d'un épisode
        return_t = 0  # Initialiser le retour à zéro
        first_visit = torch.zeros(n_state)  # Suivi des premières visites
        G = torch.zeros(n_state)  # Retour cumulé pour chaque état
        
        # Calculer les retours en parcourant les états en sens inverse
        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):
            return_t = gamma * return_t + reward_t  # Calculer le retour actualisé
            G[state_t] = return_t  # Mettre à jour le retour pour cet état
            first_visit[state_t] = 1  # Marquer la première visite de cet état

        # Mettre à jour la fonction de valeur en utilisant la première visite
        for state in range(n_state):
            if first_visit[state] > 0:
                V[state] += G[state]  # Ajouter le retour à la fonction de valeur
                N[state] += 1  # Incrémenter le compteur de visites

    # Calculer la moyenne des retours pour chaque état
    for state in range(n_state):
        if N[state] > 0:
            V[state] = V[state] / N[state]  # Moyenne des retours
    
    return V  # Retourner la fonction de valeur
```

### Explication :

1. **V = torch.zeros(n_state)** : Nous initialisons la fonction de valeur pour chaque état à 0.
2. **run_episode(env, policy)** : Nous simulons un épisode et récupérons les états visités et les récompenses reçues.
3. **return_t = gamma * return_t + reward_t** : Nous calculons le retour actualisé en fonction du facteur de discount \(\gamma\).
4. **V[state] += G[state]** : Nous mettons à jour la fonction de valeur en ajoutant le retour pour chaque état visité.

---

## Étape 5 : Exécution de l'algorithme

Nous allons maintenant exécuter l'algorithme en utilisant une politique optimale prédéfinie pour FrozenLake, et calculer la fonction de valeur après avoir simulé 10 000 épisodes.

### Code :

```python
gamma = 1  # Facteur de discount
n_episode = 10000  # Nombre d'épisodes à simuler

# Politique optimale prédéfinie pour FrozenLake
optimal_policy = torch.tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])

# Calculer la fonction de valeur avec la prédiction MC de première visite
value = mc_prediction_first_visit(env, optimal_policy, gamma, n_episode)

print('La fonction de valeur calculée par la prédiction Monte Carlo de première visite :\n', value)
```

### Explication :

1. **gamma = 1** : Nous utilisons un facteur de discount de 1 (aucune dépréciation des récompenses futures).
2. **n_episode = 10000** : Nous simulons 10 000 épisodes pour obtenir des estimations précises de la fonction de valeur.
3. **optimal_policy** : La politique optimale prédéfinie que l'agent suit.

---

## Conclusion

En suivant ces étapes, les étudiants peuvent comprendre comment la méthode Monte Carlo est utilisée pour évaluer une politique. Chaque étape est décomposée pour faciliter la compréhension des concepts de base, comme la simulation d'épisodes, le calcul des retours, et l'estimation de la fonction de valeur.

