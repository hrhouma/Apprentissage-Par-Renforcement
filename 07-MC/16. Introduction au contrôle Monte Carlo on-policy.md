# Contrôle Monte Carlo pour le Blackjack optimal

## Introduction générale

Le **Contrôle Monte Carlo** vise à trouver une politique optimale dans un environnement inconnu. Contrairement à la prédiction Monte Carlo qui évalue une politique donnée, le contrôle Monte Carlo détermine la meilleure politique à adopter. Deux approches principales existent :

1. **On-policy** : La politique est appliquée et améliorée simultanément.
2. **Off-policy** : La politique est améliorée en utilisant des données générées par une autre politique.

## Étapes détaillées

### 1. Évaluation et amélioration alternées

Le contrôle Monte Carlo alterne entre deux phases :

a) **Évaluation** : L'algorithme évalue la fonction de valeur-action Q(s,a), représentant l'utilité de chaque paire état-action (s,a).

b) **Amélioration** : La politique est mise à jour en choisissant l'action optimale :

$$\pi(s) = \arg\max_a Q(s,a)$$

### 2. Calcul de la fonction de valeur d'action Q(s,a)

L'algorithme simule de nombreuses parties de Blackjack (par exemple, 500 000 épisodes). À chaque épisode, il réévalue les états-actions rencontrés et met à jour Q(s,a) en tenant compte des récompenses obtenues.

## Algorithme détaillé (On-policy Monte Carlo Control)

1. **Initialisation** :
   - Créer un environnement Blackjack avec `gym`.
   - Définir une fonction pour exécuter un épisode selon une politique donnée.

2. **Phase d'évaluation (calcul de Q)** :
   - Exécuter des épisodes et mettre à jour Q(s,a) pour chaque action prise.
   - Calculer la valeur des actions basée sur les retours observés.

3. **Phase d'amélioration** :
   - Après chaque épisode, mettre à jour la politique en choisissant l'action maximisant Q(s,a).
   - Répéter jusqu'à convergence vers la meilleure action pour chaque état.

4. **Visualisation des résultats** :
   - Créer des visualisations des fonctions de valeur optimales avec `matplotlib` et `plot_surface`.

## Exemple de code

```python
import gym
import numpy as np
from collections import defaultdict

env = gym.make('Blackjack-v0')

def run_episode(env, policy):
    states, actions, rewards = [], [], []
    state = env.reset()
    while True:
        states.append(state)
        action = policy[state]
        actions.append(action)
        state, reward, done, _ = env.step(action)
        rewards.append(reward)
        if done:
            break
    return states, actions, rewards

def on_policy_mc_control(env, n_episodes, gamma=1.0, epsilon=0.1):
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
    returns_sum = defaultdict(float)
    returns_count = defaultdict(float)
    policy = defaultdict(lambda: np.ones(env.action_space.n) / env.action_space.n)

    for _ in range(n_episodes):
        states, actions, rewards = run_episode(env, policy)
        G = 0
        for t in reversed(range(len(states))):
            G = gamma * G + rewards[t]
            sa_pair = (states[t], actions[t])
            if sa_pair not in zip(states[:t], actions[:t]):  # Première visite
                returns_sum[sa_pair] += G
                returns_count[sa_pair] += 1
                Q[sa_pair[0]][sa_pair[1]] = returns_sum[sa_pair] / returns_count[sa_pair]
                policy[states[t]] = np.argmax(Q[states[t]])  # Mise à jour de la politique
    return Q, policy
```

## Conclusion

- Après un nombre suffisant d'épisodes, cet algorithme devrait converger vers une politique optimale maximisant les gains du joueur dans diverses situations de Blackjack. 
- Cette approche démontre la puissance des méthodes Monte Carlo pour résoudre des problèmes complexes sans connaissance préalable du modèle de l'environnement.
