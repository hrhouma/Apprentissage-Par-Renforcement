# Contrôle Monte Carlo pour le Blackjack optimal

## Introduction générale

Le **Contrôle Monte Carlo** vise à trouver une politique optimale dans un environnement inconnu. Contrairement à la prédiction Monte Carlo qui évalue une politique donnée, le contrôle Monte Carlo détermine la meilleure politique à adopter. Deux approches principales existent :

1. **On-policy** : La politique est appliquée et améliorée simultanément.
2. **Off-policy** : La politique est améliorée en utilisant des données générées par une autre politique.

## Étapes détaillées

### 1. Évaluation et amélioration alternées

Le contrôle Monte Carlo alterne entre deux phases :

a) **Évaluation** : L'algorithme évalue la fonction de valeur-action Q(s,a), représentant l'utilité de chaque paire état-action (s,a).

b) **Amélioration** : La politique est mise à jour en choisissant l'action optimale :

$$\pi(s) = \arg\max_a Q(s,a)$$

### 2. Calcul de la fonction de valeur d'action Q(s,a)

L'algorithme simule de nombreuses parties de Blackjack (par exemple, 500 000 épisodes). À chaque épisode, il réévalue les états-actions rencontrés et met à jour Q(s,a) en tenant compte des récompenses obtenues.

## Algorithme détaillé (On-policy Monte Carlo Control)

1. **Initialisation** :
   - Créer un environnement Blackjack avec `gym`.
   - Définir une fonction pour exécuter un épisode selon une politique donnée.

2. **Phase d'évaluation (calcul de Q)** :
   - Exécuter des épisodes et mettre à jour Q(s,a) pour chaque action prise.
   - Calculer la valeur des actions basée sur les retours observés.

3. **Phase d'amélioration** :
   - Après chaque épisode, mettre à jour la politique en choisissant l'action maximisant Q(s,a).
   - Répéter jusqu'à convergence vers la meilleure action pour chaque état.

4. **Visualisation des résultats** :
   - Créer des visualisations des fonctions de valeur optimales avec `matplotlib` et `plot_surface`.

## Exemple de code

```python
import gym
import numpy as np
from collections import defaultdict

env = gym.make('Blackjack-v0')

def run_episode(env, policy):
    states, actions, rewards = [], [], []
    state = env.reset()
    while True:
        states.append(state)
        action = policy[state]
        actions.append(action)
        state, reward, done, _ = env.step(action)
        rewards.append(reward)
        if done:
            break
    return states, actions, rewards

def on_policy_mc_control(env, n_episodes, gamma=1.0, epsilon=0.1):
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
    returns_sum = defaultdict(float)
    returns_count = defaultdict(float)
    policy = defaultdict(lambda: np.ones(env.action_space.n) / env.action_space.n)

    for _ in range(n_episodes):
        states, actions, rewards = run_episode(env, policy)
        G = 0
        for t in reversed(range(len(states))):
            G = gamma * G + rewards[t]
            sa_pair = (states[t], actions[t])
            if sa_pair not in zip(states[:t], actions[:t]):  # Première visite
                returns_sum[sa_pair] += G
                returns_count[sa_pair] += 1
                Q[sa_pair[0]][sa_pair[1]] = returns_sum[sa_pair] / returns_count[sa_pair]
                policy[states[t]] = np.argmax(Q[states[t]])  # Mise à jour de la politique
    return Q, policy
```

## Conclusion

- Après un nombre suffisant d'épisodes, cet algorithme devrait converger vers une politique optimale maximisant les gains du joueur dans diverses situations de Blackjack. 
- Cette approche démontre la puissance des méthodes Monte Carlo pour résoudre des problèmes complexes sans connaissance préalable du modèle de l'environnement.

# Annexe :


Pour expliquer en détails l'algorithme de contrôle Monte Carlo pour jouer au Blackjack de manière optimale, voici les points clés  :

### Introduction générale
L'objectif de **Monte Carlo Control** est de trouver une politique optimale lorsque celle-ci n'est pas donnée. Contrairement à la **prédiction Monte Carlo** qui évalue une politique donnée, le contrôle Monte Carlo permet de déterminer la meilleure politique à adopter dans un environnement. Il existe deux approches principales : **on-policy** (politique appliquée en même temps qu'elle est améliorée) et **off-policy** (amélioration de la politique en utilisant des données générées par une autre politique).

### Étapes détaillées
#### 1. **Évaluation et amélioration alternées**
Le contrôle Monte Carlo s'apparente à une itération de politique où deux phases s’alternent :

1. **Évaluation** : Ici, l'algorithme évalue la "valeur-action" (appelée fonction **Q**), c'est-à-dire l'utilité de chaque paire état-action \( (s, a) \) en exécutant l'action \( a \) dans l'état \( s \) sous une politique donnée. Cette évaluation peut se faire en utilisant une approche de première visite ou de toutes les visites.
   
2. **Amélioration** : Ensuite, la politique est mise à jour en attribuant l’action optimale \( \pi(s) = \arg\max_a Q(s, a) \) pour chaque état.

#### 2. Calcul de la fonction de valeur d'action **$$Q(s, a)$$**
Dans chaque épisode, l'algorithme simule un certain nombre de parties de Blackjack (par exemple, 500 000 épisodes). À la fin de chaque épisode, il réévalue chaque état-action rencontré dans la partie et met à jour la fonction **\( Q(s, a) \)** pour chaque paire état-action **$$(s, a)$$** en prenant en compte le retour de récompenses de cette action dans cet état.

### Algorithme détaillé
Voici les étapes de l’algorithme **on-policy Monte Carlo Control** en pratique :

1. **Initialisation** :
   - Créez un environnement de Blackjack en utilisant la bibliothèque `gym` de Python.
   - Définissez une fonction qui exécute un épisode de Blackjack et prend des actions selon une politique définie.
   
2. **Phase d'évaluation (calcul de Q)** :
   - Exécutez des épisodes, mettez à jour \( Q(s, a) \) à chaque fois qu’une action est prise dans un état donné.
   - Calculez la valeur des actions prises dans différents états en vous basant sur les retours observés.

3. **Phase d'amélioration** :
   - Après chaque épisode, mettez à jour la politique en choisissant toujours l'action qui maximise la valeur \( Q(s, a) \).
   - L’objectif est que, après un nombre suffisant d’itérations, la politique converge vers la meilleure action possible pour chaque état.

4. **Visualisation des résultats** :
   - Créez des visualisations pour examiner les fonctions de valeur des politiques optimales en utilisant `matplotlib` et `plot_surface`.

### Exemple de code
```python
import gym
import numpy as np
from collections import defaultdict

# Création de l'environnement Blackjack
env = gym.make('Blackjack-v0')

def run_episode(env, policy):
    states, actions, rewards = [], [], []
    state = env.reset()
    while True:
        states.append(state)
        action = policy[state]  # Politique définie par Q(s, a)
        actions.append(action)
        state, reward, done, _ = env.step(action)
        rewards.append(reward)
        if done:
            break
    return states, actions, rewards

def on_policy_mc_control(env, n_episodes, gamma=1.0, epsilon=0.1):
    Q = defaultdict(lambda: np.zeros(env.action_space.n))
    returns_sum = defaultdict(float)
    returns_count = defaultdict(float)
    policy = defaultdict(lambda: np.ones(env.action_space.n) / env.action_space.n)

    for _ in range(n_episodes):
        states, actions, rewards = run_episode(env, policy)
        G = 0
        for t in reversed(range(len(states))):
            G = gamma * G + rewards[t]
            sa_pair = (states[t], actions[t])
            if sa_pair not in zip(states[:t], actions[:t]):  # Première visite
                returns_sum[sa_pair] += G
                returns_count[sa_pair] += 1
                Q[sa_pair[0]][sa_pair[1]] = returns_sum[sa_pair] / returns_count[sa_pair]
                policy[states[t]] = np.argmax(Q[states[t]])  # Politique mise à jour avec l'action optimale
    return Q, policy
```

### Conclusion
En fin de compte, après avoir suffisamment exécuté cet algorithme avec un grand nombre d’épisodes, vous devriez obtenir une politique qui maximise les gains pour le joueur dans différentes situations de Blackjack.
