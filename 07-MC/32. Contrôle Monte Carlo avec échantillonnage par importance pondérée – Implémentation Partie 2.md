
Pour diviser les explications du code et des concepts associés en quatre parties, je vais organiser les sections en fonction des parties que vous avez mentionnées, tout en veillant à ce que chaque partie soit exhaustive et bien expliquée.

---

### **Partie 1 : 29. Introduction au contrôle Monte Carlo avec échantillonnage par importance pondérée**

**Explication :**
Dans cette première partie, nous introduisons le contrôle Monte Carlo (MC) avec échantillonnage par importance pondérée. Ce type d'algorithme MC permet de résoudre des problèmes de prise de décision où la politique de comportement (celle qui génère les données) est différente de la politique cible (celle que l'on souhaite optimiser). L'avantage de l'échantillonnage par importance est qu'il permet d'utiliser des données générées par une autre politique pour améliorer la politique cible.

**Images associées :**
1. Image de l'introduction "Introduction to Developing MC control with weighted importance sampling".
2. Explication sur l'importance pondérée par rapport à l'échantillonnage classique :  
    ![Equation Importance Sampling](https://example.com/image/importance-sampling.png)
3. Exemple montrant la variance plus faible du weighted importance sampling par rapport à l'importance sampling classique.

**Extrait du code :**
```python
import torch
import gym

env = gym.make('Blackjack-v0')

def gen_random_policy(n_action):
    probs = torch.ones(n_action) / n_action
    def policy_function(state):
        return probs
    return policy_function

random_policy = gen_random_policy(env.action_space.n)
```

Ici, nous avons un exemple de génération de politique aléatoire qui sélectionne chaque action avec une probabilité égale. Cette fonction servira de politique de comportement pour générer des actions aléatoires dans le cadre de l'algorithme.

---

### **Partie 2 : 30. Contrôle Monte Carlo avec échantillonnage par importance pondérée – Implémentation Partie 1**

**Explication :**
Dans cette section, nous allons implémenter la première partie du contrôle Monte Carlo avec échantillonnage pondéré. Nous allons exécuter un épisode en utilisant une politique de comportement, enregistrer les états, actions, et récompenses, puis utiliser ces données pour ajuster la politique cible via l'échantillonnage pondéré.

**Images associées :**
1. Étapes de l'implémentation :  
   - Définir la politique de comportement.
   - Réutiliser la fonction pour exécuter un épisode.
   - Implémentation de l'algorithme off-policy MC avec échantillonnage pondéré.

2. Une représentation visuelle de la mise à jour de la Q-fonction.

**Extrait du code :**
```python
def run_episode(env, behavior_policy):
    state = env.reset()
    rewards = []
    actions = []
    states = []
    is_done = False
    while not is_done:
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
    return states, actions, rewards
```

Dans cet extrait, la fonction `run_episode` exécute un épisode complet sous la politique de comportement, et enregistre toutes les informations nécessaires pour ajuster la politique cible par la suite.

---

### **Partie 3 : 31. Explication du contrôle Monte Carlo avec échantillonnage par importance pondérée**

**Explication :**
Cette partie explore les concepts théoriques derrière l'échantillonnage par importance pondérée, en mettant en évidence la manière dont les retours sont mis à jour en fonction des ratios d'importance. L'idée est de pondérer les retours par les probabilités associées à la politique de comportement par rapport à la politique cible.

**Images associées :**
1. Visualisation de l'équation du weighted importance sampling.  
   ![Weighted Importance Sampling](https://example.com/image/weighted-sampling.png)

**Extrait du code :**
```python
from collections import defaultdict

def mc_control_off_policy_weighted(env, gamma, n_episode, behavior_policy):
    n_action = env.action_space.n
    N = defaultdict(float)
    Q = defaultdict(lambda: torch.empty(n_action))
    for episode in range(n_episode):
        w = 1.
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            N[(state_t, action_t)] += w
            Q[state_t][action_t] += (w/N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])
            if action_t != torch.argmax(Q[state_t]).item():
                break
            w *= 1. / behavior_policy(state_t)[action_t]    
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy
```

Dans cet extrait, nous implémentons l'algorithme MC avec échantillonnage par importance pondérée. Chaque épisode est exécuté, les retours sont pondérés et la Q-fonction est mise à jour en fonction des retours pondérés.

---

### **Partie 4 : 32. Contrôle Monte Carlo avec échantillonnage par importance pondérée – Implémentation Partie 2**

**Explication :**
Dans cette dernière partie, nous allons terminer l'implémentation et simuler plusieurs épisodes pour évaluer la performance de la politique optimale obtenue via le contrôle Monte Carlo off-policy avec échantillonnage pondéré.

**Images associées :**
1. Simulation d'épisodes avec affichage des probabilités de victoire et de défaite.

**Extrait du code :**
```python
def simulate_episode(env, policy):
    state = env.reset()
    is_done = False
    while not is_done:
        action = policy[state]
        state, reward, is_done, info = env.step(action)
        if is_done:
            return reward

n_episode = 100000
n_win_optimal = 0
n_lose_optimal = 0

for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_optimal += 1
    elif reward == -1:
        n_lose_optimal += 1

print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))
print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))
```

Dans cet extrait final, nous simulons plusieurs épisodes avec la politique optimale obtenue, et calculons les probabilités de gagner ou de perdre sous cette politique.





















----------------------
# Annexe:
----------------------

### **Code complet pour le contrôle Monte Carlo avec échantillonnage par importance pondérée :**

```python
import torch
import gym

# Création de l'environnement Blackjack avec Gym
env = gym.make('Blackjack-v0')

# Génération d'une politique aléatoire
def gen_random_policy(n_action):
    probs = torch.ones(n_action) / n_action
    def policy_function(state):
        return probs
    return policy_function

random_policy = gen_random_policy(env.action_space.n)

# Exécution d'un épisode sous la politique de comportement
def run_episode(env, behavior_policy):
    """
    Exécution d'un épisode donné une politique de comportement
    @param env: OpenAI Gym environment
    @param behavior_policy: politique de comportement
    @return: états, actions, et récompenses de l'épisode entier
    """
    state = env.reset()  # Réinitialise l'environnement
    rewards = []
    actions = []
    states = []
    is_done = False
    while not is_done:
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
    return states, actions, rewards

# Implémentation du contrôle MC avec échantillonnage par importance pondérée
from collections import defaultdict

def mc_control_off_policy_weighted(env, gamma, n_episode, behavior_policy):
    """
    Obtention de la politique optimale avec l'échantillonnage par importance pondérée
    @param env: OpenAI Gym environment
    @param gamma: facteur de réduction
    @param n_episode: nombre d'épisodes
    @param behavior_policy: politique de comportement
    @return: fonction Q optimale et la politique optimale
    """
    n_action = env.action_space.n
    N = defaultdict(float)  # Nombre d'occurrences de chaque paire état-action
    Q = defaultdict(lambda: torch.empty(n_action))  # Fonction Q initialisée vide
    
    for episode in range(n_episode):
        w = 1.  # Poids initial
        # Exécute un épisode complet
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0
        # Parcours à l'envers des états, actions et récompenses
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t  # Calcul des retours pondérés
            N[(state_t, action_t)] += w  # Mise à jour de la pondération
            Q[state_t][action_t] += (w/N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])  # Mise à jour de la fonction Q
            # Si l'action ne correspond pas à l'action optimale, on arrête
            if action_t != torch.argmax(Q[state_t]).item():
                break
            # Mise à jour du poids
            w *= 1. / behavior_policy(state_t)[action_t]    
    # Création de la politique optimale
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy

gamma = 1  # Facteur de réduction

n_episode = 500000  # Nombre d'épisodes

# Exécution de l'algorithme pour obtenir la politique optimale
optimal_Q, optimal_policy = mc_control_off_policy_weighted(env, gamma, n_episode, random_policy)

# Simulation d'épisodes pour tester la politique optimale obtenue
def simulate_episode(env, policy):
    state = env.reset()
    is_done = False
    while not is_done:
        action = policy[state]  # Choix de l'action selon la politique optimale
        state, reward, is_done, info = env.step(action)
        if is_done:
            return reward  # Retourne la récompense

# Simulation de plusieurs épisodes
n_episode = 100000
n_win_optimal = 0  # Nombre de victoires
n_lose_optimal = 0  # Nombre de défaites

for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_optimal += 1
    elif reward == -1:
        n_lose_optimal += 1

# Affichage des probabilités de victoire et de défaite
print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))
print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))
```

---

### **Explication étape par étape :**

1. **Création de l'environnement :**
   - Utilisation de `gym.make('Blackjack-v0')` pour créer un environnement de Blackjack.
   - Cet environnement permet de simuler des parties de Blackjack où l'algorithme doit apprendre à prendre les meilleures décisions.

2. **Politique aléatoire :**
   - La fonction `gen_random_policy` génère une politique qui sélectionne chaque action avec une probabilité égale.
   - Cela sert de **politique de comportement** pour explorer l'environnement.

3. **Exécution d'un épisode :**
   - La fonction `run_episode` simule un épisode complet sous la politique de comportement, en enregistrant les états, actions, et récompenses.
   - Cette information est utilisée pour ajuster la politique cible via l'échantillonnage pondéré.

4. **Contrôle Monte Carlo Off-Policy avec échantillonnage pondéré :**
   - La fonction `mc_control_off_policy_weighted` utilise les données de l'épisode pour mettre à jour la fonction Q via l'échantillonnage pondéré.
   - Pour chaque paire état-action rencontrée, le retour est mis à jour en fonction des poids calculés.
   - La politique optimale est obtenue en choisissant l'action avec la valeur Q la plus élevée pour chaque état.

5. **Simulation des épisodes sous la politique optimale :**
   - Après avoir obtenu la politique optimale, la fonction `simulate_episode` exécute plusieurs épisodes pour tester la performance de la politique.
   - Les probabilités de victoire et de défaite sont ensuite calculées.

---

### **Résultats :**
En exécutant le code, vous verrez des statistiques comme :
- **Probabilité de gagner** sous la politique optimale.
- **Probabilité de perdre** sous la politique optimale.
