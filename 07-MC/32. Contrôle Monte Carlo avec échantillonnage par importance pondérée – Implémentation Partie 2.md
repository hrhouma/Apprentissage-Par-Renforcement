### **Partie 3 : Explication du contrôle Monte Carlo avec échantillonnage par importance pondérée**

#### **Explication :**

Dans cette partie, nous abordons le **contrôle Monte Carlo off-policy** avec **échantillonnage par importance pondérée**. L'objectif de cet algorithme est d'améliorer une politique cible en utilisant des données générées par une politique de comportement différente. Cela permet de séparer le processus d'exploration (politique de comportement) de l'apprentissage (politique cible).

##### **Pourquoi l’échantillonnage par importance pondérée ?**

Lorsque nous utilisons une **politique de comportement** pour générer des données, les actions prises ne correspondent pas forcément à la **politique cible**. Pour ajuster la politique cible correctement, nous devons prendre en compte la différence entre ces deux politiques. C’est là qu’intervient **l’échantillonnage par importance pondérée**, qui ajuste les retours observés à chaque étape en fonction du ratio entre les probabilités d'action dans la politique cible et celles de la politique de comportement.

La mise à jour de la **Q-fonction** se fait ainsi en pondérant les retours selon ce ratio. Plus la politique de comportement et la politique cible sont similaires, plus le poids attribué aux retours sera élevé. À l’inverse, si les politiques sont très différentes, le poids sera plus faible.

##### **Étapes clés** :

1. **Collecte des données** : Pour chaque épisode, nous collectons les **états**, **actions** et **récompenses** générés par la politique de comportement.
   
2. **Calcul des retours pondérés** : À chaque étape, les **retours futurs** sont pondérés par le **ratio** des probabilités de la politique cible sur la politique de comportement.

3. **Mise à jour de la Q-fonction** : À chaque étape, la Q-fonction est mise à jour en tenant compte du retour pondéré. Plus le ratio est élevé (les politiques sont similaires), plus la Q-fonction est ajustée de manière significative.

4. **Arrêt de la mise à jour** : Si, à un moment donné, l'action prise par la politique de comportement est différente de l'action optimale de la politique cible (c’est-à-dire celle avec la plus haute valeur Q), on arrête la mise à jour pour cet épisode.

---

#### **Exemple d'algorithme :**

Voici l'algorithme complet du contrôle Monte Carlo avec échantillonnage par importance pondérée. Il prend en compte la différence entre la politique de comportement et la politique cible pour pondérer les retours et mettre à jour la Q-fonction.

```python
from collections import defaultdict

def mc_control_off_policy_weighted(env, gamma, n_episode, behavior_policy):
    """
    Implémentation du contrôle Monte Carlo avec échantillonnage par importance pondérée
    @param env: Environnement de simulation (Gym - Blackjack)
    @param gamma: Facteur de discount (importance des récompenses futures)
    @param n_episode: Nombre d'épisodes simulés
    @param behavior_policy: Politique de comportement utilisée pour explorer l'environnement
    @return: Q-fonction optimale et politique optimale
    """
    # Nombre d'actions possibles
    n_action = env.action_space.n
    
    # N stocke le nombre de visites pondérées pour chaque couple état-action
    N = defaultdict(float)
    
    # Q stocke la valeur de la Q-fonction pour chaque couple état-action
    Q = defaultdict(lambda: torch.empty(n_action))
    
    # Simulation sur plusieurs épisodes
    for episode in range(n_episode):
        # Initialisation du poids d'importance pour chaque épisode
        w = 1.0
        
        # Exécute un épisode sous la politique de comportement
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        
        # Calcul des retours à rebours (depuis la fin de l'épisode)
        return_t = 0
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            # Mise à jour du retour total en tenant compte du facteur de discount gamma
            return_t = gamma * return_t + reward_t
            
            # Mise à jour du nombre de visites pondérées pour le couple état-action
            N[(state_t, action_t)] += w
            
            # Mise à jour de la Q-fonction en utilisant le retour pondéré
            Q[state_t][action_t] += (w / N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])
            
            # Si l'action prise par la politique de comportement n'est pas la meilleure action selon la Q-fonction, on arrête la mise à jour
            if action_t != torch.argmax(Q[state_t]).item():
                break
            
            # Mise à jour du poids d'importance pour l'étape suivante
            w *= 1.0 / behavior_policy(state_t)[action_t]
    
    # Détermination de la politique optimale en prenant l'action avec la plus haute valeur Q dans chaque état
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    
    return Q, policy
```

#### **Explication détaillée du code :**

1. **Initialisation des structures** :
   - `N`: Stocke le **nombre de visites pondérées** pour chaque couple état-action.
   - `Q`: Stocke la **Q-fonction** pour chaque couple état-action.
   
2. **Boucle principale sur les épisodes** :
   - Pour chaque épisode, on initialise un poids d’importance **`w`** à 1.0. Ce poids sera ajusté à chaque étape en fonction du ratio des probabilités.
   
3. **Exécution de l'épisode** :
   - On exécute un épisode complet avec la **politique de comportement** en collectant les **états**, **actions**, et **récompenses** à chaque étape.
   
4. **Mise à jour à rebours** :
   - On parcourt les états, actions et récompenses dans l'ordre inverse (de la fin de l'épisode au début).
   - On met à jour les retours cumulés avec le facteur de discount **`gamma`**.
   - On met à jour la Q-fonction en pondérant le retour avec le poids d'importance calculé.
   
5. **Arrêt de la mise à jour** :
   - Si l'action prise par la politique de comportement est différente de l'action optimale selon la politique cible (celle avec la plus haute valeur dans la Q-fonction), on **arrête la mise à jour** pour cet épisode.
   
6. **Mise à jour du poids d'importance** :
   - Le poids est ajusté à chaque étape en fonction du ratio des probabilités de la politique de comportement et de la politique cible.

7. **Détermination de la politique optimale** :
   - Une fois la Q-fonction mise à jour pour tous les épisodes, on détermine la **politique optimale** en prenant l’action avec la plus haute valeur dans la Q-fonction pour chaque état.

---

### **Partie 4 : Contrôle Monte Carlo avec échantillonnage par importance pondérée – Implémentation Partie 2**

#### **Explication :**

Cette dernière partie complète l'implémentation du contrôle Monte Carlo avec échantillonnage par importance pondérée en simulant des épisodes pour évaluer la performance de la **politique optimale** obtenue. Une fois la Q-fonction apprise, nous utilisons la politique optimale pour exécuter un grand nombre d'épisodes et calculer les **probabilités de victoire** et de **défaite**.

##### **Simulation et évaluation de la politique optimale :**

Après avoir appris la politique optimale avec l'algorithme Monte Carlo, nous voulons vérifier comment elle se comporte dans des conditions réelles en simulant des épisodes dans l'environnement. Nous utilisons cette politique pour voir si elle maximise les récompenses, c’est-à-dire si elle augmente les chances de victoire dans le jeu de Blackjack, par exemple.

---

#### **Exemple d'algorithme de simulation :**

Voici la fonction qui simule des épisodes en utilisant la politique optimale obtenue et calcule les probabilités de **victoire** et de **défaite**.

```python
def simulate_episode(env, policy):
    """
    Simule un épisode avec la politique optimale
    @param env: Environnement de Gym (Blackjack)
    @param policy: Politique optimale calculée via l'algorithme MC off-policy
    @return: La récompense obtenue (1 pour une victoire, -1 pour une défaite)
    """
    # Réinitialisation de l'environnement
    state = env.reset()
    is_done = False
    
    # Boucle jusqu'à la fin de l'épisode
    while not is_done:
        # Choix de l'action optimale pour l'état actuel
        action = policy[state]
        state, reward, is_done, info = env.step(action)
        
        # Si l'épisode est terminé, retourner la récompense (victoire ou défaite)
        if is_done:
            return reward

# Nombre d'épisodes à simuler
n_episode = 100000

# Variables pour stocker le nombre de victoires et de défaites
n_win_optimal = 0
n_lose_optimal = 0

# Simuler n_episode épisodes
for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_optimal += 1  # Victoire
    elif reward == -1:
        n_lose_optimal += 1  # Défaite

# Affichage des probabilités de victoire et de défaite
print('Probabilité de victoire avec la politique optimale: {}'.format(n_win_optimal/n_episode))
print('Probabilité de défaite avec la politique optimale: {}'.format(n_lose_optimal/n_episode))
```




#### **Explication du code de simulation :**

1. **Initialisation de l'environnement et de la politique optimale** :
   - La fonction `simulate_episode` exécute un épisode sous la **politique optimale** obtenue précédemment avec le contrôle Monte Carlo. La politique optimale décide de l'action à prendre à chaque état, et l'épisode se termine lorsque le jeu est résolu (victoire ou défaite).

2. **Boucle principale sur les épisodes** :
   - Nous simulons un nombre important d'épisodes (ici, 100 000) pour observer les performances de la politique optimale.
   
3. **Comptabilisation des victoires et des défaites** :
   - À la fin de chaque épisode, la récompense est examinée : 
     - Si la récompense est `1`, cela signifie que la politique optimale a conduit à une **victoire**, et on incrémente `n_win_optimal`.
     - Si la récompense est `-1`, cela signifie que la politique optimale a conduit à une **défaite**, et on incrémente `n_lose_optimal`.
   
4. **Affichage des résultats** :
   - Après avoir simulé tous les épisodes, nous calculons les **probabilités de victoire et de défaite** en divisant le nombre de victoires et de défaites par le nombre total d'épisodes simulés.
   - Les résultats sont affichés pour que l'on puisse évaluer la performance de la politique optimale.

#### **Pourquoi simuler un grand nombre d'épisodes ?**

Simuler un grand nombre d'épisodes permet d'obtenir une estimation plus fiable de la performance de la politique optimale. Avec un petit nombre d'épisodes, il pourrait y avoir des fluctuations importantes dues à la variabilité des situations rencontrées dans le jeu de Blackjack. En simulant des centaines de milliers d'épisodes, on se rapproche d'une estimation statistiquement significative des performances.

---

### **Conclusion de la Partie 4 :**

Dans cette partie finale de l'implémentation, nous avons simulé la politique optimale obtenue via le contrôle Monte Carlo avec échantillonnage par importance pondérée, et nous avons évalué sa performance. En comparant les **probabilités de victoire** et de **défaite**, nous pouvons observer dans quelle mesure la politique optimale maximise la récompense dans notre environnement de Blackjack. Cette approche montre la puissance de l'algorithme Monte Carlo off-policy lorsqu'il est associé à une pondération correcte des retours à l'aide de l'échantillonnage par importance.











----------------------
# Annexe:
----------------------

### **Code complet pour le contrôle Monte Carlo avec échantillonnage par importance pondérée :**

```python
import torch
import gym

# Création de l'environnement Blackjack avec Gym
env = gym.make('Blackjack-v0')

# Génération d'une politique aléatoire
def gen_random_policy(n_action):
    probs = torch.ones(n_action) / n_action
    def policy_function(state):
        return probs
    return policy_function

random_policy = gen_random_policy(env.action_space.n)

# Exécution d'un épisode sous la politique de comportement
def run_episode(env, behavior_policy):
    """
    Exécution d'un épisode donné une politique de comportement
    @param env: OpenAI Gym environment
    @param behavior_policy: politique de comportement
    @return: états, actions, et récompenses de l'épisode entier
    """
    state = env.reset()  # Réinitialise l'environnement
    rewards = []
    actions = []
    states = []
    is_done = False
    while not is_done:
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
    return states, actions, rewards

# Implémentation du contrôle MC avec échantillonnage par importance pondérée
from collections import defaultdict

def mc_control_off_policy_weighted(env, gamma, n_episode, behavior_policy):
    """
    Obtention de la politique optimale avec l'échantillonnage par importance pondérée
    @param env: OpenAI Gym environment
    @param gamma: facteur de réduction
    @param n_episode: nombre d'épisodes
    @param behavior_policy: politique de comportement
    @return: fonction Q optimale et la politique optimale
    """
    n_action = env.action_space.n
    N = defaultdict(float)  # Nombre d'occurrences de chaque paire état-action
    Q = defaultdict(lambda: torch.empty(n_action))  # Fonction Q initialisée vide
    
    for episode in range(n_episode):
        w = 1.  # Poids initial
        # Exécute un épisode complet
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0
        # Parcours à l'envers des états, actions et récompenses
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t  # Calcul des retours pondérés
            N[(state_t, action_t)] += w  # Mise à jour de la pondération
            Q[state_t][action_t] += (w/N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])  # Mise à jour de la fonction Q
            # Si l'action ne correspond pas à l'action optimale, on arrête
            if action_t != torch.argmax(Q[state_t]).item():
                break
            # Mise à jour du poids
            w *= 1. / behavior_policy(state_t)[action_t]    
    # Création de la politique optimale
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy

gamma = 1  # Facteur de réduction

n_episode = 500000  # Nombre d'épisodes

# Exécution de l'algorithme pour obtenir la politique optimale
optimal_Q, optimal_policy = mc_control_off_policy_weighted(env, gamma, n_episode, random_policy)

# Simulation d'épisodes pour tester la politique optimale obtenue
def simulate_episode(env, policy):
    state = env.reset()
    is_done = False
    while not is_done:
        action = policy[state]  # Choix de l'action selon la politique optimale
        state, reward, is_done, info = env.step(action)
        if is_done:
            return reward  # Retourne la récompense

# Simulation de plusieurs épisodes
n_episode = 100000
n_win_optimal = 0  # Nombre de victoires
n_lose_optimal = 0  # Nombre de défaites

for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_optimal += 1
    elif reward == -1:
        n_lose_optimal += 1

# Affichage des probabilités de victoire et de défaite
print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))
print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))
```

---

### **Explication étape par étape :**

1. **Création de l'environnement :**
   - Utilisation de `gym.make('Blackjack-v0')` pour créer un environnement de Blackjack.
   - Cet environnement permet de simuler des parties de Blackjack où l'algorithme doit apprendre à prendre les meilleures décisions.

2. **Politique aléatoire :**
   - La fonction `gen_random_policy` génère une politique qui sélectionne chaque action avec une probabilité égale.
   - Cela sert de **politique de comportement** pour explorer l'environnement.

3. **Exécution d'un épisode :**
   - La fonction `run_episode` simule un épisode complet sous la politique de comportement, en enregistrant les états, actions, et récompenses.
   - Cette information est utilisée pour ajuster la politique cible via l'échantillonnage pondéré.

4. **Contrôle Monte Carlo Off-Policy avec échantillonnage pondéré :**
   - La fonction `mc_control_off_policy_weighted` utilise les données de l'épisode pour mettre à jour la fonction Q via l'échantillonnage pondéré.
   - Pour chaque paire état-action rencontrée, le retour est mis à jour en fonction des poids calculés.
   - La politique optimale est obtenue en choisissant l'action avec la valeur Q la plus élevée pour chaque état.

5. **Simulation des épisodes sous la politique optimale :**
   - Après avoir obtenu la politique optimale, la fonction `simulate_episode` exécute plusieurs épisodes pour tester la performance de la politique.
   - Les probabilités de victoire et de défaite sont ensuite calculées.

---

### **Résultats :**
En exécutant le code, vous verrez des statistiques comme :
- **Probabilité de gagner** sous la politique optimale.
- **Probabilité de perdre** sous la politique optimale.
