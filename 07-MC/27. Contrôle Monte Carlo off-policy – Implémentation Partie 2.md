# Introduction
Le **contrôle Monte Carlo off-policy** est une méthode utilisée en **apprentissage par renforcement**. Elle permet à un agent d'apprendre comment agir de manière optimale dans un environnement en jouant plusieurs épisodes du jeu, en observant ce qui se passe et en ajustant sa stratégie (ou politique) pour obtenir de meilleurs résultats à long terme. Dans ce contexte, le jeu choisi est le **Blackjack**.

---

### Objectif
L’objectif est de trouver la meilleure stratégie possible pour gagner le plus de parties de Blackjack, tout en essayant plusieurs actions (exploration) et en améliorant la stratégie progressivement (exploitation). Nous utilisons deux types de politiques (façons de prendre des décisions) :
1. **Politique de comportement** (Behavior Policy) : Une politique qui choisit des actions aléatoires pour explorer l’environnement.
2. **Politique cible** (Target Policy) : La politique que nous voulons améliorer, celle qui choisit les meilleures actions basées sur les expériences passées.

---

### Étape 1 : Créer l’environnement du Blackjack
Nous commençons par créer un environnement de jeu **Blackjack** à l’aide de la bibliothèque `gym`. C’est comme lancer une table de Blackjack où l’agent peut jouer.

```python
import gym
env = gym.make('Blackjack-v0')
```

Ici, nous avons créé un environnement de jeu où l’agent peut jouer à **Blackjack** et recevoir des cartes.

---

### Étape 2 : Politique de Comportement Aléatoire
Nous avons besoin d'une **politique de comportement** qui permet à l'agent de choisir des actions au hasard. Cela aide l'agent à explorer différentes actions, même si elles ne semblent pas être les meilleures à première vue.

Imaginez que vous êtes à un croisement. Parfois, au lieu de toujours prendre la route qui vous semble la plus courte, vous essayez de prendre un autre chemin pour voir si ça peut être mieux. Cette exploration aide à découvrir de meilleures routes.

```python
def gen_random_policy(n_action):
    probs = torch.ones(n_action) / n_action
    def policy_function(state):
        return probs
    return policy_function
```

Dans cette fonction, l’agent choisit chaque action avec une probabilité égale, c’est-à-dire au hasard.

---

### Étape 3 : Jouer un Épisode
Un **épisode** signifie une partie complète du jeu, depuis le moment où les cartes sont distribuées jusqu’à la fin de la partie. Dans cette étape, l’agent va jouer un épisode et enregistrer ce qui se passe (quelles cartes il reçoit, quelles actions il choisit, quelle récompense il obtient).

Imaginez que vous jouez à une partie de Blackjack et que vous notez chaque carte que vous recevez, chaque action que vous prenez (tirer une carte ou rester), et si vous avez gagné ou perdu.

```python
def run_episode(env, behavior_policy):
    state = env.reset()
    rewards = []
    actions = []
    states = []
    is_done = False
    while not is_done:
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
    return states, actions, rewards
```

- **state** : Les cartes que vous et le croupier avez.
- **actions** : Les décisions que vous prenez (tirer une carte ou rester).
- **rewards** : Les résultats (gagner ou perdre).

---

### Étape 4 : Importance Sampling (échantillonnage d'importance)
Cette partie est un peu technique, mais essayons de la simplifier. L'idée principale ici est de récompenser les actions qui conduisent à de bons résultats. Mais comme l'agent utilise parfois une politique aléatoire (behavior policy), il faut ajuster ses observations pour se concentrer davantage sur la politique cible (celle qui choisit les meilleures actions).

Imaginez que vous essayez plusieurs stratégies dans un jeu, mais vous voulez apprendre surtout des stratégies qui donnent les meilleurs résultats.

```python
def mc_control_off_policy(env, gamma, n_episode, behavior_policy):
    n_action = env.action_space.n
    G_sum = defaultdict(float)
    N = defaultdict(int)
    Q = defaultdict(lambda: torch.empty(n_action))
    for episode in range(n_episode):
        W = {}
        w = 1.
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0
        G = {}
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            G[(state_t, action_t)] = return_t
            W[(state_t, action_t)] = w
            if action_t != torch.argmax(Q[state_t]).item():
                break
            w *= 1. / behavior_policy(state_t)[action_t]
        for state_action, return_t in G.items():
            state, action = state_action
            if state[0] <= 21:
                G_sum[state_action] += return_t * W[state_action]
                N[state_action] += 1
                Q[state][action] = G_sum[state_action] / N[state_action]
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy
```

Ce que nous faisons ici :
- Nous jouons plusieurs parties (épisodes).
- Nous calculons la valeur des actions prises à chaque étape.
- Nous utilisons un ajustement appelé **importance sampling** pour mettre à jour les Q-valeurs.

---

### Étape 5 : Mise à jour Incrémentale des Q-valeurs
Au lieu de tout recalculer à chaque fois, nous mettons à jour progressivement les **Q-valeurs** pour chaque état-action, en fonction des nouvelles informations. Cela permet d'être plus efficace et d'économiser de la mémoire.

Imaginez que vous modifiez vos décisions au fur et à mesure que vous apprenez, sans tout recommencer à zéro.

```python
def mc_control_off_policy_incremental(env, gamma, n_episode, behavior_policy):
    n_action = env.action_space.n
    N = defaultdict(int)
    Q = defaultdict(lambda: torch.empty(n_action))
    for episode in range(n_episode):
        W = 1.
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0.
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            N[(state_t, action_t)] += 1
            Q[state_t][action_t] += (W / N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])
            if action_t != torch.argmax(Q[state_t]).item():
                break
            W *= 1./ behavior_policy(state_t)[action_t]
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy
```

---

### Étape 6 : Tester la Politique Optimale
Nous testons ensuite la politique apprise pour voir si elle permet de gagner plus souvent. Nous simulons 100 000 parties et nous calculons la probabilité de victoire et de défaite.

```python
n_episode = 100000
n_win_optimal = 0
n_lose_optimal = 0

for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_optimal += 1
    elif reward == -1:
        n_lose_optimal += 1

print('Probabilité de victoire avec la politique optimale: {}'.format(n_win_optimal / n_episode))
print('Probabilité de défaite avec la politique optimale: {}'.format(n_lose_optimal / n_episode))
```

Cette partie vérifie si la politique optimale conduit à plus de victoires que de défaites. Vous verrez les probabilités de gagner ou de perdre avec cette politique.

---

### Conclusion
Pour résumer :
- **L’agent** explore l’environnement en prenant des décisions au hasard au début (exploration).
- Il enregistre les résultats (gains/pertes) et ajuste sa stratégie pour mieux jouer à l’avenir (exploitation).
- **L'échantillonnage d'importance** aide l'agent à se concentrer sur les décisions qui apportent les meilleurs résultats.
- **La mise à jour incrémentale** rend l’algorithme plus efficace, car il améliore les décisions au fur et à mesure sans tout recalculer.

En utilisant ces concepts, l'agent peut progressivement améliorer sa stratégie pour maximiser ses chances de gagner au Blackjack.
