# Étape 2 : Contrôle Monte Carlo On-Policy pour le Blackjack

Dans cette partie 2, nous continuons notre projet en mettant en œuvre l'algorithme de contrôle Monte Carlo en mode on-policy. Voici un résumé des étapes importantes et des explications détaillées :

### Étape 1 : Spécification des paramètres
Nous devons spécifier le **taux d'actualisation** et le **nombre d'épisodes**. 
- `gamma = 1`: Ce paramètre est utilisé pour indiquer le facteur de réduction des récompenses futures.
- `n_episode = 500000`: Il s'agit du nombre d'épisodes que nous allons simuler pour apprendre la politique optimale.

```python
gamma = 1
n_episode = 500000
```

### Étape 2 : Détermination de la politique optimale et de la fonction de valeur Q
Nous utilisons la fonction `mc_control_on_policy` pour obtenir la **politique optimale** et la **fonction Q optimale**.

```python
optimal_Q, optimal_policy = mc_control_on_policy(env, gamma, n_episode)
print(optimal_policy)
```

Cette fonction retourne deux éléments :
- `optimal_Q`: La fonction de valeur Q optimale pour chaque état-action.
- `optimal_policy`: La politique optimale basée sur la fonction Q.

### Étape 3 : Affichage des valeurs optimales
Nous calculons et affichons les valeurs optimales pour chaque état en utilisant la fonction suivante :

```python
optimal_value = defaultdict(float)
for state, action_values in optimal_Q.items():
    optimal_value[state] = torch.max(action_values).item()
print(optimal_value)
```

Cela permet de voir quelle est la meilleure action pour chaque état selon la politique apprise.

### Étape 4 : Visualisation des résultats
Nous utilisons un graphique 3D pour visualiser les valeurs optimales obtenues par le modèle. Nous affichons deux types de graphiques : l'un sans As utilisable, et l'autre avec un As utilisable.

Voici comment nous traçons les graphiques :

```python
def plot_surface(X, Y, Z, title):
    fig = plt.figure(figsize=(20, 10))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)
    ax.set_xlabel('Player Sum')
    ax.set_ylabel('Dealer Showing')
    ax.set_zlabel('Value')
    ax.set_title(title)
    ax.view_init(ax.elev, -120)
    fig.colorbar(surf)
    plt.show()

def plot_blackjack_value(V):
    player_sum_range = range(12, 22)
    dealer_show_range = range(1, 11)
    X, Y = torch.meshgrid([torch.tensor(player_sum_range), torch.tensor(dealer_show_range)])
    values_to_plot = torch.zeros((len(player_sum_range), len(dealer_show_range), 2))
    for i, player in enumerate(player_sum_range):
        for j, dealer in enumerate(dealer_show_range):
            for k, ace in enumerate([False, True]):
                values_to_plot[i, j, k] = V[(player, dealer, ace)]
    plot_surface(X, Y, values_to_plot[:,:,0].numpy(), "Blackjack Value Function Without Usable Ace")
    plot_surface(X, Y, values_to_plot[:,:,1].numpy(), "Blackjack Value Function With Usable Ace")
```

- **Plot 1** : Sans As utilisable (sans flexibilité dans la valeur de l'As).
- **Plot 2** : Avec un As utilisable (l'As peut valoir 1 ou 11).

### Étape 5 : Explication du processus
Le processus de contrôle on-policy en Monte Carlo alterne entre **évaluation de la politique** et **amélioration de la politique**. 
1. **Évaluation de la politique** : On utilise la fonction de valeur d'action (fonction Q) pour estimer la valeur d'un état-action en exécutant la politique actuelle.
2. **Amélioration de la politique** : À chaque itération, nous mettons à jour la politique en prenant l'action qui maximise la valeur Q dans chaque état.

### Erreurs rencontrées et solutions
Pendant l'exécution, une erreur liée à la bibliothèque `matplotlib` a été rencontrée, car elle n'était pas importée. Nous avons corrigé cela en important correctement les bibliothèques nécessaires comme suit :

```python
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib
```

### Conclusion
À la fin, les graphiques générés montrent les valeurs optimales pour chaque combinaison d'état et d'action, avec et sans As utilisable. Cela nous donne une bonne intuition de la meilleure stratégie à adopter dans chaque situation du jeu.



Cette version est parfaitement adaptée pour un document destiné à des étudiants, sans mention de vidéos ou d'étapes à suivre ultérieurement.
