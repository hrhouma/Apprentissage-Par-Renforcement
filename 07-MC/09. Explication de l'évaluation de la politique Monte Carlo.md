# Étape 3 : Prédiction Monte Carlo

Dans cette étape, nous réalisons les tâches suivantes lors de la prédiction Monte Carlo :

- Nous simulons **n_episode épisodes**.
- Pour chaque épisode, nous calculons les retours pour la **première visite** de chaque état.
- Pour chaque état, nous obtenons la fonction de valeur en **faisant la moyenne** des retours obtenus lors de toutes les visites de cet état sur tous les épisodes simulés.

### Pourquoi utiliser la méthode Monte Carlo ?

Dans la prédiction Monte Carlo, il n'est **pas nécessaire de connaître le modèle complet** de l'environnement. En effet, dans la majorité des situations réelles, la matrice de transition (qui détermine la probabilité de passer d'un état à un autre) et la matrice de récompenses ne sont pas toujours connues à l'avance, et peuvent être très difficiles à obtenir. 

Imagine, par exemple, un jeu comme le **Go** ou les **échecs**. Le nombre d'états possibles et d'actions dans ces jeux est si immense qu'il est presque impossible de déterminer une matrice de transition ou une matrice de récompenses.

Ainsi, l'apprentissage par renforcement sans modèle (**model-free**) consiste à apprendre directement à partir de l'expérience, en interagissant avec l'environnement. 

---

## Mise en œuvre de la prédiction Monte Carlo de toutes les visites

### Explication

Nous avons déjà parlé de la **prédiction de première visite**, où l'on prend en compte uniquement la première fois qu'un état est visité dans un épisode. À présent, avec la méthode de **toutes les visites**, nous faisons la moyenne des retours pour **chaque occurrence** d'un état.

Nous avons défini une fonction `mc_prediction_every_visit` qui effectue la prédiction Monte Carlo en prenant en compte **toutes les visites** d'un état dans un épisode. Voici un exemple de code pour cette fonction :

### Code Python

```python
def mc_prediction_every_visit(env, policy, gamma, n_episode):
    n_state = policy.shape[0]
    V = torch.zeros(n_state)  # Fonction de valeur initialisée à 0 pour chaque état
    N = torch.zeros(n_state)  # Compteur de visites pour chaque état
    G = torch.zeros(n_state)  # Retour cumulé pour chaque état

    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, policy)  # Obtenir les états et récompenses d'un épisode
        return_t = 0

        # Calculer les retours en parcourant les états et récompenses en sens inverse
        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):
            return_t = gamma * return_t + reward_t  # Calculer le retour actualisé
            G[state_t] += return_t  # Ajouter le retour à G pour cet état
            N[state_t] += 1  # Incrémenter le compteur de visites pour cet état

    # Moyenne des retours pour chaque état
    for state in range(n_state):
        if N[state] > 0:
            V[state] = G[state] / N[state]  # Calculer la moyenne des retours pour cet état

    return V  # Retourner la fonction de valeur
```

### Affichage des résultats

Après avoir exécuté la prédiction Monte Carlo avec la méthode **toutes les visites**, nous obtenons la fonction de valeur pour chaque état. Voici comment nous affichons les valeurs obtenues :

```python
print('La fonction de valeur calculée par la prédiction Monte Carlo de toutes les visites :\n', value)
```

### Exemple de résultats

```python
tensor([0.6221, 0.4322, 0.3903, 0.3578, 0.6246, 0.0000, 0.3520, 0.0000,
        0.6428, 0.6759, 0.6323, 0.0000, 0.0000, 0.7624, 0.8801, 0.0000])
```

### Explication des résultats :

Chaque valeur dans le tenseur représente la **valeur d'un état spécifique** dans l'environnement **FrozenLake** en suivant la politique donnée. Par exemple, la valeur **0.6221** pour le premier état signifie que, si l'agent commence dans cet état et suit la politique, il peut s'attendre à recevoir une récompense moyenne de **0.6221** à long terme.

---

## Conclusion

L'évaluation de politique avec la méthode Monte Carlo permet d'estimer la fonction de valeur sans avoir besoin de connaître le modèle complet de l'environnement. La **prédiction de toutes les visites** fournit une estimation plus complète en prenant en compte chaque occurrence d'un état, contrairement à la **prédiction de première visite**.

Si vous continuez à simuler des épisodes supplémentaires, vous verrez que les valeurs de la fonction de valeur convergent avec le temps, ce qui est similaire à ce que nous avons vu lors de l'estimation de **π**.

-------------------------
# Annexe et résumé
-------------------------







## Contexte général : L'évaluation de politique avec Monte Carlo

### Rappel : Qu'est-ce qu'une **politique** ?
Une **politique** en apprentissage par renforcement est un plan d'action que l'agent suit. Cela signifie que pour chaque état dans lequel se trouve l'agent, la politique lui dit quelle action il doit entreprendre. On peut représenter une politique sous forme d'un tableau où chaque état est associé à une action.

### Qu'est-ce que la **fonction de valeur** ?
La **fonction de valeur** est la valeur attendue (la somme des récompenses futures) qu'un agent peut espérer obtenir à partir d'un état donné en suivant une certaine politique. En d'autres termes, elle mesure "à quel point" un état est bon en termes de récompenses futures si l'on suit la politique.

- **\( V(s) \)** représente la fonction de valeur pour un état \( s \).
- \( V(s) \) est la récompense moyenne attendue si l'agent commence dans l'état \( s \) et suit la politique donnée.

### Qu'est-ce que **Monte Carlo** ?
La méthode **Monte Carlo** est une méthode qui repose sur la répétition d'expériences aléatoires pour estimer une certaine valeur. En apprentissage par renforcement, nous utilisons Monte Carlo pour estimer la fonction de valeur en exécutant un grand nombre d'épisodes dans l'environnement et en observant les récompenses reçues.

**Pourquoi utiliser Monte Carlo ?**
- Elle ne nécessite pas de connaître le modèle de l'environnement (contrairement à des méthodes comme la **programmation dynamique**).
- Elle apprend directement de l'expérience, en interagissant avec l'environnement.

---

## Étape 1 : Mise en place de l'environnement FrozenLake

**FrozenLake** est un environnement dans lequel l'agent doit naviguer sur une surface gelée pour atteindre une destination finale. Cependant, certaines tuiles du lac sont dangereuses (trous) et feront tomber l'agent. Le but de l'agent est d'atteindre la destination sans tomber dans un trou, en suivant une politique.

### Code pour créer l'environnement FrozenLake

```python
import torch
import gym

# Créer l'environnement FrozenLake
env = gym.make('FrozenLake-v0')
```

- **gym.make('FrozenLake-v0')** : Cela crée une instance de l'environnement **FrozenLake** dans Gym. Ce sera l'environnement où notre agent va évoluer. À chaque étape, l'agent peut choisir de se déplacer (haut, bas, gauche, droite), et l'environnement lui fournit une récompense selon la case sur laquelle il atterrit (0 pour les cases vides, 1 pour l'atteinte de l'objectif).

---

## Étape 2 : Comprendre les **épisodes** et les **états**

Un **épisode** est une séquence d'actions effectuées par l'agent dans l'environnement, depuis l'état initial jusqu'à l'état final (où l'agent atteint la case objectif ou tombe dans un trou). Un **état** est une situation dans laquelle se trouve l'agent à un moment donné (par exemple, la position sur la grille de FrozenLake).

### Simuler un épisode

Nous devons écrire une fonction pour exécuter un épisode. Cette fonction va :
1. **Réinitialiser l'environnement**.
2. **Effectuer des actions selon la politique** de l'agent.
3. **Enregistrer les états visités et les récompenses** obtenues.

### Code pour simuler un épisode

```python
def run_episode(env, policy):
    state = env.reset()  # Réinitialiser l'environnement
    rewards = []  # Liste pour stocker les récompenses reçues
    states = [state]  # Liste pour stocker les états visités
    is_done = False  # L'épisode n'est pas terminé

    # Boucle tant que l'épisode n'est pas terminé
    while not is_done:
        action = policy[state].item()  # Choisir une action selon la politique
        state, reward, is_done, info = env.step(action)  # Effectuer l'action
        states.append(state)  # Ajouter l'état visité à la liste
        rewards.append(reward)  # Ajouter la récompense à la liste

    # Convertir les listes d'états et de récompenses en tenseurs PyTorch
    states = torch.tensor(states)
    rewards = torch.tensor(rewards)
    return states, rewards
```

### Explication du code :

1. **env.reset()** : Réinitialise l'environnement, c'est-à-dire que l'agent est replacé au point de départ.
2. **policy[state].item()** : La politique est utilisée pour déterminer quelle action prendre dans un état donné.
3. **env.step(action)** : L'environnement simule l'action choisie et renvoie le nouvel état, la récompense associée, et une variable indiquant si l'épisode est terminé (l'agent est tombé dans un trou ou a atteint la destination).
4. **states.append(state)** et **rewards.append(reward)** : Nous stockons les états visités et les récompenses reçues pour les utiliser plus tard.

---

## Étape 3 : Prédiction Monte Carlo de Première Visite

La **méthode de première visite** consiste à prendre en compte uniquement la **première** fois qu'un état est visité dans un épisode pour estimer la fonction de valeur. Cela signifie que, même si l'agent revisite un état plusieurs fois dans un épisode, nous ne considérons que la première occurrence pour calculer la fonction de valeur.

### Code pour la prédiction Monte Carlo de Première Visite

```python
def mc_prediction_first_visit(env, policy, gamma, n_episode):
    n_state = policy.shape[0]  # Nombre d'états dans la politique
    V = torch.zeros(n_state)  # Fonction de valeur initialisée à 0
    N = torch.zeros(n_state)  # Compteur de visites pour chaque état

    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, policy)  # Obtenir les états et récompenses
        return_t = 0  # Initialiser le retour à zéro
        first_visit = torch.zeros(n_state)  # Marquer les premières visites d'états
        G = torch.zeros(n_state)  # Stocker le retour pour chaque état

        # Parcourir les états et récompenses en sens inverse pour calculer les retours
        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):
            return_t = gamma * return_t + reward_t  # Calculer le retour actualisé
            G[state_t] = return_t  # Stocker le retour pour cet état
            first_visit[state_t] = 1  # Marquer la première visite

        # Mettre à jour la fonction de valeur en utilisant la première visite
        for state in range(n_state):
            if first_visit[state] > 0:  # Si c'est la première visite
                V[state] += G[state]  # Ajouter le retour à la fonction de valeur
                N[state] += 1  # Incrémenter le compteur de visites

    # Moyenne des retours pour chaque état
    for state in range(n_state):
        if N[state] > 0:
            V[state] = V[state] / N[state]  # Calculer la moyenne

    return V  # Retourner la fonction de valeur
```

### Explication des variables importantes :

- **\( V \)** : La fonction de valeur pour chaque état, initialisée à 0.
- **\( N \)** : Un tableau qui compte combien de fois chaque état a été visité.
- **first_visit** : Nous enregistrons uniquement la première fois qu'un état est visité pour chaque épisode.

---

## Étape 4 : Calculer la fonction de valeur

Nous allons maintenant exécuter la prédiction Monte Carlo de première visite pour estimer la fonction de valeur après avoir simulé plusieurs épisodes.

### Code :

```python
gamma = 1  # Facteur de discount (pas de dépréciation des récompenses futures)
n_episode = 10000  # Nombre d'épisodes à simuler

# Politique optimale prédéfinie pour FrozenLake
optimal_policy = torch.tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])

# Calculer la fonction de valeur avec la prédiction MC de première visite
value = mc_prediction_first_visit(env, optimal_policy, gamma, n_episode)

print('La fonction de valeur calculée par la prédiction Monte Carlo de première visite :\n', value)
```

---

## Étape 5 : Comparer avec la méthode de toutes les visites

- Dans la **méthode de toutes les visites**, nous prenons en compte **chaque** fois qu'un état est visité dans un épisode (contrairement à la première visite). 
- Cette méthode est plus complète mais requiert plus de calculs.


### Code détaillé pour la prédiction Monte Carlo de toutes les visites

```python
def mc_prediction_every_visit(env, policy, gamma, n_episode):
    n_state = policy.shape[0]  # Nombre d'états dans la politique
    V = torch.zeros(n_state)  # Fonction de valeur initialisée à 0 pour chaque état
    N = torch.zeros(n_state)  # Compteur du nombre de fois où chaque état est visité
    G = torch.zeros(n_state)  # Stocker les retours cumulés pour chaque état

    # Boucle pour simuler plusieurs épisodes
    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, policy)  # Obtenir les états et récompenses pour cet épisode
        return_t = 0  # Initialiser le retour à 0

        # Calculer les retours en parcourant les états et récompenses en sens inverse
        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):
            return_t = gamma * return_t + reward_t  # Calculer le retour actualisé
            G[state_t] += return_t  # Ajouter le retour à G pour cet état
            N[state_t] += 1  # Incrémenter le compteur de visites pour cet état

    # Calculer la moyenne des retours pour chaque état
    for state in range(n_state):
        if N[state] > 0:
            V[state] = G[state] / N[state]  # Moyenne des retours pour cet état

    return V  # Retourner la fonction de valeur estimée
```

### Explication :

1. **\( V \)** : La fonction de valeur pour chaque état, initialisée à 0. Elle est mise à jour avec la somme des retours.
2. **\( G \)** : La somme des retours pour chaque état. Chaque fois qu'un état est visité, son retour actualisé est ajouté à \( G[state] \).
3. **\( N \)** : Un compteur du nombre de fois où chaque état est visité. À chaque fois qu'un état est visité, \( N[state] \) est incrémenté.
4. **return_t = gamma * return_t + reward_t** : Cette ligne de code met à jour le retour actualisé en tenant compte du facteur de discount \( \gamma \), et en ajoutant la récompense immédiate \( reward_t \).

### Différence avec la méthode de première visite :
Dans la **méthode de toutes les visites**, chaque fois qu’un état est visité, son retour est pris en compte, alors que dans la **méthode de première visite**, seul le premier passage dans cet état est considéré pour cet épisode.

---

## Étape 6 : Comparaison des résultats

Nous allons comparer les résultats obtenus avec la méthode de première visite et celle de toutes les visites pour voir la différence dans l'estimation de la fonction de valeur.

### Code pour afficher les résultats

```python
# Calculer la fonction de valeur avec la prédiction Monte Carlo de toutes les visites
value_every_visit = mc_prediction_every_visit(env, optimal_policy, gamma, n_episode)

# Afficher les résultats des deux méthodes
print('Fonction de valeur avec la méthode de première visite :\n', value)
print('Fonction de valeur avec la méthode de toutes les visites :\n', value_every_visit)
```

### Exemple de résultats attendus

Voici un exemple de ce que pourrait donner les résultats après l’exécution des deux méthodes :

```python
Fonction de valeur avec la méthode de première visite :
tensor([0.6221, 0.4322, 0.3903, 0.3578, 0.6246, 0.0000, 0.3520, 0.0000,
        0.6428, 0.6759, 0.6323, 0.0000, 0.0000, 0.7624, 0.8801, 0.0000])

Fonction de valeur avec la méthode de toutes les visites :
tensor([0.6210, 0.4311, 0.3895, 0.3569, 0.6235, 0.0000, 0.3512, 0.0000,
        0.6415, 0.6748, 0.6312, 0.0000, 0.0000, 0.7612, 0.8789, 0.0000])
```

### Explication des résultats :

- Chaque valeur dans le tenseur représente la valeur estimée pour un état donné dans l'environnement **FrozenLake** en suivant la politique optimale. 
- Par exemple, la valeur **0.6221** pour le premier état dans la méthode de première visite signifie que, si l'agent commence dans cet état et suit la politique donnée, il peut s'attendre à recevoir une récompense moyenne de **0.6221** sur le long terme.

Les valeurs obtenues par les deux méthodes (première visite et toutes les visites) sont très proches, ce qui montre que les deux approches fournissent des estimations similaires de la fonction de valeur. Cependant, il peut y avoir des différences minimes en raison de la façon dont les retours sont moyennés.

---

## Étape 7 : Importance des itérations dans Monte Carlo

Les deux méthodes **(première visite et toutes les visites)** sont des méthodes basées sur **l’expérience**. Plus on simule d’épisodes, plus les estimations de la fonction de valeur sont précises. C'est pourquoi nous avons choisi de simuler **10 000 épisodes**. Si vous simulez moins d'épisodes, les valeurs peuvent être moins précises.

### Influence du nombre d'épisodes

- **Avec un faible nombre d'épisodes** (par exemple, 100 ou 500), les estimations de la fonction de valeur seront beaucoup plus bruyantes, car nous avons moins de données sur lesquelles baser nos calculs.
- **Avec un grand nombre d'épisodes** (par exemple, 10 000 ou plus), les estimations convergent plus près de la vraie fonction de valeur.

### Graphique d'évolution des valeurs :

Si nous suivons l'évolution de \( V(s) \) au fil des épisodes, nous verrions que les valeurs oscillent au début, puis convergent lentement vers une estimation stable.

---

## Conclusion

- **Méthode de première visite** : Plus rapide, car elle ne prend en compte qu'une seule occurrence de chaque état dans un épisode, mais peut être moins précise sur un faible nombre d'épisodes.
- **Méthode de toutes les visites** : Prend en compte toutes les occurrences de chaque état, ce qui la rend plus précise, mais plus coûteuse en termes de calculs.

Les méthodes Monte Carlo sont puissantes car elles permettent de calculer la fonction de valeur sans connaître le modèle de l'environnement (les matrices de transition et de récompenses). Cependant, elles nécessitent un grand nombre d'itérations pour obtenir des estimations fiables.

---

## Pour aller plus loin

Vous pouvez expérimenter avec :
1. Changer le facteur de discount **$$\gamma$$** pour voir comment cela affecte la convergence.
2. **Réduire ou augmenter le nombre d’épisodes** pour observer les effets sur la précision des estimations.
3. **Modifier la politique** utilisée par l'agent pour voir comment cela affecte les valeurs estimées.


