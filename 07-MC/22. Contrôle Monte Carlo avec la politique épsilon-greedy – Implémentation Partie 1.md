### Cours : Contrôle Monte Carlo On-Policy avec Epsilon-Greedy pour l'environnement Blackjack

# Partie 1 : Introduction et Implémentation de l'Epsilon-Greedy

Dans cette partie, nous allons introduire et mettre en œuvre l'algorithme Monte Carlo (MC) on-policy pour résoudre le jeu du Blackjack dans un environnement Gym. Nous utiliserons une stratégie **epsilon-greedy**, qui alterne entre l'exploration et l'exploitation. Cela nous permet d'améliorer la politique d'action de manière progressive tout en garantissant que nous explorons différentes options.

##### Objectifs

1. Comprendre l'algorithme de contrôle MC on-policy avec epsilon-greedy.
2. Implémenter l'algorithme pour apprendre une politique optimale dans un environnement Gym (Blackjack).
3. Simuler et évaluer la performance de la politique optimale apprise.

##### Concepts clés

- **Exploration** : Essayer de nouvelles actions pour découvrir potentiellement de meilleures stratégies.
- **Exploitation** : Prendre des actions basées sur la meilleure connaissance actuelle pour maximiser les gains immédiats.
- **Epsilon-Greedy** : Une méthode pour équilibrer l'exploration et l'exploitation. Avec une probabilité epsilon, une action est prise aléatoirement (exploration). Sinon, l'action avec la meilleure valeur connue est choisie (exploitation).

##### Le code complet d'implémentation

Voici le code pour le contrôle MC on-policy avec epsilon-greedy appliqué au Blackjack.

```python
import torch
import gym
from collections import defaultdict

# Création de l'environnement Blackjack
env = gym.make('Blackjack-v0')

def run_episode(env, Q, epsilon, n_action):
    """
    Exécute un épisode en utilisant une politique epsilon-greedy.
    @param env: environnement OpenAI Gym
    @param Q: fonction Q
    @param epsilon: taux d'exploration/exploitation
    @param n_action: espace d'actions
    @return: états, actions et récompenses pour l'épisode complet
    """
    state = env.reset()  # Réinitialiser l'environnement
    rewards, actions, states = [], [], []
    is_done = False
    while not is_done:
        # Politique epsilon-greedy
        probs = torch.ones(n_action) * epsilon / n_action
        best_action = torch.argmax(Q[state]).item()
        probs[best_action] += 1.0 - epsilon
        action = torch.multinomial(probs, 1).item()  # Choisir une action
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
    return states, actions, rewards

def mc_control_epsilon_greedy(env, gamma, n_episode, epsilon):
    """
    Applique le contrôle MC avec une politique epsilon-greedy.
    @param env: environnement OpenAI Gym
    @param gamma: facteur de réduction
    @param n_episode: nombre d'épisodes
    @param epsilon: taux d'exploration/exploitation
    @return: fonction Q optimale et politique optimale
    """
    n_action = env.action_space.n
    G_sum, N, Q = defaultdict(float), defaultdict(int), defaultdict(lambda: torch.empty(n_action))
    
    for episode in range(n_episode):
        states_t, actions_t, rewards_t = run_episode(env, Q, epsilon, n_action)
        return_t = 0
        G = {}
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            G[(state_t, action_t)] = return_t
        for state_action, return_t in G.items():
            state, action = state_action
            if state[0] <= 21:
                G_sum[state_action] += return_t
                N[state_action] += 1
                Q[state][action] = G_sum[state_action] / N[state_action]
    
    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}
    return Q, policy

# Paramètres pour l'apprentissage
gamma = 1
n_episode = 500000
epsilon = 0.1

# Obtention de la fonction Q et de la politique optimales
optimal_Q, optimal_policy = mc_control_epsilon_greedy(env, gamma, n_episode, epsilon)
```

##### Explications du code

- **run_episode** : Cette fonction exécute un épisode complet du jeu en suivant la politique epsilon-greedy.
- **mc_control_epsilon_greedy** : Applique l'algorithme de contrôle Monte Carlo pour apprendre la meilleure politique (fonction Q).
- **Epsilon-Greedy** : Avec une probabilité epsilon, une action aléatoire est prise pour favoriser l'exploration, et avec une probabilité \(1 - \epsilon\), la meilleure action connue est sélectionnée.

---

# Partie 2 : Simulation et Évaluation de la Politique Apprise

Dans cette partie, nous allons simuler des épisodes pour évaluer la performance de la politique apprise. Nous allons calculer les probabilités de victoire et de défaite après avoir exécuté 100 000 épisodes sous la politique optimale apprise.

##### Simulation

Voici la fonction pour simuler un épisode et observer la performance de la politique apprise.

```python
def simulate_episode(env, policy):
    state = env.reset()
    is_done = False
    while not is_done:
        action = policy[state]
        state, reward, is_done, info = env.step(action)
        if is_done:
            return reward
```

##### Évaluation de la politique optimale

```python
n_episode = 100000
n_win_optimal = 0
n_lose_optimal = 0

for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_optimal += 1
    elif reward == -1:
        n_lose_optimal += 1

# Affichage des résultats
print('Probabilité de victoire avec la politique optimale : {}'.format(n_win_optimal/n_episode))
print('Probabilité de défaite avec la politique optimale : {}'.format(n_lose_optimal/n_episode))
```

##### Explication

- **simulate_episode** : Cette fonction utilise la politique optimale apprise pour simuler un épisode du jeu et renvoyer la récompense finale.
- **n_win_optimal** et **n_lose_optimal** : Ces variables comptent respectivement le nombre de victoires et de défaites après avoir exécuté 100 000 épisodes sous la politique optimale.

##### Résultats attendus

L'algorithme imprimera les probabilités de victoire et de défaite en utilisant la politique optimale. Ces résultats nous permettront de savoir si la politique epsilon-greedy a effectivement amélioré la performance dans l'environnement de jeu.

### Conclusion

- En utilisant une politique **epsilon-greedy**, nous avons appris une politique optimale pour résoudre le jeu du Blackjack. 
- Cet algorithme nous permet d'explorer suffisamment l'espace des actions tout en exploitant les meilleures actions connues à ce jour, garantissant ainsi une amélioration progressive de la stratégie de jeu.
