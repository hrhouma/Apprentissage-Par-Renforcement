### Introduction au Contrôle Monte Carlo Hors-Politique

Dans cette section, nous présentons la méthode de **contrôle Monte Carlo (MC) hors-politique**. Cette approche repose sur le principe d'apprentissage hors-politique, où nous optimisons une **politique cible** (notée π) en utilisant des données générées par une autre politique, appelée **politique comportementale** (notée b). La politique cible se concentre sur **l'exploitation**, tandis que la politique comportementale est utilisée pour **l'exploration**.

### Processus Étape par Étape

1. **Définition des Politiques** :
    - **Politique Cible (π)** : Optimise les choix d'actions en sélectionnant la meilleure action possible basée sur les valeurs Q.
    - **Politique Comportementale (b)** : Utilisée pour générer des données par exploration. Cette politique permet d'explorer toutes les actions en choisissant aléatoirement.

2. **Étapes Communes Hors-Politique** :
    Nous nous concentrons sur les étapes communes aux deux politiques pour l'analyse, en commençant par l'étape la plus récente, où l'action prise sous la politique comportementale peut différer de celle prise sous la politique cible.

3. **Échantillonnage par Importance** :
    Pour apprendre de la politique comportementale et mettre à jour la politique cible, nous utilisons **l'échantillonnage par importance**. Cette technique permet d'estimer la valeur attendue sous une distribution différente.
    
    L'importance pondérée pour une paire état-action est calculée comme suit :

    $$w_t = \prod_{k=t}^{T} \frac{\pi(a_k | s_k)}{b(a_k | s_k)}$$

    Où :
    - π(aₖ | sₖ) est la probabilité de prendre l'action aₖ dans l'état sₖ sous la **politique cible**.
    - b(aₖ | sₖ) est la probabilité de prendre l'action aₖ dans l'état sₖ sous la **politique comportementale**.
    - wₜ est le produit des ratios des probabilités pour chaque pas de temps.

4. **Implémentation du Contrôle MC Hors-Politique** :
    Nous allons maintenant implémenter la méthode Monte Carlo hors-politique étape par étape.

---

### Explication du Code pour le Contrôle MC Hors-Politique

#### Étape 1 : Importation des Bibliothèques et Définition de l'Environnement Blackjack

```python
import torch
import gym

env = gym.make('Blackjack-v0')
```

#### Étape 2 : Définition de la Politique Comportementale

```python
def gen_random_policy(n_action):
    probs = torch.ones(n_action) / n_action
    def policy_function(state):
        return probs
    return policy_function
```

#### Étape 3 : Exécution d'un Épisode sous la Politique Comportementale

```python
def run_episode(env, behavior_policy):
    state = env.reset()
    rewards, actions, states = [], [], []
    is_done = False
    while not is_done:
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
    return states, actions, rewards
```

#### Étape 4 : Contrôle MC Hors-Politique avec Échantillonnage par Importance

```python
def mc_control_off_policy(env, gamma, n_episode, behavior_policy):
    n_action = env.action_space.n
    G_sum = defaultdict(float)
    N = defaultdict(int)
    Q = defaultdict(lambda: torch.empty(n_action))
    for episode in range(n_episode):
        W = {}
        w = 1.
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0
        G = {}
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            G[(state_t, action_t)] = return_t
            W[(state_t, action_t)] = w
            if action_t != torch.argmax(Q[state_t]).item():
                break
            w *= 1. / behavior_policy(state_t)[action_t]
        for state_action, return_t in G.items():
            state, action = state_action
            if state[0] <= 21:
                G_sum[state_action] += return_t * W[state_action]
                N[state_action] += 1
                Q[state][action] = G_sum[state_action] / N[state_action]
    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}
    return Q, policy
```

#### Étape 5 : Méthode de Mise à Jour Incrémentale

```python
def mc_control_off_policy_incremental(env, gamma, n_episode, behavior_policy):
    n_action = env.action_space.n
    N = defaultdict(int)
    Q = defaultdict(lambda: torch.empty(n_action))
    for episode in range(n_episode):
        W = 1.
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0.
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            N[(state_t, action_t)] += 1
            Q[state_t][action_t] += (W / N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])
            if action_t != torch.argmax(Q[state_t]).item():
                break
            W *= 1./ behavior_policy(state_t)[action_t]
    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}
    return Q, policy
```

#### Étape 6 : Exécution du Code

```python
gamma = 1
n_episode = 500000
random_policy = gen_random_policy(env.action_space.n)
optimal_Q, optimal_policy = mc_control_off_policy_incremental(env, gamma, n_episode, random_policy)
```

### Conclusion

- Le contrôle Monte Carlo hors-politique nous permet d'optimiser une politique cible tout en utilisant une politique comportementale différente pour l'exploration. En utilisant **l'échantillonnage par importance**, nous assurons que nous pouvons apprendre des données de la politique comportementale et mettre à jour la politique cible en conséquence. La méthode de **mise à jour incrémentale** améliore davantage l'efficacité de l'algorithme.
- Cette introduction au contrôle Monte Carlo hors-politique et son implémentation en Python avec PyTorch offrent une base solide pour comprendre et appliquer cette technique avancée d'apprentissage par renforcement.
