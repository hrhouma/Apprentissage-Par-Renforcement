### Introduction à la prédiction Monte Carlo pour le Blackjack

Dans cette section, nous allons utiliser les méthodes de prédiction Monte Carlo pour jouer au **Blackjack** et évaluer une politique simple. Cela vous permettra de mieux comprendre comment Monte Carlo peut être utilisé pour estimer des valeurs et comment il s'applique dans le cadre d'un jeu de cartes.

---

### Qu'est-ce que le Blackjack ?

Le **Blackjack** est un jeu de cartes populaire où le but est d'atteindre une somme de cartes aussi proche que possible de **21**, sans dépasser cette valeur.

- Les cartes **J**, **Q**, et **K** ont une valeur de **10** points.
- Les cartes de **2** à **10** ont leur propre valeur (par exemple, un **5** vaut **5 points**).
- L'**As** peut valoir soit **1** soit **11 points**, selon ce qui est le plus favorable pour le joueur. Un **As utilisable** est une carte où l'As peut encore être compté comme **11**.

Dans ce jeu, le joueur joue contre un **croupier**. Voici le déroulement classique :
- Au début, chaque joueur reçoit deux cartes, mais une seule des cartes du croupier est visible pour le joueur.
- Le joueur peut choisir soit de demander une nouvelle carte (appelée **hit**), soit de ne plus demander de cartes (**stick**).
- Le croupier continue à tirer des cartes tant que la somme de ses cartes est inférieure à **17**.
- Si la somme des cartes du joueur dépasse **21**, il perd (c'est appelé **bust**).
- Si le joueur a une somme de cartes plus élevée que celle du croupier sans dépasser **21**, il gagne.

### Exemple de politique simple pour le Blackjack

Nous allons utiliser une politique simple dans laquelle le joueur continue de demander des cartes (**hit**) tant que la somme de ses cartes est inférieure à **18**. Sinon, il arrête (**stick**).

---

## Simuler l'environnement Blackjack avec Gym

Le jeu de Blackjack est disponible dans le module **Gym** de OpenAI, ce qui nous permet de simuler des épisodes du jeu de manière très facile.

### Étape 1 : Importer les bibliothèques nécessaires

Nous commençons par importer **PyTorch** et **Gym** pour créer une instance de l'environnement de Blackjack.

```python
import torch
import gym

# Créer une instance de l'environnement Blackjack
env = gym.make('Blackjack-v0')
```

### Étape 2 : Réinitialiser l'environnement et prendre des actions

Nous devons réinitialiser l'environnement à chaque épisode et observer comment le jeu se déroule.

```python
# Réinitialiser l'environnement pour commencer un nouvel épisode
state = env.reset()

# Prenons des actions aléatoires pour voir comment l'environnement fonctionne
action = env.action_space.sample()  # Action aléatoire : hit ou stick
state, reward, done, info = env.step(action)  # Exécuter l'action dans l'environnement
```

### Explication :
1. **env.reset()** : Cela initialise un nouvel épisode. L'état de départ est renvoyé.
2. **env.action_space.sample()** : Cette ligne choisit une action aléatoire (hit ou stick).
3. **env.step(action)** : Cela exécute l'action et renvoie l'état suivant, la récompense, un booléen indiquant si l'épisode est terminé, et des informations supplémentaires.

---

## Définir une politique et simuler un épisode

Nous allons maintenant écrire une fonction pour simuler un épisode de Blackjack selon une politique simple.

### Code pour simuler un épisode de Blackjack

```python
def run_blackjack_episode(env, policy):
    state = env.reset()  # Réinitialiser l'environnement
    states = [state]  # Liste pour stocker les états visités
    rewards = []  # Liste pour stocker les récompenses reçues
    done = False  # Indique si l'épisode est terminé

    while not done:
        action = policy(state)  # Prendre une action selon la politique
        state, reward, done, info = env.step(action)  # Effectuer l'action
        states.append(state)  # Ajouter l'état visité à la liste
        rewards.append(reward)  # Ajouter la récompense à la liste

    return states, rewards
```

### Politique simple pour le Blackjack

Voici une politique simple : demander une carte si la somme des cartes est inférieure à **18**, sinon arrêter.

```python
def simple_policy(state):
    player_sum, dealer_card, usable_ace = state
    return 0 if player_sum >= 18 else 1  # 0 pour stick, 1 pour hit
```

### Explication de la politique :
- **player_sum** : La somme actuelle des cartes du joueur.
- **dealer_card** : La carte visible du croupier.
- **usable_ace** : Indique si le joueur a un As utilisable (peut être compté comme 11).

---

## Effectuer une prédiction Monte Carlo avec la méthode de première visite

Nous allons maintenant estimer la fonction de valeur d’une politique simple en utilisant la **prédiction Monte Carlo de première visite**.

### Code pour la prédiction Monte Carlo de première visite

```python
def mc_blackjack_prediction(env, policy, gamma, n_episode):
    V = {}  # Dictionnaire pour stocker la fonction de valeur
    returns = {}  # Dictionnaire pour stocker les retours

    for episode in range(n_episode):
        # Simuler un épisode de Blackjack
        states, rewards = run_blackjack_episode(env, policy)
        G = 0  # Initialiser le retour
        visited_states = set()  # Garder trace des états déjà visités

        for t in reversed(range(len(states))):
            G = gamma * G + rewards[t]  # Calculer le retour
            state = states[t]
            if state not in visited_states:  # Première visite de cet état
                visited_states.add(state)
                if state not in returns:
                    returns[state] = []
                returns[state].append(G)  # Ajouter le retour à la liste

                # Mettre à jour la fonction de valeur
                V[state] = torch.mean(torch.tensor(returns[state]))

    return V  # Retourner la fonction de valeur
```

### Explication :
1. **V** : Un dictionnaire qui stocke la fonction de valeur pour chaque état.
2. **returns** : Un dictionnaire qui garde une trace des retours pour chaque état.
3. **G = gamma * G + rewards[t]** : C'est la formule pour calculer les retours cumulés.

---

## Effectuer des prédictions sur 500 000 épisodes

Enfin, nous simulons plusieurs épisodes pour estimer la fonction de valeur.

```python
gamma = 1  # Facteur de discount
n_episode = 500000  # Nombre d'épisodes à simuler

# Effectuer la prédiction Monte Carlo
value_function = mc_blackjack_prediction(env, simple_policy, gamma, n_episode)

print("Fonction de valeur estimée pour la politique simple :\n", value_function)
```

---

## Résultats

Après avoir simulé **500 000 épisodes**, vous obtiendrez une estimation de la fonction de valeur pour chaque état rencontré. Vous pouvez expérimenter avec différents nombres d’épisodes pour observer l’effet sur la précision des estimations.

