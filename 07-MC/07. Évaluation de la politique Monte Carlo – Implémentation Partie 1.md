### Explication détaillée de l'évaluation de politique avec la méthode Monte Carlo

Nous allons explorer l'utilisation de la méthode Monte Carlo pour évaluer une politique dans l'environnement **FrozenLake** en utilisant deux techniques principales :
1. **Prédiction Monte Carlo de première visite (First-Visit MC)**
2. **Prédiction Monte Carlo de toutes les visites (Every-Visit MC)**

#### Objectif
Le but est d'estimer la fonction de valeur **V(s)** pour une politique donnée, représentant la valeur attendue d'un état sous cette politique.

### Code Python – Explication et Démonstration

```python
import torch
import gym

# Créer l'environnement FrozenLake
env = gym.make('FrozenLake-v0')

# Fonction pour exécuter un épisode avec une politique donnée
def run_episode(env, policy):
    state = env.reset()
    rewards = []
    states = [state]
    is_done = False
    while not is_done:
        action = policy[state].item()
        state, reward, is_done, info = env.step(action)
        states.append(state)
        rewards.append(reward)
        if is_done:
            break
    states = torch.tensor(states)
    rewards = torch.tensor(rewards)
    return states, rewards
```

### 1. Première Visite Monte Carlo (First-Visit MC)

Cette méthode calcule la moyenne des retours pour **la première occurrence** d'un état dans un épisode.

```python
def mc_prediction_first_visit(env, policy, gamma, n_episode):
    n_state = policy.shape[0]
    V = torch.zeros(n_state)
    N = torch.zeros(n_state)

    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, policy)
        return_t = 0
        first_visit = torch.zeros(n_state)
        G = torch.zeros(n_state)

        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):
            return_t = gamma * return_t + reward_t
            G[state_t] = return_t
            first_visit[state_t] = 1

        for state in range(n_state):
            if first_visit[state] > 0:
                V[state] += G[state]
                N[state] += 1

    for state in range(n_state):
        if N[state] > 0:
            V[state] = V[state] / N[state]
    return V
```

### 2. Toutes les Visites Monte Carlo (Every-Visit MC)

Ici, nous calculons la moyenne des retours pour **chaque occurrence** d'un état dans un épisode.

```python
def mc_prediction_every_visit(env, policy, gamma, n_episode):
    n_state = policy.shape[0]
    V = torch.zeros(n_state)
    N = torch.zeros(n_state)
    G = torch.zeros(n_state)

    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, policy)
        return_t = 0

        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):
            return_t = gamma * return_t + reward_t
            G[state_t] += return_t
            N[state_t] += 1

    for state in range(n_state):
        if N[state] > 0:
            V[state] = G[state] / N[state]
    return V
```

### Utilisation

```python
# Paramètres
gamma = 1
n_episode = 10000

# Politique optimale donnée pour FrozenLake
optimal_policy = torch.tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])

# Calculer la fonction de valeur avec les deux méthodes
value_first_visit = mc_prediction_first_visit(env, optimal_policy, gamma, n_episode)
value_every_visit = mc_prediction_every_visit(env, optimal_policy, gamma, n_episode)

print('Fonction de valeur (First-Visit MC):\n', value_first_visit)
print('Fonction de valeur (Every-Visit MC):\n', value_every_visit)
```

### Conclusion

- **First-Visit MC** : Plus rapide, prend en compte uniquement la première occurrence d'un état.
- **Every-Visit MC** : Plus complète mais plus coûteuse en calculs, prend en compte chaque occurrence d'un état.

L'évaluation de politique avec Monte Carlo permet de se passer d'un modèle de l'environnement, ce qui en fait une méthode flexible pour les environnements complexes comme FrozenLake.
