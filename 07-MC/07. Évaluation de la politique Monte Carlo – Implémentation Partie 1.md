### Explication détaillée de l'évaluation de politique avec la méthode Monte Carlo

Nous allons maintenant voir comment utiliser la méthode Monte Carlo pour évaluer une politique dans l'environnement **FrozenLake** en utilisant deux techniques principales :
1. **Prédiction Monte Carlo de première visite (First-Visit MC)**.
2. **Prédiction Monte Carlo de toutes les visites (Every-Visit MC)**.

#### Objectif
Le but de l'évaluation est d'estimer la fonction de valeur **\(V(s)\)** pour une politique donnée, c'est-à-dire la valeur attendue d'un état sous une certaine politique. Cette fonction de valeur est utile pour analyser et améliorer la politique utilisée par l'agent.

---

### Code Python – Explication et Démonstration

```python
import torch
import gym

# Créer l'environnement FrozenLake
env = gym.make('FrozenLake-v0')

# Fonction pour exécuter un épisode avec une politique donnée
def run_episode(env, policy):
    state = env.reset()
    rewards = []
    states = [state]
    is_done = False
    while not is_done:
        action = policy[state].item()
        state, reward, is_done, info = env.step(action)
        states.append(state)
        rewards.append(reward)
        if is_done:
            break
    states = torch.tensor(states)
    rewards = torch.tensor(rewards)
    return states, rewards
```

### 1. **Première Visite Monte Carlo (First-Visit MC)**

Cette méthode calcule la moyenne des retours pour **la première occurrence** d'un état dans un épisode. Cela simplifie le calcul car nous n'avons besoin de prendre en compte qu'une seule visite par épisode.

```python
def mc_prediction_first_visit(env, policy, gamma, n_episode):
    n_state = policy.shape[0]
    V = torch.zeros(n_state)  # Valeur initiale pour chaque état
    N = torch.zeros(n_state)  # Compteur de visites pour chaque état

    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, policy)
        return_t = 0
        first_visit = torch.zeros(n_state)
        G = torch.zeros(n_state)

        # Parcourir les états et les récompenses en sens inverse
        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):
            return_t = gamma * return_t + reward_t
            G[state_t] = return_t
            first_visit[state_t] = 1

        # Mettre à jour la fonction de valeur pour chaque première visite
        for state in range(n_state):
            if first_visit[state] > 0:
                V[state] += G[state]
                N[state] += 1

    # Moyenne des retours pour chaque état
    for state in range(n_state):
        if N[state] > 0:
            V[state] = V[state] / N[state]
    return V

# Paramètres
gamma = 1
n_episode = 10000

# Politique optimale donnée pour FrozenLake
optimal_policy = torch.tensor([0., 3., 3., 3., 0., 3., 2., 3., 3., 1., 0., 3., 3., 2., 1., 3.])

# Calculer la fonction de valeur avec la prédiction MC de première visite
value = mc_prediction_first_visit(env, optimal_policy, gamma, n_episode)

print('La fonction de valeur calculée par la prédiction Monte Carlo de première visite :\n', value)
```

### 2. **Toutes les Visites Monte Carlo (Every-Visit MC)**

Ici, nous calculons la moyenne des retours pour **chaque occurrence** d'un état dans un épisode. Cette méthode est plus complète mais nécessite plus de calculs.

```python
def mc_prediction_every_visit(env, policy, gamma, n_episode):
    n_state = policy.shape[0]
    V = torch.zeros(n_state)
    N = torch.zeros(n_state)
    G = torch.zeros(n_state)

    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, policy)
        return_t = 0

        for state_t, reward_t in zip(reversed(states_t)[1:], reversed(rewards_t)):
            return_t = gamma * return_t + reward_t
            G[state_t] += return_t
            N[state_t] += 1

    # Moyenne des retours pour chaque état
    for state in range(n_state):
        if N[state] > 0:
            V[state] = G[state] / N[state]
    return V

# Calculer la fonction de valeur avec la prédiction MC de toutes les visites
value = mc_prediction_every_visit(env, optimal_policy, gamma, n_episode)

print('La fonction de valeur calculée par la prédiction Monte Carlo de toutes les visites :\n', value)
```

---

### Conclusion

- **Prédiction de première visite** : Plus rapide à calculer car elle ne prend en compte que la première occurrence d'un état.
- **Prédiction de toutes les visites** : Plus complète, mais plus coûteuse en termes de calculs car elle prend en compte chaque occurrence d'un état dans l'épisode.

L'évaluation de politique avec Monte Carlo permet de se passer d'un modèle de l'environnement (matrice de transitions et de récompenses), ce qui en fait une méthode flexible et puissante pour les environnements complexes comme FrozenLake.

