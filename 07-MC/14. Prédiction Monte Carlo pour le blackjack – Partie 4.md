# Explication détaillée du code pour la prédiction Monte Carlo et visualisation dans le projet Blackjack

#### 1. **Initialisation de l'environnement Blackjack**

```python
import torch
import gym
env = gym.make('Blackjack-v0')
```
Ici, nous utilisons la bibliothèque **OpenAI Gym** pour initialiser un environnement de Blackjack. Cela crée un environnement simulé où le joueur peut jouer au Blackjack contre un croupier.

#### 2. **Fonction pour simuler un épisode de Blackjack**

```python
def run_episode(env, hold_score):
    state = env.reset()
    rewards = []
    states = [state]
    is_done = False
    while not is_done:
        action = 1 if state[0] < hold_score else 0  # hit si score joueur < hold_score, sinon stick
        state, reward, is_done, info = env.step(action)  # Obtenez la nouvelle situation
        states.append(state)
        rewards.append(reward)
        if is_done:
            break
    return states, rewards
```
Cette fonction simule un **épisode de Blackjack**. À chaque étape :
- Le joueur décide de **tirer une carte** ("hit") si son score est inférieur au `hold_score` (par exemple 18) ou de **rester** ("stick") autrement.
- Chaque état du jeu et chaque récompense (victoire, perte ou égalité) est stocké.
- L'épisode se termine lorsque le joueur perd, gagne ou atteint un total de 21.

#### 3. **Prédiction Monte Carlo pour la première visite**

```python
from collections import defaultdict
def mc_prediction_first_visit(env, hold_score, gamma, n_episode):
    V = defaultdict(float)
    N = defaultdict(int)
    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, hold_score)
        return_t = 0
        G = {}
        for state_t, reward_t in zip(states_t[1::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            G[state_t] = return_t
        for state, return_t in G.items():
            if state[0] <= 21:  # On ne considère que les états valides (<21)
                V[state] += return_t
                N[state] += 1
    for state in V:
        V[state] = V[state] / N[state]  # Moyenne des retours pour chaque état
    return V
```

Cette fonction est l'implémentation de la **prédiction Monte Carlo pour la première visite**. Elle fonctionne de la manière suivante :
- **V** : Stocke la somme des retours pour chaque état.
- **N** : Stocke le nombre de fois qu'un état a été visité.
- Pour chaque épisode simulé, on collecte les états et récompenses, puis on calcule la **valeur des retours** pour chaque état visité, en partant de la fin de l'épisode (technique de Monte Carlo).
- La fonction calcule ensuite la **valeur moyenne** pour chaque état en divisant la somme des retours par le nombre de visites de l'état.

#### 4. **Lancement des épisodes et calcul de la fonction de valeur**

```python
hold_score = 18
gamma = 1
n_episode = 500000
value = mc_prediction_first_visit(env, hold_score, gamma, n_episode)

print('The value function calculated by first-visit MC prediction:\n', value)
print('Number of states:', len(value))
```

Nous définissons ici le **score de maintien** à 18, et nous simulons **500 000 épisodes** pour estimer la fonction de valeur en utilisant l'approche Monte Carlo. La fonction de valeur calcule la récompense moyenne pour chaque état rencontré dans les épisodes simulés.

#### 5. **Visualisation en 3D de la fonction de valeur**

```python
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import matplotlib

def plot_surface(X, Y, Z, title):
    fig = plt.figure(figsize=(20, 10))
    ax = fig.add_subplot(111, projection='3d')
    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)
    ax.set_xlabel('Player Sum')
    ax.set_ylabel('Dealer Showing')
    ax.set_zlabel('Value')
    ax.set_title(title)
    ax.view_init(ax.elev, -120)
    fig.colorbar(surf)
    plt.show()

def plot_blackjack_value(V):
    player_sum_range = range(12, 22)  # La somme des cartes du joueur
    dealer_show_range = range(1, 11)  # Carte visible du croupier
    X, Y = torch.meshgrid([torch.tensor(player_sum_range), torch.tensor(dealer_show_range)])
    values_to_plot = torch.zeros((len(player_sum_range), len(dealer_show_range), 2))
    for i, player in enumerate(player_sum_range):
        for j, dealer in enumerate(dealer_show_range):
            for k, ace in enumerate([False, True]):
                values_to_plot[i, j, k] = V[(player, dealer, ace)]
    plot_surface(X, Y, values_to_plot[:,:,0].numpy(), "Blackjack Value Function Without Usable Ace")
    plot_surface(X, Y, values_to_plot[:,:,1].numpy(), "Blackjack Value Function With Usable Ace")
```

Ces fonctions permettent de **visualiser en 3D** la fonction de valeur. Nous avons deux graphiques :
- **Avec un As utilisable**.
- **Sans As utilisable**.

Le graphique montre comment la valeur d'un état (probabilité de gagner) varie en fonction de la somme des cartes du joueur et de la carte visible du croupier. L'axe Z représente la valeur de l'état, l'axe X la somme des cartes du joueur, et l'axe Y la carte du croupier.

#### 6. **Exécution de la visualisation**

```python
plot_blackjack_value(value)
```
Cette commande exécute les deux visualisations de la fonction de valeur avec et sans As utilisable. Le résultat est un graphique en 3D montrant la probabilité estimée de gagner dans différentes situations.

### Conclusion

En résumant :
- Le projet simule des épisodes de Blackjack, collecte les retours et calcule la fonction de valeur.
- La fonction de valeur est ensuite visualisée pour mieux comprendre comment le joueur devrait agir dans différentes situations.
- Vous pouvez jouer avec le paramètre `hold_score` ou augmenter le nombre d'épisodes pour affiner les résultats.

Ce projet fournit une excellente illustration de la manière dont les **algorithmes Monte Carlo** sont utilisés pour **apprendre des politiques** dans un jeu comme le Blackjack.
