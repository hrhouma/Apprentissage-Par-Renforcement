
# Introduction

Le contrôle Monte Carlo est une technique d'apprentissage par renforcement qui utilise des échantillons d'épisodes pour estimer la meilleure politique dans un environnement donné. **Off-policy** signifie que l'agent peut apprendre à partir d'une **politique de comportement** (la politique qui génère les actions) différente de la **politique cible** (la politique que l'agent cherche à améliorer).

### Qu'est-ce que le contrôle Monte Carlo Off-Policy ?

Dans l'apprentissage off-policy, nous utilisons deux politiques différentes :
- **La politique cible** (π) : C'est la politique que l'on souhaite optimiser. Elle choisit des actions de manière optimale, basée sur les récompenses attendues.
- **La politique de comportement** (b) : C'est la politique qui explore différentes actions pour générer des expériences.

#### Exemple simple :
Imaginez que vous appreniez à jouer au Blackjack. Votre stratégie cible est de maximiser les chances de gagner, donc elle est "optimale" (exploitation). Mais, pour apprendre, vous commencez par essayer des actions aléatoires (exploration) avec la politique de comportement.

### Pourquoi utiliser l'échantillonnage par importance ?

L'**échantillonnage par importance** nous aide à ajuster les différences entre la politique de comportement et la politique cible. Il permet de réutiliser les données générées par une politique différente tout en les "pondérant" pour refléter ce que la politique cible aurait fait.

#### Exemple :
Supposons que votre politique de comportement vous pousse à tirer une carte dans un état donné, mais la politique cible aurait préféré s'arrêter. En utilisant l'échantillonnage par importance, vous pondérez la contribution de cette action de manière à ne pas biaiser l'apprentissage de la politique cible.

### Calcul de la pondération avec échantillonnage par importance
Voici comment calculer la pondération :

$$w_t = \prod_{k=t}^{T} \frac{\pi(a_k | s_k)}{b(a_k | s_k)}$$

Où :
- π(aₖ | sₖ) est la probabilité de choisir l'action aₖ dans l'état sₖ sous la politique cible.
- b(aₖ | sₖ) est la probabilité de choisir aₖ sous la politique de comportement.
- wₜ est la pondération qui ajuste la différence entre les deux politiques.

### Étapes de l'algorithme MC off-policy

1. **Initialisation** : On initialise la fonction de valeur d'action Q(s, a) avec des valeurs arbitraires pour chaque paire état-action.
   
2. **Génération d'épisodes** : On génère plusieurs épisodes en suivant la **politique de comportement** (aléatoire ou exploratoire).
   
3. **Calcul des retours pondérés** : Pour chaque épisode, on calcule le retour pondéré (pondération calculée à l'aide de l'échantillonnage par importance) pour chaque paire état-action.
   
4. **Mise à jour de Q(s, a)** : On met à jour Q(s, a) en fonction des retours pondérés obtenus.

5. **Amélioration de la politique** : La politique cible est mise à jour en choisissant les actions qui maximisent Q(s, a).

### Implémentation en Python (simplifié)

```python
import torch
import gym
from collections import defaultdict

# Environnement Blackjack
env = gym.make('Blackjack-v0')

def run_episode(env, behavior_policy):
    state = env.reset()
    states, actions, rewards = [], [], []
    done = False
    while not done:
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()
        states.append(state)
        actions.append(action)
        state, reward, done, _ = env.step(action)
        rewards.append(reward)
    return states, actions, rewards

def off_policy_mc_control(env, gamma, n_episodes, behavior_policy):
    Q = defaultdict(lambda: torch.zeros(env.action_space.n))
    returns_sum = defaultdict(float)
    returns_count = defaultdict(int)
    for episode in range(n_episodes):
        states, actions, rewards = run_episode(env, behavior_policy)
        G = 0
        W = 1.0
        for t in range(len(states)-1, -1, -1):
            state, action = states[t], actions[t]
            G = gamma * G + rewards[t]
            returns_sum[(state, action)] += W * G
            returns_count[(state, action)] += 1
            Q[state][action] = returns_sum[(state, action)] / returns_count[(state, action)]
            if action != torch.argmax(Q[state]).item():
                break
            W *= 1.0 / behavior_policy(state)[action]
    policy = {state: torch.argmax(actions).item() for state, actions in Q.items()}
    return Q, policy

# Politique aléatoire
def random_policy(n_actions):
    return torch.ones(n_actions) / n_actions

n_episodes = 500000
gamma = 1.0
random_policy = lambda state: random_policy(env.action_space.n)

optimal_Q, optimal_policy = off_policy_mc_control(env, gamma, n_episodes, random_policy)

print("Politique optimisée : ", optimal_policy)
```

### Conclusion

- Le **contrôle Monte Carlo off-policy** avec échantillonnage par importance permet à un agent d'apprendre une politique optimale même lorsqu'il suit une autre politique pour explorer l'environnement. C'est une méthode puissante qui aide à maximiser la réutilisation des expériences, même si elles ne sont pas générées par la politique cible.
- En simplifiant, l'idée est d'apprendre en utilisant les expériences d'autres politiques tout en s'assurant que ces données sont correctement pondérées pour refléter la politique cible.
- Cela permet d'explorer et d'apprendre efficacement dans des environnements complexes.
