# Explication du Contrôle Monte Carlo On-Policy

- Dans cette section, nous allons mettre en œuvre un contrôle Monte Carlo On-Policy pour optimiser une politique de jeu de Blackjack. 
- Nous allons alterner entre deux phases principales : **l'évaluation** et **l'amélioration de la politique**.

### Processus général

1. **Initialisation avec une action aléatoire** : Nous débutons chaque épisode en choisissant une action de manière aléatoire, un processus connu sous le nom de **"Exploring Starts"**. Cela permet de garantir que toutes les actions possibles sont explorées.
   
2. **Prise d'actions basée sur la fonction Q actuelle** : Après la première action, nous prenons les actions en fonction de la politique actuelle, c'est-à-dire celle dérivée de la fonction Q, qui estime la valeur des actions.

3. **Enregistrement des états, actions et récompenses** : À chaque étape de l'épisode, nous enregistrons les états, les actions effectuées et les récompenses reçues. Ces informations seront utilisées pour mettre à jour la politique lors de la phase d'évaluation.

4. **Optimisation de la politique** : Le contrôle Monte Carlo On-Policy permet de faire converger la politique vers une solution optimale en actualisant la fonction Q et en ajustant la politique en fonction des valeurs calculées.

### Détails du processus d'amélioration

1. **Initialisation des valeurs Q** : Nous commençons avec des valeurs arbitraires faibles pour la fonction Q.
   
2. **Exécution de plusieurs épisodes** : À chaque épisode, nous effectuons une **amélioration de la politique** en mettant à jour la fonction Q basée sur les états, actions et récompenses collectées.

3. **Évaluation des politiques** : Pour chaque épisode, nous appliquons la méthode **first-visit Monte Carlo** pour mettre à jour la fonction Q. Cela consiste à utiliser les premières visites des états pour ajuster les valeurs.

4. **Obtention de la politique optimale** : La politique finale est dérivée de la fonction Q en prenant l'action qui maximise la valeur dans chaque état.

### Simulation et comparaison avec une politique simple

Une fois que la politique optimale a été déterminée, nous allons comparer ses performances avec une **politique simple** qui consiste à prendre des actions selon une règle fixe (par exemple, s'arrêter lorsque le score atteint 18). Nous allons simuler 100 000 épisodes pour les deux politiques et comparer les taux de victoire et de défaite.

Voici les étapes clés à suivre :

1. **Définition d'une politique simple** : Par exemple, une politique où l'on s'arrête lorsque le score atteint 18.
   
2. **Simulation de 100 000 épisodes** : Pour chaque politique (optimale et simple), nous allons simuler un grand nombre d'épisodes et enregistrer les victoires et les défaites.

3. **Comparaison des résultats** : Nous comparerons les taux de victoire et de défaite pour chaque politique afin de déterminer laquelle est la plus efficace.

### Conclusion

Le processus d'optimisation par le contrôle Monte Carlo On-Policy nous permet de trouver une politique optimale à travers l'alternance des phases d'évaluation et d'amélioration. En simulant plusieurs épisodes et en comparant les résultats, nous pouvons démontrer que la politique optimisée donne généralement de meilleurs résultats qu'une politique simple.

