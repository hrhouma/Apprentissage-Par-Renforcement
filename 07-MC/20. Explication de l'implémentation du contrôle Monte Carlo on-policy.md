# Explication du Contrôle Monte Carlo On-Policy

- Dans cette section, nous allons mettre en œuvre un contrôle Monte Carlo On-Policy pour optimiser une politique de jeu de Blackjack. 
- Nous allons alterner entre deux phases principales : **l'évaluation** et **l'amélioration de la politique**.

### Processus général

1. **Initialisation avec une action aléatoire** : Nous débutons chaque épisode en choisissant une action de manière aléatoire, un processus connu sous le nom de **"Exploring Starts"**. Cela permet de garantir que toutes les actions possibles sont explorées.
   
2. **Prise d'actions basée sur la fonction Q actuelle** : Après la première action, nous prenons les actions en fonction de la politique actuelle, c'est-à-dire celle dérivée de la fonction Q, qui estime la valeur des actions.

3. **Enregistrement des états, actions et récompenses** : À chaque étape de l'épisode, nous enregistrons les états, les actions effectuées et les récompenses reçues. Ces informations seront utilisées pour mettre à jour la politique lors de la phase d'évaluation.

4. **Optimisation de la politique** : Le contrôle Monte Carlo On-Policy permet de faire converger la politique vers une solution optimale en actualisant la fonction Q et en ajustant la politique en fonction des valeurs calculées.

### Détails du processus d'amélioration

1. **Initialisation des valeurs Q** : Nous commençons avec des valeurs arbitraires faibles pour la fonction Q.
   
2. **Exécution de plusieurs épisodes** : À chaque épisode, nous effectuons une **amélioration de la politique** en mettant à jour la fonction Q basée sur les états, actions et récompenses collectées.

3. **Évaluation des politiques** : Pour chaque épisode, nous appliquons la méthode **first-visit Monte Carlo** pour mettre à jour la fonction Q. Cela consiste à utiliser les premières visites des états pour ajuster les valeurs.

4. **Obtention de la politique optimale** : La politique finale est dérivée de la fonction Q en prenant l'action qui maximise la valeur dans chaque état.

### Simulation et comparaison avec une politique simple

Une fois que la politique optimale a été déterminée, nous allons comparer ses performances avec une **politique simple** qui consiste à prendre des actions selon une règle fixe (par exemple, s'arrêter lorsque le score atteint 18). Nous allons simuler 100 000 épisodes pour les deux politiques et comparer les taux de victoire et de défaite.

Voici les étapes clés à suivre :

1. **Définition d'une politique simple** : Par exemple, une politique où l'on s'arrête lorsque le score atteint 18.
   
2. **Simulation de 100 000 épisodes** : Pour chaque politique (optimale et simple), nous allons simuler un grand nombre d'épisodes et enregistrer les victoires et les défaites.

3. **Comparaison des résultats** : Nous comparerons les taux de victoire et de défaite pour chaque politique afin de déterminer laquelle est la plus efficace.

### Conclusion

Le processus d'optimisation par le contrôle Monte Carlo On-Policy nous permet de trouver une politique optimale à travers l'alternance des phases d'évaluation et d'amélioration. En simulant plusieurs épisodes et en comparant les résultats, nous pouvons démontrer que la politique optimisée donne généralement de meilleurs résultats qu'une politique simple.


------------------
# Annexe : **Optimisation de la Politique avec le Contrôle Monte Carlo On-Policy**
------------------

## **Introduction**
Dans ce cours, nous allons nous plonger dans l’algorithme **Monte Carlo On-Policy Control**, un outil puissant pour trouver une politique optimale dans les environnements où les transitions entre états ne sont pas explicitement modélisées, comme dans le jeu de **Blackjack**. Nous allons détailler chaque étape de l’algorithme, comprendre pourquoi et comment il converge vers une solution optimale, et enfin comparer les performances d’une **politique optimale** avec une **politique simple**.

### **Objectifs du Cours**
1. **Comprendre le processus d’évaluation et d’amélioration d’une politique** à l’aide de l’algorithme Monte Carlo On-Policy.
2. **Maîtriser l’utilisation de la fonction de valeur Q** pour estimer la qualité des actions dans différents états du jeu.
3. **Visualiser et comparer** les résultats des politiques optimales et simples dans un environnement de Blackjack.

---

## **Concepts Fondamentaux**

### **Qu’est-ce que l’algorithme Monte Carlo On-Policy ?**
L'algorithme **Monte Carlo On-Policy** est une méthode d'apprentissage par renforcement qui alterne entre deux phases : 
- **Évaluation de la politique actuelle** : Évaluer la qualité de la politique utilisée à travers plusieurs épisodes du jeu.
- **Amélioration de la politique** : Modifier la politique en fonction des résultats de l’évaluation afin de choisir des actions qui maximisent la récompense à long terme.

L'idée principale de cet algorithme est d'exploiter des données obtenues par l'exécution d'une politique, en ajustant cette politique progressivement pour tendre vers une solution optimale.

### **Exploring Starts** : Pourquoi commencer avec une action aléatoire ?
Dans chaque épisode, la première action est choisie **aléatoirement**. Ce processus est connu sous le nom d'**Exploring Starts** et est crucial pour garantir que chaque état-action ait une chance d'être exploré. Sans cela, certaines actions pourraient être négligées et la politique pourrait devenir sous-optimale.

---

## **Étape par Étape : L'Algorithme Monte Carlo On-Policy**

### **Étape 1 : Initialisation**
Avant de commencer à simuler des épisodes, nous devons :
- **Initialiser une fonction de valeur Q** pour chaque combinaison d’état et d’action, avec des valeurs arbitraires faibles. 
- **Définir le taux d'actualisation**, ou **gamma**, qui détermine l’importance des récompenses futures. Dans notre exemple, nous allons utiliser `gamma = 1`, ce qui signifie que nous donnons une égale importance à toutes les récompenses à travers le temps.

```python
gamma = 1
n_episode = 500000  # Nombre d'épisodes simulés
```

### **Étape 2 : Simulation d’un épisode**
Chaque épisode se déroule comme suit :
1. **Choix d'une action initiale aléatoire** (Exploring Starts).
2. **Choix des actions suivantes** en fonction de la politique actuelle basée sur la fonction Q.
3. **Enregistrement des états, des actions et des récompenses** pour chaque étape de l’épisode.

```python
def run_episode(env, Q, n_action):
    state = env.reset()
    actions, states, rewards = [], [], []
    action = torch.randint(0, n_action, [1]).item()
    while not is_done:
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
        action = torch.argmax(Q[state]).item()
    return states, actions, rewards
```

---

### **Étape 3 : Amélioration de la Politique**
Après chaque épisode, nous mettons à jour la politique en utilisant la méthode de **première visite Monte Carlo** (First-Visit MC), qui consiste à utiliser la première occurrence de chaque état pour mettre à jour la fonction Q.

**Comment cela fonctionne :**
- À chaque visite d’un état, nous calculons la récompense obtenue à partir de cette étape jusqu’à la fin de l’épisode (Return).
- Cette valeur est ensuite utilisée pour ajuster la fonction Q, qui représente la valeur attendue de chaque action dans chaque état.

```python
for state_action, return_t in G.items():
    state, action = state_action
    if state[0] <= 21:
        G_sum[state_action] += return_t
        N[state_action] += 1
        Q[state][action] = G_sum[state_action] / N[state_action]
```

### **Étape 4 : Obtention de la Politique Optimale**
Une fois que nous avons simulé suffisamment d’épisodes et mis à jour les valeurs Q, nous pouvons dériver la politique optimale en choisissant, pour chaque état, l’action qui maximise la valeur Q.

```python
policy = {}
for state, actions in Q.items():
    policy[state] = torch.argmax(actions).item()
```

---

## **Visualisation des Résultats**
Nous utilisons des graphiques 3D pour **visualiser la fonction de valeur optimale**, avec et sans un As utilisable (qui peut valoir 1 ou 11).

```python
def plot_blackjack_value(V):
    player_sum_range = range(12, 22)
    dealer_show_range = range(1, 11)
    X, Y = torch.meshgrid([torch.tensor(player_sum_range), torch.tensor(dealer_show_range)])
    values_to_plot = torch.zeros((len(player_sum_range), len(dealer_show_range), 2))
    for i, player in enumerate(player_sum_range):
        for j, dealer in enumerate(dealer_show_range):
            for k, ace in enumerate([False, True]):
                values_to_plot[i, j, k] = V[(player, dealer, ace)]
    plot_surface(X, Y, values_to_plot[:,:,0].numpy(), "Blackjack Value Function Without Usable Ace")
    plot_surface(X, Y, values_to_plot[:,:,1].numpy(), "Blackjack Value Function With Usable Ace")
```

Les graphiques montrent la valeur attendue pour différents états, en fonction de la somme des cartes du joueur et de la carte visible du croupier.

---

## **Comparaison : Politique Optimale vs Politique Simple**
Nous comparons maintenant la politique optimisée avec une politique simple où le joueur s’arrête lorsque sa somme atteint 18. Nous simulons 100 000 épisodes pour chaque politique.

```python
n_win_optimal, n_win_simple = 0, 0
for _ in range(n_episode):
    reward_optimal = simulate_episode(env, optimal_policy)
    if reward_optimal == 1:
        n_win_optimal += 1
    reward_simple = simulate_episode(env, hold_policy)
    if reward_simple == 1:
        n_win_simple += 1
```

Les résultats montrent que la politique optimisée a un taux de victoire supérieur à la politique simple.

---------------------
# Annexe 2
---------------------

# Contrôle Monte Carlo On-Policy pour Blackjack

Dans cette section, nous allons mettre en œuvre l'algorithme de contrôle Monte Carlo **On-Policy** pour résoudre un jeu de Blackjack. Cet algorithme optimise une politique en alternant entre deux phases principales : l'évaluation et l'amélioration. L'objectif est de trouver une politique qui maximise les gains du joueur au Blackjack.

## Processus Général

1. **Initialisation avec une action aléatoire** : Nous commençons chaque épisode en prenant une action aléatoire, une approche connue sous le nom d'**"Exploring Starts"**. Cela garantit que toutes les actions sont explorées au moins une fois.

2. **Prise d'actions basée sur la fonction Q actuelle** : Après la première action, nous basons les actions suivantes sur la politique dérivée de la fonction Q, qui estime la valeur des actions dans chaque état du jeu.

3. **Enregistrement des états, actions et récompenses** : Nous enregistrons chaque état visité, chaque action prise et la récompense associée. Ces informations sont ensuite utilisées pour améliorer la politique.

4. **Optimisation de la politique** : Nous ajustons la politique en fonction des valeurs de la fonction Q obtenue, ce qui permet d'améliorer la stratégie au fur et à mesure des simulations.

## Étapes Détaillées

### Étape 1 : Initialisation et Exploration

Dans cette phase, nous :
- Initialisons un épisode.
- Prenons une action aléatoire (Exploring Start).
- Après cette action, nous choisissons les actions en fonction de la fonction Q actuelle.
- Enregistrons les états, actions et récompenses à chaque étape.

### Étape 2 : Amélioration de la Politique

Une fois que nous avons enregistré les actions et récompenses, nous utilisons ces informations pour améliorer la politique en :

1. Initialisant la fonction Q avec de petites valeurs arbitraires.
2. Exécutant plusieurs épisodes pour explorer l'environnement et ajuster la fonction Q.
3. Effectuant des améliorations de la politique et des évaluations à chaque épisode en mettant à jour la fonction Q en fonction des récompenses accumulées.
4. Finalement, la fonction Q et la politique sont optimisées, et nous prenons les actions optimales pour chaque état.

La formule qui guide l'amélioration de la politique, où nous choisissons l'action optimale pour chaque état s, est :

$$\pi(s) = \arg\max_a Q(s, a)$$

### Étape 3 : Visualisation de la Politique Optimale

Nous avons visualisé la fonction de valeur pour des états avec et sans As utilisable. Voici un exemple de graphique généré pour une **fonction de valeur avec As utilisable** :

[Insérez ici l'image du graphique de la fonction de valeur avec As utilisable]

Ce graphique montre les valeurs associées à chaque combinaison d'états, où l'As est utilisable (c'est-à-dire qu'il peut valoir 1 ou 11).

## Comparaison avec une Politique Simple

Nous avons simulé **100 000 épisodes** de Blackjack sous la politique optimale que nous avons apprise, ainsi qu'une politique simple qui consiste à s'arrêter lorsque le score atteint 18. 

### Étapes de la Simulation

1. Définir une **politique simple** : la politique où l'on s'arrête à 18.
2. Spécifier et exécuter **100 000 épisodes** pour chaque politique (optimale et simple).
3. Enregistrer les victoires et les défaites pour chaque épisode.
4. Comparer les taux de victoire et de défaite pour les deux politiques.

### Résultats de la Simulation

Nous avons comparé les performances des deux politiques. Voici un récapitulatif des étapes :

1. **Définition de la politique simple** : l'action "stick" (arrêt) est prise lorsque le score atteint 18.
2. **Simulation de 100 000 épisodes** et suivi des victoires et défaites.
3. **Comparaison des résultats** entre la politique simple et la politique optimale.

Ces résultats montrent que la **politique optimale** surpasse la politique simple en termes de probabilités de victoires, confirmant ainsi l'efficacité de notre approche Monte Carlo.












## **Conclusion**

En utilisant l’algorithme Monte Carlo On-Policy, nous avons réussi à optimiser la stratégie de jeu pour le Blackjack. L’évaluation par épisodes et l’amélioration progressive de la politique nous ont permis de trouver une politique qui maximise les chances de victoire par rapport à une approche simple. Cette méthode est particulièrement utile dans les environnements où les transitions ne sont pas clairement définies.

