# Quiz : Monte Carlo en Apprentissage par Renforcement

#### 1. Introduction à la Méthode Monte Carlo en RL

1. Dans le contexte de l'apprentissage par renforcement, que représente la méthode Monte Carlo ?
   - A) Une technique de supervision
   - B) Une méthode de simulation basée sur des échantillons d'expériences complètes
   - C) Une technique de clustering
   - D) Un modèle de prédiction basé sur les séries temporelles

2. Comment la méthode Monte Carlo estime-t-elle les valeurs des états ou des paires état-action ?
   - A) En utilisant un modèle de transition connu
   - B) En prenant la moyenne des récompenses cumulées à la fin des épisodes
   - C) En utilisant uniquement la récompense immédiate
   - D) En minimisant la variance

3. Quelle est la principale caractéristique de Monte Carlo en RL ?
   - A) Elle met à jour les valeurs après chaque action
   - B) Elle nécessite la fin d'un épisode pour mettre à jour les estimations
   - C) Elle utilise un modèle de dynamique de l'environnement
   - D) Elle ne prend pas en compte les récompenses futures

4. Pourquoi appelle-t-on cette méthode "Monte Carlo" ?
   - A) En référence à un mathématicien nommé Monte Carlo
   - B) Parce qu’elle utilise des expériences aléatoires, similaires aux jeux de hasard
   - C) Parce qu’elle utilise des réseaux de neurones
   - D) Parce qu’elle se base sur la régression linéaire

---

#### 2. Concepts Clés

5. Quelle est la différence entre la méthode Monte Carlo et les méthodes TD (Temporal Difference) ?
   - A) Monte Carlo utilise des approximations, tandis que TD utilise des valeurs exactes
   - B) Monte Carlo met à jour après chaque étape, TD à la fin d'un épisode
   - C) Monte Carlo nécessite la fin de l'épisode pour mettre à jour, TD peut mettre à jour après chaque action
   - D) TD est utilisé pour les problèmes de classification uniquement

6. Que signifie "Estimation par la méthode Monte Carlo" ?
   - A) Calculer une moyenne des récompenses pour un état en utilisant des transitions aléatoires
   - B) Calculer une somme des récompenses pour chaque étape
   - C) Utiliser une estimation mathématique des récompenses
   - D) Utiliser des approximations basées sur les voisins

7. Dans quel type d'environnement la méthode Monte Carlo est-elle la plus utile ?
   - A) Dans des environnements à épisodes courts
   - B) Dans des environnements continus et sans fin
   - C) Dans des environnements où les épisodes peuvent être terminés
   - D) Dans des environnements sans récompenses

8. Comment Monte Carlo estime-t-il la valeur d'un état ?
   - A) En utilisant la valeur du prochain état
   - B) En prenant la moyenne des récompenses cumulées pour cet état à la fin de plusieurs épisodes
   - C) En calculant uniquement la première récompense
   - D) En utilisant une valeur constante

---

#### 3. Variantes de la Méthode Monte Carlo

9. Quelle est la différence entre les méthodes "Every-visit" et "First-visit" Monte Carlo ?
   - A) "Every-visit" met à jour après chaque action, "First-visit" uniquement à la fin
   - B) "First-visit" ne met à jour que la première occurrence d'un état, "Every-visit" à chaque occurrence
   - C) "Every-visit" utilise les valeurs estimées, "First-visit" utilise les valeurs exactes
   - D) Les deux utilisent uniquement les récompenses finales de chaque épisode

10. Quand utilise-t-on généralement la méthode "First-visit" Monte Carlo ?
    - A) Quand on veut ignorer les premières occurrences des états
    - B) Quand on veut estimer les valeurs en utilisant uniquement la première occurrence d’un état dans un épisode
    - C) Quand on veut utiliser toutes les occurrences d’un état
    - D) Quand on connaît les transitions de l'environnement

11. Que signifie "Politique On-policy" dans le cadre de Monte Carlo ?
    - A) La politique est fixe et ne change jamais
    - B) La politique utilisée pour évaluer un état est la même que celle utilisée pour interagir avec l'environnement
    - C) La politique est aléatoire à chaque épisode
    - D) La politique n'est jamais améliorée

12. Dans une politique "Off-policy" Monte Carlo, que fait l’agent ?
    - A) Il suit et améliore sa propre politique uniquement
    - B) Il apprend à partir d'une politique différente de celle qu'il suit pour ses actions
    - C) Il n'apprend rien de ses expériences
    - D) Il utilise un modèle explicite de transitions

---

#### 4. Fonctionnement et Mise à Jour

13. Comment la méthode Monte Carlo met-elle à jour les valeurs d’état ?
    - A) En ajustant la valeur d’état après chaque épisode en prenant la moyenne des récompenses cumulées
    - B) En ajustant la valeur d’état après chaque action
    - C) En ignorant les récompenses futures
    - D) En utilisant un taux d’apprentissage très faible

14. Que représente une "évaluation de politique" en utilisant Monte Carlo ?
    - A) L’apprentissage des actions optimales pour chaque état
    - B) L’évaluation de la valeur des états sous une politique donnée
    - C) La création d'une politique optimale
    - D) L'évaluation des récompenses immédiates uniquement

15. Qu'est-ce que "l'amélioration de politique" dans Monte Carlo ?
    - A) Améliorer une politique en ajustant les valeurs d'état en fonction des récompenses cumulées
    - B) Ne pas utiliser de politique fixe
    - C) Réduire le nombre d’actions possibles
    - D) Minimiser les états futurs

16. Dans quelle situation une méthode Monte Carlo n'est-elle pas efficace ?
    - A) Dans un environnement à épisodes continus
    - B) Dans des environnements avec des épisodes définis
    - C) Lorsqu'il y a beaucoup de variations dans les récompenses
    - D) Quand les transitions d’état sont aléatoires

---

#### 5. Applications et Avantages

17. Pourquoi la méthode Monte Carlo est-elle adaptée pour les environnements où un modèle de l'environnement n'est pas disponible ?
    - A) Parce qu'elle se base uniquement sur des expériences concrètes sans nécessiter de modèle de transition
    - B) Parce qu'elle nécessite des calculs complexes des transitions
    - C) Parce qu’elle utilise une politique aléatoire uniquement
    - D) Parce qu’elle ignore les récompenses futures

18. Pourquoi les méthodes Monte Carlo peuvent-elles être lentes à apprendre ?
    - A) Parce qu'elles nécessitent un modèle de transition complexe
    - B) Parce qu'elles nécessitent la fin d'un épisode avant de mettre à jour les valeurs
    - C) Parce qu'elles ignorent les récompenses futures
    - D) Parce qu'elles ajustent les valeurs à chaque étape

19. Quel est un avantage de la méthode Monte Carlo pour les environnements complexes ?
    - A) Elle permet d'obtenir des estimations précises sans modèle complet de l’environnement
    - B) Elle ne fonctionne que dans des environnements simples
    - C) Elle nécessite des récompenses immédiates
    - D) Elle ne prend en compte que les transitions d’états fixes

20. Dans une application pratique, comment Monte Carlo pourrait-il être utilisé pour optimiser un système de recommandation ?
    - A) En utilisant un modèle fixe de toutes les interactions utilisateur
    - B) En estimant la valeur des recommandations basées sur des épisodes complets d’interactions utilisateurs
    - C) En ajustant la recommandation après chaque clic
    - D) En ignorant l’historique des interactions précédentes
