### Partie 2 : Simulation d'une politique simple dans le Blackjack avec prédiction Monte Carlo

Dans cette partie, nous allons définir une **politique simple** dans le jeu de Blackjack. Cette politique consiste à continuer de demander des cartes tant que la somme des cartes est inférieure à **18**.

#### Code pour exécuter un épisode de Blackjack :

```python
import torch
import gym
from collections import defaultdict

# Créer l'environnement Blackjack
env = gym.make('Blackjack-v0')

# Fonction pour exécuter un épisode
def run_episode(env, hold_score):
    states = env.reset()
    rewards = []
    states = [states]
    is_done = False
    
    # Tant que le jeu n'est pas terminé
    while not is_done:
        action = 1 if states[0] < hold_score else 0  # Si la somme est inférieure au score limite, "hit", sinon "stick"
        state, reward, is_done, info = env.step(action)  # Exécuter l'action
        states.append(state)
        rewards.append(reward)
        if is_done:
            break
    
    return states, rewards
```

#### Explication de la politique :
- **hold_score** : Le score seuil où le joueur décide d'arrêter de demander des cartes. Ici, fixé à **18**.
- **env.step(action)** : Exécute l'action (1 pour **hit**, 0 pour **stick**) et retourne le nouvel état, la récompense, et si l'épisode est terminé.

#### Code pour la prédiction Monte Carlo (première visite) :

```python
def mc_prediction_first_visit(env, hold_score, gamma, n_episode):
    V = defaultdict(float)
    N = defaultdict(int)
    
    # Simulation des épisodes
    for episode in range(n_episode):
        states_t, rewards_t = run_episode(env, hold_score)
        return_t = 0
        G = {}  # Valeur de retour pour chaque état
        
        # Calculer les retours pour chaque état en commençant par la fin
        for state_t, reward_t in zip(states_t[1:][::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            G[state_t] = return_t
        
        # Mettre à jour la fonction de valeur pour chaque état rencontré
        for state, return_t in G.items():
            if state[0] <= 21:  # On ne met à jour que les états où la somme est <= 21
                V[state] += return_t
                N[state] += 1
    
    # Calcul de la moyenne des retours
    for state in V:
        V[state] = V[state] / N[state]
    
    return V
```

### Simulation des paramètres et exécution de la prédiction :

```python
hold_score = 18  # Politique simple : arrêter de demander des cartes si la somme atteint 18
gamma = 1  # Taux d'actualisation
n_episode = 500000  # Nombre d'épisodes simulés

# Calculer la prédiction de Monte Carlo (première visite)
value = mc_prediction_first_visit(env, hold_score, gamma, n_episode)
```

### Explication des variables :
- **gamma** : Taux d'actualisation (ici, on suppose **1** car les épisodes sont courts et les récompenses sont immédiates).
- **n_episode** : Nombre d'épisodes simulés (ici **500 000** pour une estimation plus précise).

Cette fonction calcule la valeur de chaque état rencontré dans le jeu en appliquant la politique définie et en utilisant la méthode de la **première visite** de Monte Carlo. Les valeurs des états sont mises à jour au fur et à mesure que les épisodes sont simulés, ce qui permet d'obtenir une estimation de la fonction de valeur sous la politique définie.

### Suite de l'explication :
Dans la prochaine partie, nous examinerons les résultats de cette prédiction et nous ajusterons éventuellement la politique pour optimiser les gains. Nous utiliserons également des variantes de la méthode Monte Carlo, comme la **prédiction par toutes les visites**.

