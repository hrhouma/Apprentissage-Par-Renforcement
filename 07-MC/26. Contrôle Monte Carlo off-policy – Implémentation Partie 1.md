### Importation des Bibliothèques et Création de l'Environnement
Tout d'abord, nous allons importer les bibliothèques nécessaires et créer l'environnement Blackjack.

```python
import torch
import gym
from collections import defaultdict

# Créer l'environnement Blackjack
env = gym.make('Blackjack-v0')
```

### Politique de Comportement Aléatoire (Behavior Policy)
Nous définissons la politique de comportement (behavior policy) qui choisit des actions aléatoires avec des probabilités égales.

```python
def gen_random_policy(n_action):
    """
    Générer une politique aléatoire où chaque action a la même probabilité d'être choisie.
    @param n_action: nombre d'actions disponibles
    @return: fonction de politique
    """
    probs = torch.ones(n_action) / n_action  # Probabilités égales pour chaque action
    def policy_function(state):
        return probs
    return policy_function
```

### Exécution d'un Épisode avec la Politique de Comportement
Nous allons maintenant exécuter un épisode en suivant la politique de comportement aléatoire. Cette fonction enregistre les états, actions, et récompenses pour chaque étape de l'épisode.

```python
def run_episode(env, behavior_policy):
    """
    Exécuter un épisode complet avec la politique de comportement donnée.
    @param env: Environnement OpenAI Gym
    @param behavior_policy: Politique de comportement
    @return: Liste des états, actions, et récompenses de l'épisode
    """
    state = env.reset()
    rewards = []
    actions = []
    states = []
    is_done = False
    while not is_done:
        # Sélectionner l'action selon la politique de comportement
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()
        actions.append(action)
        states.append(state)
        # Effectuer l'action et recevoir la récompense
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
    return states, actions, rewards
```

### Implémentation du Contrôle Monte Carlo Off-Policy avec Importance Sampling
Cette partie met en œuvre le contrôle Monte Carlo off-policy en utilisant l'échantillonnage d'importance pour ajuster les Q-valeurs.

```python
def mc_control_off_policy(env, gamma, n_episode, behavior_policy):
    """
    Obtenir la politique optimale avec le contrôle MC off-policy en utilisant l'échantillonnage d'importance.
    @param env: Environnement OpenAI Gym
    @param gamma: Facteur de réduction (discount factor)
    @param n_episode: Nombre d'épisodes à simuler
    @param behavior_policy: Politique de comportement pour l'exploration
    @return: Q-fonction optimale et la politique optimale
    """
    n_action = env.action_space.n
    G_sum = defaultdict(float)  # Somme des retours pondérés
    N = defaultdict(int)  # Compte des visites (nombre d'occurrences)
    Q = defaultdict(lambda: torch.empty(n_action))  # Q-valeurs pour chaque état-action
    for episode in range(n_episode):
        W = {}
        w = 1.
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0
        G = {}
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            G[(state_t, action_t)] = return_t
            W[(state_t, action_t)] = w
            # Vérifier si l'action diffère de celle recommandée par la politique cible
            if action_t != torch.argmax(Q[state_t]).item():
                break
            w *= 1. / behavior_policy(state_t)[action_t]
        # Mise à jour des Q-valeurs en fonction de l'échantillonnage d'importance
        for state_action, return_t in G.items():
            state, action = state_action
            if state[0] <= 21:  # Filtrer les états valides
                G_sum[state_action] += return_t * W[state_action]
                N[state_action] += 1
                Q[state][action] = G_sum[state_action] / N[state_action]
    policy = {}
    # Construire la politique optimale en choisissant l'action avec la plus haute valeur Q
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy
```

### Méthode d'Actualisation Incrémentale (Incremental Update Method)
Cette version du contrôle Monte Carlo off-policy met à jour les Q-valeurs de manière incrémentale pour améliorer l'efficacité en termes de mémoire.

```python
def mc_control_off_policy_incremental(env, gamma, n_episode, behavior_policy):
    """
    Implémentation incrémentale du contrôle MC off-policy avec mise à jour des Q-valeurs sur le vol.
    @param env: Environnement OpenAI Gym
    @param gamma: Facteur de réduction
    @param n_episode: Nombre d'épisodes
    @param behavior_policy: Politique de comportement
    @return: Q-fonction optimale et la politique optimale
    """
    n_action = env.action_space.n
    N = defaultdict(int)  # Nombre de visites (occurrences) pour chaque état-action
    Q = defaultdict(lambda: torch.empty(n_action))  # Q-fonction pour chaque état-action
    for episode in range(n_episode):
        W = 1.  # Importance sampling weight
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0.
        # Boucle à travers les étapes de l'épisode (en commençant par la fin)
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t
            N[(state_t, action_t)] += 1
            # Mise à jour incrémentale des Q-valeurs
            Q[state_t][action_t] += (W / N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])
            if action_t != torch.argmax(Q[state_t]).item():
                break
            # Mise à jour du poids d'importance
            W *= 1./ behavior_policy(state_t)[action_t]
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy
```

### Simulation et Test de la Politique Optimale
Nous allons maintenant exécuter la méthode et comparer la performance de la politique optimale.

```python
gamma = 1  # Facteur de réduction
n_episode = 500000  # Nombre d'épisodes à simuler

# Générer une politique de comportement aléatoire
random_policy = gen_random_policy(env.action_space.n)

# Exécuter le contrôle MC off-policy avec mise à jour incrémentale
optimal_Q, optimal_policy = mc_control_off_policy_incremental(env, gamma, n_episode, random_policy)

# Fonction pour simuler un épisode avec la politique optimale
def simulate_episode(env, policy):
    state = env.reset()
    is_done = False
    while not is_done:
        action = policy[state]
        state, reward, is_done, info = env.step(action)
        if is_done:
            return reward

# Simuler 100 000 épisodes pour tester la politique optimale
n_episode = 100000
n_win_optimal = 0
n_lose_optimal = 0

for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_optimal += 1
    elif reward == -1:
        n_lose_optimal += 1

# Afficher les résultats
print('Probabilité de victoire avec la politique optimale: {}'.format(n_win_optimal / n_episode))
print('Probabilité de défaite avec la politique optimale: {}'.format(n_lose_optimal / n_episode))
```

### Conclusion

- Ce code implémente le contrôle Monte Carlo off-policy avec une politique de comportement aléatoire pour explorer l'espace des actions, puis met à jour la politique cible (greedy) en utilisant l'échantillonnage d'importance. La méthode d'actualisation incrémentale permet d'améliorer l'efficacité du processus de mise à jour des Q-valeurs, ce qui est essentiel lorsque l'on travaille avec de grands ensembles de données.
- Cette approche peut être appliquée à d'autres environnements d'apprentissage par renforcement, et peut être enrichie en ajoutant des techniques comme la réduction progressive d'ε (epsilon-greedy) pour améliorer l'exploration pendant l'apprentissage.
