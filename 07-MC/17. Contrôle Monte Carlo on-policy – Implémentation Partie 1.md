# Étape 1

- L'algorithme de **Monte Carlo Control On-policy** pour le Blackjack, tel qu'il est implémenté dans votre code, suit plusieurs étapes pour déterminer la meilleure politique en jouant au Blackjack. 
- Voici une explication détaillée de chacune des étapes du code :

### Fonction **run_episode**

Cette fonction simule un épisode de Blackjack. Elle exécute des actions à chaque tour, stocke les états, les actions et les récompenses, et continue jusqu'à ce que l'épisode soit terminé (lorsque le jeu est terminé, soit par une victoire, une défaite ou un "bust"). 

```python
def run_episode(env, Q, n_action):
    states = []
    actions = []
    rewards = []
    state = env.reset()
    is_done = False
    while not is_done:
        action = torch.randint(0, n_action, [1]).item()  # Choix aléatoire d'action au début
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)  # Effectue l'action dans l'environnement
        rewards.append(reward)
        if is_done:
            break
        action = torch.argmax(Q[state]).item()  # Mise à jour de l'action avec la meilleure selon Q
    return states, actions, rewards
```

- **env.step(action)** : Simule une action dans l'environnement et renvoie le nouvel état, la récompense, l'information si l'épisode est terminé (`is_done`).
- **torch.argmax(Q[state])** : Sélectionne l'action optimale en fonction de la fonction \( Q \), qui est progressivement mise à jour.

### Fonction **mc_control_on_policy**

Cette fonction implémente l'algorithme Monte Carlo Control en mode **on-policy**, où la politique est mise à jour à chaque itération en fonction des valeurs calculées \( Q(s, a) \).

```python
def mc_control_on_policy(env, gamma, n_episode):
    n_action = env.action_space.n  # Nombre d'actions possibles
    G_sum = defaultdict(float)
    N = defaultdict(int)  # Nombre de visites
    Q = defaultdict(lambda: torch.empty(env.action_space.n))  # Initialisation de Q(s,a)
    
    for episode in range(n_episode):
        states_t, actions_t, rewards_t = run_episode(env, Q, n_action)  # Exécute un épisode
        return_t = 0
        G = {}
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t  # Calcul du retour de la récompense pour (s, a)
            G[(state_t, action_t)] = return_t  # Stockage du retour pour (s, a)
        
        for state_action, return_t in G.items():
            state, action = state_action
            if state[0] <= 21:  # Si l'état est valide (joueur n'a pas dépassé 21)
                G_sum[state_action] += return_t
                N[state_action] += 1
                Q[state][action] = G_sum[state_action] / N[state_action]  # Mise à jour de Q(s,a)
    
    policy = defaultdict(int)
    for state, actions in Q.items():
        policy[state] = torch.argmax(Q[state]).item()  # Mise à jour de la politique avec l'action optimale
    return Q, policy
```

### Explication de l'algorithme :

1. **Initialisation** :
   - **Q** : Fonction de valeur-action qui est mise à jour au fur et à mesure des épisodes.
   - **G_sum** et **N** : Suivent respectivement la somme des retours et le nombre de visites pour chaque paire \( (s, a) \).

2. **Boucle d'épisodes** :
   - Pour chaque épisode, on exécute la fonction `run_episode` qui simule une partie entière de Blackjack.
   - Ensuite, la récompense cumulée \( G_t \) pour chaque état-action est calculée en partant de la fin de l'épisode vers le début.

3. **Mise à jour de Q et de la politique** :
   - \( Q(s, a) \) est mis à jour en prenant la moyenne des retours pour chaque paire \( (s, a) \).
   - La politique est mise à jour pour chaque état en prenant l'action qui maximise \( Q(s, a) \).

### Poursuivre avec les visualisations

Dans la prochaine étape, vous pouvez exécuter cet algorithme sur plusieurs épisodes (par exemple 500 000), puis utiliser des bibliothèques comme `matplotlib` pour visualiser les résultats, comme illustré dans les précédentes parties.

