# Partie 1 : Introduction au contrôle Monte Carlo avec échantillonnage par importance pondérée

## Explication :

Le **contrôle Monte Carlo avec échantillonnage par importance pondérée** est une méthode d'apprentissage par renforcement qui permet de **réutiliser des expériences générées par une politique différente** de celle que l'on cherche à optimiser (politique cible). L'idée clé est de pondérer les retours pour compenser le fait que les actions prises sous la **politique de comportement** ne correspondent pas nécessairement à celles que la **politique cible** aurait choisies.

En pratique, cela signifie que l'algorithme peut **utiliser des expériences générées par une politique exploratoire** pour mettre à jour une politique plus optimale (souvent une politique "greedy" qui choisit toujours la meilleure action possible). 

L'objectif final est de trouver la **meilleure politique cible** en utilisant une méthode d'échantillonnage qui prend en compte l'importance des données générées sous la politique de comportement.

### Importance pondérée vs échantillonnage classique :
Dans l'échantillonnage classique, nous calculons simplement la moyenne des retours, mais dans l'échantillonnage par importance pondérée, nous appliquons des **poids** pour ajuster les retours en fonction de la différence entre la politique de comportement et la politique cible. Cela permet de **réduire la variance** et d'améliorer la stabilité de l'algorithme.

Voici un aperçu de la formule de l'importance pondérée :

$$V_{t} = \frac{\sum_{k=1}^{n} w_k R_k}{\sum_{k=1}^{n} w_k}$$

Avec :
- $w_k$ étant le rapport des probabilités entre la politique cible et la politique de comportement pour l'action $a_k$ dans l'état $s_k$,
- $R_k$ étant la récompense observée.

### Résumé des avantages :
- **Variance plus faible** : En pondérant les retours, on obtient une variance plus faible par rapport à l'échantillonnage par importance classique, ce qui signifie que les résultats fluctuent moins entre différentes exécutions de l'algorithme.
- **Utilisation efficace des données** : L'échantillonnage par importance permet d'exploiter pleinement les données collectées sous une politique exploratoire, même si elle diffère de la politique que l'on cherche à optimiser.

## Extrait du code :

Voici un exemple simple de génération d'une politique aléatoire qui sélectionne chaque action avec une probabilité égale. Cette fonction servira de **politique de comportement** pour générer des actions aléatoires.

```python
import torch
import gym

# Création de l'environnement Blackjack avec Gym
env = gym.make('Blackjack-v0')

# Génération d'une politique aléatoire
def gen_random_policy(n_action):
    probs = torch.ones(n_action) / n_action
    def policy_function(state):
        return probs
    return policy_function

# Génère une politique aléatoire avec un nombre donné d'actions
random_policy = gen_random_policy(env.action_space.n)
```

### Explication détaillée du code :

1. **`env = gym.make('Blackjack-v0')`** :  
   Cela crée un environnement simulé de Blackjack à l'aide de **Gym**, une bibliothèque populaire pour tester des algorithmes d'apprentissage par renforcement dans des environnements simulés.

2. **`gen_random_policy(n_action)`** :  
   Cette fonction génère une **politique aléatoire** où chaque action a une probabilité égale d'être sélectionnée. La fonction `policy_function` renvoie un vecteur de probabilités uniformes qui est ensuite utilisé pour sélectionner des actions de manière aléatoire.

### Concept clé : 
- **Politique de comportement (behavior policy)** : C'est la politique qui génère des données. Dans notre exemple, il s'agit d'une politique aléatoire, mais cela peut être n'importe quelle politique d'exploration.
- **Politique cible (target policy)** : C'est la politique que nous souhaitons optimiser. Elle est mise à jour en fonction des expériences collectées sous la politique de comportement.

### Points importants à retenir :
- **Pondération** : En MC avec importance pondérée, les retours sont pondérés pour refléter l'importance des actions prises par la politique de comportement par rapport à celles de la politique cible.
- **Stabilité** : La pondération permet de réduire les fluctuations dans les résultats et d'obtenir une convergence plus stable de la politique cible.

En résumé, cette première partie introduit l'idée clé du contrôle Monte Carlo avec échantillonnage par importance pondérée, en expliquant son utilité pour l'apprentissage à partir de politiques de comportement distinctes, et en montrant comment une politique aléatoire peut être générée pour cet objectif.

















----------------------
# Annexe:
----------------------

### **Code complet pour le contrôle Monte Carlo avec échantillonnage par importance pondérée :**

```python
import torch
import gym

# Création de l'environnement Blackjack avec Gym
env = gym.make('Blackjack-v0')

# Génération d'une politique aléatoire
def gen_random_policy(n_action):
    probs = torch.ones(n_action) / n_action
    def policy_function(state):
        return probs
    return policy_function

random_policy = gen_random_policy(env.action_space.n)

# Exécution d'un épisode sous la politique de comportement
def run_episode(env, behavior_policy):
    """
    Exécution d'un épisode donné une politique de comportement
    @param env: OpenAI Gym environment
    @param behavior_policy: politique de comportement
    @return: états, actions, et récompenses de l'épisode entier
    """
    state = env.reset()  # Réinitialise l'environnement
    rewards = []
    actions = []
    states = []
    is_done = False
    while not is_done:
        probs = behavior_policy(state)
        action = torch.multinomial(probs, 1).item()
        actions.append(action)
        states.append(state)
        state, reward, is_done, info = env.step(action)
        rewards.append(reward)
        if is_done:
            break
    return states, actions, rewards

# Implémentation du contrôle MC avec échantillonnage par importance pondérée
from collections import defaultdict

def mc_control_off_policy_weighted(env, gamma, n_episode, behavior_policy):
    """
    Obtention de la politique optimale avec l'échantillonnage par importance pondérée
    @param env: OpenAI Gym environment
    @param gamma: facteur de réduction
    @param n_episode: nombre d'épisodes
    @param behavior_policy: politique de comportement
    @return: fonction Q optimale et la politique optimale
    """
    n_action = env.action_space.n
    N = defaultdict(float)  # Nombre d'occurrences de chaque paire état-action
    Q = defaultdict(lambda: torch.empty(n_action))  # Fonction Q initialisée vide
    
    for episode in range(n_episode):
        w = 1.  # Poids initial
        # Exécute un épisode complet
        states_t, actions_t, rewards_t = run_episode(env, behavior_policy)
        return_t = 0
        # Parcours à l'envers des états, actions et récompenses
        for state_t, action_t, reward_t in zip(states_t[::-1], actions_t[::-1], rewards_t[::-1]):
            return_t = gamma * return_t + reward_t  # Calcul des retours pondérés
            N[(state_t, action_t)] += w  # Mise à jour de la pondération
            Q[state_t][action_t] += (w/N[(state_t, action_t)]) * (return_t - Q[state_t][action_t])  # Mise à jour de la fonction Q
            # Si l'action ne correspond pas à l'action optimale, on arrête
            if action_t != torch.argmax(Q[state_t]).item():
                break
            # Mise à jour du poids
            w *= 1. / behavior_policy(state_t)[action_t]    
    # Création de la politique optimale
    policy = {}
    for state, actions in Q.items():
        policy[state] = torch.argmax(actions).item()
    return Q, policy

gamma = 1  # Facteur de réduction

n_episode = 500000  # Nombre d'épisodes

# Exécution de l'algorithme pour obtenir la politique optimale
optimal_Q, optimal_policy = mc_control_off_policy_weighted(env, gamma, n_episode, random_policy)

# Simulation d'épisodes pour tester la politique optimale obtenue
def simulate_episode(env, policy):
    state = env.reset()
    is_done = False
    while not is_done:
        action = policy[state]  # Choix de l'action selon la politique optimale
        state, reward, is_done, info = env.step(action)
        if is_done:
            return reward  # Retourne la récompense

# Simulation de plusieurs épisodes
n_episode = 100000
n_win_optimal = 0  # Nombre de victoires
n_lose_optimal = 0  # Nombre de défaites

for _ in range(n_episode):
    reward = simulate_episode(env, optimal_policy)
    if reward == 1:
        n_win_optimal += 1
    elif reward == -1:
        n_lose_optimal += 1

# Affichage des probabilités de victoire et de défaite
print('Winning probability under the optimal policy: {}'.format(n_win_optimal/n_episode))
print('Losing probability under the optimal policy: {}'.format(n_lose_optimal/n_episode))
```

---

### **Explication étape par étape :**

1. **Création de l'environnement :**
   - Utilisation de `gym.make('Blackjack-v0')` pour créer un environnement de Blackjack.
   - Cet environnement permet de simuler des parties de Blackjack où l'algorithme doit apprendre à prendre les meilleures décisions.

2. **Politique aléatoire :**
   - La fonction `gen_random_policy` génère une politique qui sélectionne chaque action avec une probabilité égale.
   - Cela sert de **politique de comportement** pour explorer l'environnement.

3. **Exécution d'un épisode :**
   - La fonction `run_episode` simule un épisode complet sous la politique de comportement, en enregistrant les états, actions, et récompenses.
   - Cette information est utilisée pour ajuster la politique cible via l'échantillonnage pondéré.

4. **Contrôle Monte Carlo Off-Policy avec échantillonnage pondéré :**
   - La fonction `mc_control_off_policy_weighted` utilise les données de l'épisode pour mettre à jour la fonction Q via l'échantillonnage pondéré.
   - Pour chaque paire état-action rencontrée, le retour est mis à jour en fonction des poids calculés.
   - La politique optimale est obtenue en choisissant l'action avec la valeur Q la plus élevée pour chaque état.

5. **Simulation des épisodes sous la politique optimale :**
   - Après avoir obtenu la politique optimale, la fonction `simulate_episode` exécute plusieurs épisodes pour tester la performance de la politique.
   - Les probabilités de victoire et de défaite sont ensuite calculées.

---

### **Résultats :**
En exécutant le code, vous verrez des statistiques comme :
- **Probabilité de gagner** sous la politique optimale.
- **Probabilité de perdre** sous la politique optimale.
